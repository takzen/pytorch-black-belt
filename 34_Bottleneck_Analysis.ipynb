{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751f324f",
   "metadata": {},
   "source": [
    "# ü•ã Lekcja 34: Profilowanie (Gdzie ucieka czas?)\n",
    "\n",
    "Tw√≥j model trenuje siƒô wolno. Dlaczego?\n",
    "Zamiast zgadywaƒá, u≈ºyj **`torch.profiler`**.\n",
    "\n",
    "Profiler ≈õledzi ka≈ºde wywo≈Çanie operacji (Operator) w PyTorch i mierzy dwa czasy:\n",
    "1.  **CPU Time:** Ile czasu procesor spƒôdzi≈Ç na wydawaniu rozkazu.\n",
    "2.  **CUDA Time:** Ile czasu karta graficzna faktycznie liczy≈Ça.\n",
    "\n",
    "**Kluczowe pojƒôcia w tabeli wynik√≥w:**\n",
    "*   **Self Time:** Czas spƒôdzony w *tej konkretnej* funkcji (bez jej \"dzieci\").\n",
    "*   **Total Time:** Czas spƒôdzony w funkcji i wszystkich podfunkcjach, kt√≥re wywo≈Ça≈Ça.\n",
    "\n",
    "Je≈õli `Self CPU` jest wysokie -> Masz wolny kod Pythona (pƒôtle, listy).\n",
    "Je≈õli `Self CUDA` jest wysokie -> Masz ciƒô≈ºkƒÖ matematykƒô (du≈ºe macierze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb94c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profilujemy na: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Konfiguracja\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Profilujemy na: {device}\")\n",
    "\n",
    "# Je≈õli CPU, to nie bƒôdziemy mieli kolumn CUDA, ale logika jest ta sama.\n",
    "activities = [ProfilerActivity.CPU]\n",
    "if device == \"cuda\":\n",
    "    activities.append(ProfilerActivity.CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf32cac",
   "metadata": {},
   "source": [
    "## Symulacja: Model z \"WƒÖskim Gard≈Çem\"\n",
    "\n",
    "Stworzymy sieƒá, kt√≥ra ma 3 czƒô≈õci:\n",
    "1.  **Szybka:** Ma≈Çe mno≈ºenie macierzy.\n",
    "2.  **Wolna (Matematycznie):** Gigantyczne mno≈ºenie macierzy (obciƒÖ≈ºa GPU).\n",
    "3.  **G≈Çupia (Pythonowa):** Pƒôtla `for` wewnƒÖtrz modelu (obciƒÖ≈ºa CPU i blokuje GPU).\n",
    "\n",
    "Zobaczymy, czy Profiler to wykryje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e52320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gotowy. Czas na rentgen.\n"
     ]
    }
   ],
   "source": [
    "class SlowModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fast_layer = nn.Linear(100, 100)\n",
    "        self.heavy_layer = nn.Linear(4000, 4000) # 16 mln parametr√≥w!\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Czƒô≈õƒá Szybka\n",
    "        # record_function nadaje nazwƒô temu blokowi w raporcie\n",
    "        with record_function(\"1_FAST_PART\"):\n",
    "            x = self.fast_layer(x)\n",
    "            x = torch.relu(x)\n",
    "        \n",
    "        # 2. Czƒô≈õƒá G≈Çupia (Pƒôtla w Pythonie)\n",
    "        # To zabija wydajno≈õƒá, bo GPU czeka na Pythona\n",
    "        with record_function(\"2_STUPID_LOOP\"):\n",
    "            # Symulujemy bezsensownƒÖ operacjƒô\n",
    "            for _ in range(100): \n",
    "                x = x + 0.001\n",
    "        \n",
    "        # 3. Czƒô≈õƒá Ciƒô≈ºka (Du≈ºe macierze)\n",
    "        # Tu GPU powinno siƒô napociƒá\n",
    "        with record_function(\"3_HEAVY_MATH\"):\n",
    "            # Rozdmuchujemy x, ≈ºeby pasowa≈Ç do du≈ºej warstwy\n",
    "            x_big = x.repeat(1, 40) \n",
    "            x_out = self.heavy_layer(x_big)\n",
    "            \n",
    "        return x_out\n",
    "\n",
    "model = SlowModel().to(device)\n",
    "dummy_input = torch.randn(128, 100).to(device)\n",
    "\n",
    "print(\"Model gotowy. Czas na rentgen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d3723",
   "metadata": {},
   "source": [
    "## Uruchomienie Profilera\n",
    "\n",
    "U≈ºywamy `with profile(...)`.\n",
    "*   `record_shapes=True`: Zapisuje wymiary tensor√≥w (pomaga znale≈∫ƒá, gdzie zjadamy pamiƒôƒá).\n",
    "*   `with_stack=True`: Zapisuje, w kt√≥rej linijce kodu to siƒô sta≈Ço.\n",
    "\n",
    "**Wa≈ºne:** Pierwsze przej≈õcie przez sieƒá jest zawsze wolne (rozgrzewka/alokacja pamiƒôci). Profiler to poka≈ºe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a4eb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profilowanie zako≈Ñczone. Generowanie raportu...\n"
     ]
    }
   ],
   "source": [
    "# Uruchamiamy profilowanie\n",
    "with profile(activities=activities, record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(dummy_input)\n",
    "\n",
    "print(\"Profilowanie zako≈Ñczone. Generowanie raportu...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3287d",
   "metadata": {},
   "source": [
    "## Analiza Wynik√≥w\n",
    "\n",
    "Wy≈õwietlimy tabelƒô posortowanƒÖ wed≈Çug czasu **CUDA Total** (je≈õli masz GPU) lub **CPU Total**.\n",
    "\n",
    "Czego szukamy?\n",
    "1.  **`2_STUPID_LOOP`**: Powinno mieƒá wysoki czas CPU, a niski lub zerowy czas CUDA (bo to Python mieli).\n",
    "2.  **`3_HEAVY_MATH`**: Powinno mieƒá wysoki czas CUDA (bo to ciƒô≈ºka macierz).\n",
    "3.  **`1_FAST_PART`**: Powinno byƒá na dole listy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06999263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "     model_inference         0.22%     224.300us       100.00%     101.516ms     101.516ms             1  \n",
      "         1_FAST_PART         0.41%     411.600us        80.46%      81.679ms      81.679ms             1  \n",
      "        aten::linear         0.19%     196.800us        58.45%      59.339ms      29.670ms             2  \n",
      "             aten::t         0.21%     211.800us         0.54%     549.400us     274.700us             2  \n",
      "     aten::transpose         0.31%     317.600us         0.33%     337.600us     168.800us             2  \n",
      "    aten::as_strided         0.03%      30.000us         0.03%      30.000us       5.000us             6  \n",
      "         aten::addmm        57.72%      58.593ms        57.72%      58.593ms      29.296ms             2  \n",
      "          aten::relu         0.23%     235.300us        21.98%      22.314ms      22.314ms             1  \n",
      "     aten::clamp_min        21.75%      22.079ms        21.75%      22.079ms      22.079ms             1  \n",
      "       2_STUPID_LOOP         0.53%     538.000us         9.83%       9.983ms       9.983ms             1  \n",
      "           aten::add         9.30%       9.445ms         9.30%       9.445ms      94.452us           100  \n",
      "        3_HEAVY_MATH         0.20%     202.600us         9.49%       9.630ms       9.630ms             1  \n",
      "        aten::repeat         0.84%     849.200us         8.91%       9.041ms       9.041ms             1  \n",
      "        aten::expand         0.02%      15.900us         0.02%      22.800us      11.400us             2  \n",
      "         aten::empty         0.03%      29.000us         0.03%      29.000us      29.000us             1  \n",
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 101.516ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sortujemy po czasie GPU (lub CPU je≈õli brak GPU)\n",
    "sort_key = \"cuda_time_total\" if device == \"cuda\" else \"cpu_time_total\"\n",
    "\n",
    "print(prof.key_averages().table(sort_by=sort_key, row_limit=15))\n",
    "\n",
    "# Je≈õli tabela jest nieczytelna, sp√≥jrz na nazwy, kt√≥re sami nadali≈õmy (1_, 2_, 3_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29f5c7",
   "metadata": {},
   "source": [
    "## ü•ã Black Belt Summary\n",
    "\n",
    "Jak czytaƒá ten raport?\n",
    "\n",
    "1.  **Addmm (Matrix Multiply):** To zazwyczaj `nn.Linear`. Je≈õli zajmuje 90% czasu CUDA -> Zmniejsz model lub Batch Size.\n",
    "2.  **Aten::add / Aten::mul (Drobne operacje):** Je≈õli widzisz ich tysiƒÖce i zajmujƒÖ du≈ºo czasu CPU -> Masz pƒôtlƒô `for` w kodzie. U≈ºyj `torch.compile` (Lekcja 33) lub przepisz to wektorowo.\n",
    "3.  **Memcpy (Kopiowanie):** Je≈õli `to(device)` jest na szczycie -> U≈ºyj `num_workers` i `pin_memory` (Lekcja 18).\n",
    "\n",
    "**Zasada optymalizacji:**\n",
    "Zawsze najpierw naprawiaj to, co jest na szczycie listy (\"Low Hanging Fruit\"). Przyspieszanie ma≈Çej warstwy (FAST_PART) nie ma sensu, je≈õli 90% czasu zjada HEAVY_MATH."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
