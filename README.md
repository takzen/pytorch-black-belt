# ğŸ¥‹ PyTorch Black Belt

> **Od `import torch` do Mistrzostwa InÅ¼ynierskiego.**
> Kompleksowa kolekcja notebookÃ³w zaprojektowanych, aby wypeÅ‚niÄ‡ lukÄ™ miÄ™dzy "uruchomieniem modelu" a zrozumieniem mechanizmu pod maskÄ….

![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white) ![Status](https://img.shields.io/badge/Status-Aktywny_RozwÃ³j-success?style=for-the-badge)

## ğŸ¯ Cel

WiÄ™kszoÅ›Ä‡ tutoriali koÅ„czy siÄ™ na `model.fit()`. To repozytorium zaczyna tam, gdzie one koÅ„czÄ….
**PyTorch Black Belt** to zrozumienie "dlaczego" i "jak":

- Dlaczego `.view()` rzuca bÅ‚Ä™dy na nieciÄ…gÅ‚ych tensorach?
- Jak napisaÄ‡ wÅ‚asne przejÅ›cie wsteczne (Backward) dla nowej operacji?
- Jak debugowaÄ‡ ciche awarie, ktÃ³re nie rzucajÄ… wyjÄ…tkÃ³w, ale rujnujÄ… trening?
- Jak zoptymalizowaÄ‡ potoki danych, gdy wykorzystanie GPU jest niskie?

## ğŸ“š Program nauczania: ÅšcieÅ¼ka do Mistrzostwa

### ğŸ—ï¸ ModuÅ‚ 1: GÅ‚Ä™bokie zanurzenie w Tensory (Fundamenty)

_Nie opanujesz modelu, jeÅ›li nie opanujesz struktury danych._

- **Dojo Broadcastingu:** Matematyka stojÄ…ca za rozszerzaniem wymiarÃ³w.
- **Kroki i PamiÄ™Ä‡:** Zrozumienie `storage()`, `view()` vs `reshape()` oraz `contiguous()`.
- **Einsum:** Magiczna funkcja zastÄ™pujÄ…ca zÅ‚oÅ¼one operacje macierzowe.
- **Operacje w miejscu:** Kiedy `x += 1` oszczÄ™dza pamiÄ™Ä‡, a kiedy psuje Autograd.

### ğŸ§® ModuÅ‚ 2: WnÄ™trze Autogradu (Silnik)

_Hakowanie silnika pochodnych._

- **Graf Obliczeniowy:** Wizualizacja dynamicznej konstrukcji grafu.
- **`retain_graph=True`:** Przypadki uÅ¼ycia wykraczajÄ…ce poza podstawy.
- **Niestandardowe Funkcje Autogradu:** Pisanie wÅ‚asnych metod `forward` i `backward`.
- **Akumulacja GradientÃ³w:** Symulowanie duÅ¼ych batch'y na maÅ‚ym VRAM.

### ğŸ’¿ ModuÅ‚ 3: InÅ¼ynieria Danych (Paliwo)

_Åšmieci na wejÅ›ciu, Å›mieci na wyjÅ›ciu. Wolne wejÅ›cie, wolny trening._

- **IterableDataset:** ObsÅ‚uga strumieni i zbiorÃ³w danych wiÄ™kszych niÅ¼ RAM.
- **Niestandardowy Collate:** ZarzÄ…dzanie sekwencjami o zmiennej dÅ‚ugoÅ›ci i padding w locie.
- **Zaawansowane PrÃ³bkowanie:** Dynamiczne balansowanie niezbalansowanych zbiorÃ³w danych.
- **Analiza WÄ…skich GardeÅ‚:** Optymalizacja `num_workers`, `pin_memory` i prefetchingu.

### ğŸ§  ModuÅ‚ 4: Zaawansowana Architektura (Konstrukcja)

_Budowanie solidnych i zÅ‚oÅ¼onych systemÃ³w._

- **Hooki:** Inspekcja aktywacji i gradientÃ³w wewnÄ…trz czarnej skrzynki.
- **Strategie Inicjalizacji:** Dlaczego Xavier i Kaiming majÄ… znaczenie dla zbieÅ¼noÅ›ci.
- **WspÃ³Å‚dzielenie Wag:** WiÄ…zanie parametrÃ³w miÄ™dzy warstwami (np. Autokodery).
- **Dynamiczny PrzepÅ‚yw Sterowania:** UÅ¼ywanie logiki Pythona (`if/else`) wewnÄ…trz grafu.

### âš¡ ModuÅ‚ 5: Trening i Optymalizacja (SzybkoÅ›Ä‡)

_Wyciskanie kaÅ¼dego FLOPS-a z twojego GPU._

- **Mieszana Precyzja (AMP):** Implementacja `fp16` dla 2x przyspieszenia.
- **Schedulery:** Rozgrzewka, Cosine Annealing i Cykliczne Tempo Uczenia.
- **Przycinanie GradientÃ³w:** Zapobieganie eksplodujÄ…cym gradientom w RNN/Transformerach.
- **Torch 2.0:** Opanowanie `torch.compile` i strategii fuzji.

### ğŸ“¦ ModuÅ‚ 6: Ekosystem i Produkcja (Skala)

_PrzejÅ›cie z notebooka do klastra._

- **PyTorch Lightning:** Strukturyzowanie kodu dla powtarzalnoÅ›ci.
- **TorchScript i Tracing:** Eksportowanie modeli do Å›rodowisk C++.
- **DDP (Distributed Data Parallel):** Mechanika treningu na wielu GPU.
- **Profilowanie:** UÅ¼ywanie PyTorch Profiler do znajdowania wÄ…skich gardeÅ‚ w kodzie.

## ğŸ› ï¸ Stos Technologiczny i NarzÄ™dzia

Ten projekt koncentruje siÄ™ na nowoczesnym, wydajnym ekosystemie PyTorch:

- **Python 3.10+**
- **PyTorch 2.x** (GÅ‚Ã³wny framework, koncentracja na `torch.compile` i dynamicznych grafach)
- **Einops** (Czytelne i potÄ™Å¼ne operacje tensorowe)
- **PyTorch Lightning** (Organizacja zÅ‚oÅ¼onych potokÃ³w treningowych)
- **Torch Profiler i TensorBoard** (Debugowanie wydajnoÅ›ci)
- **NumPy i Pandas** (Manipulacja danymi)
- **Matplotlib i Seaborn** (Wizualizacja wnÄ™trz i krajobrazÃ³w strat)

## ğŸš€ Jak UÅ¼ywaÄ‡

Masz dwie opcje: natychmiastowe wykonanie w chmurze lub profesjonalnÄ… konfiguracjÄ™ lokalnÄ….

### â˜ï¸ Opcja 1: Google Colab (Zero Konfiguracji)

Najszybszy sposÃ³b na naukÄ™. KaÅ¼dy notebook w tym repozytorium ma przycisk **"Open in Colab"** na gÃ³rze.

1.  OtwÃ³rz dowolny plik `.ipynb` z listy plikÃ³w.
2.  Kliknij przycisk <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" style="vertical-align: middle">.
3.  Kod uruchamia siÄ™ natychmiast na darmowych GPU Google.

### ğŸ’» Opcja 2: RozwÃ³j Lokalny (VS Code + uv)

Zalecane dla inÅ¼ynierÃ³w budujÄ…cych wÅ‚asne Å›rodowisko eksperymentalne.

1.  **Sklonuj repozytorium:**

    ```bash
    git clone https://github.com/takzen/pytorch-black-belt.git
    cd pytorch-black-belt
    ```

2.  **Zainicjalizuj Å›rodowisko z `uv`:**

    ```bash
    # UtwÃ³rz wirtualne Å›rodowisko
    uv venv

    # Aktywuj je:
    # Windows:
    .venv\Scripts\activate
    # Linux/Mac:
    source .venv/bin/activate
    ```

3.  **Zainstaluj ZaleÅ¼noÅ›ci (Stos InÅ¼ynierski):**

    ```bash
    # 1. Zainstaluj PyTorch ze wsparciem CUDA (Dostosuj index-url dla twojego GPU)
    uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

    # 2. Zainstaluj NarzÄ™dzia (Einops, Profilery, Wizualizacja)
    uv pip install numpy pandas matplotlib seaborn einops lightning tensorboard torch-tb-profiler jupyterlab ipywidgets
    ```

---

## ğŸ“Š Statystyki Projektu

- **Kompleksowy program nauczania** skupiajÄ…cy siÄ™ wyÅ‚Ä…cznie na wewnÄ™trznych mechanizmach PyTorch i inÅ¼ynierii.
- **Od Matematyki do Produkcji:** Od rÄ™cznej implementacji propagacji wstecznej do treningu rozproszonego (DDP).
- **6 ModuÅ‚Ã³w GÅ‚Ä™bokiego Zanurzenia:** Tensory, Autograd, InÅ¼ynieria Danych, Architektura, Optymalizacja, Produkcja.
- **Implementacje Referencyjne:** Niestandardowe kernele CUDA, Memory-efficient Attention, Gradient Checkpointing.
- **Nowoczesny PyTorch 2.0:** Wykorzystanie `torch.compile` i strategii fuzji.

---

**Autor:** Krzysztof Pika
