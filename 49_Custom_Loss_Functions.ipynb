{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e6a3d3",
   "metadata": {},
   "source": [
    "# ü•ã Lekcja 49: Custom Loss Functions (Triplet Loss & Vectorization)\n",
    "\n",
    "Pisanie w≈Çasnej funkcji kosztu w PyTorch jest proste: wystarczy napisaƒá funkcjƒô, kt√≥ra przyjmuje Tensory i zwraca skalar, u≈ºywajƒÖc operacji r√≥≈ºniczkowalnych PyTorcha.\n",
    "\n",
    "Trudno≈õƒá le≈ºy w **wydajno≈õci** i **stabilno≈õci numerycznej**.\n",
    "\n",
    "**Studium przypadku: Triplet Loss**\n",
    "Chcemy nauczyƒá sieƒá, ≈ºe:\n",
    "*   Twarz A (Anchor) jest podobna do Twarzy P (Positive).\n",
    "*   Twarz A jest r√≥≈ºna od Twarzy N (Negative).\n",
    "\n",
    "Wz√≥r:\n",
    "$$ L = \\max(0, \\text{dist}(A, P) - \\text{dist}(A, N) + \\text{margin}) $$\n",
    "\n",
    "Wyzwaniem jest obliczenie odleg≈Ço≈õci euklidesowej dla ca≈Çego batcha naraz, bez pƒôtli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ef062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UrzƒÖdzenie: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"UrzƒÖdzenie: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52fe990",
   "metadata": {},
   "source": [
    "## Wersja 1: Naiwna (Powolna)\n",
    "\n",
    "Zaimplementujmy to \"po ludzku\", u≈ºywajƒÖc wbudowanej funkcji `pairwise_distance`.\n",
    "To dzia≈Ça, ale w bardziej skomplikowanych wariantach (np. szukanie najtrudniejszych negatyw√≥w w batchu - Hard Mining) wymaga≈Çoby pƒôtli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b86bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Loss: 1.4534\n"
     ]
    }
   ],
   "source": [
    "class NaiveTripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # anchor, positive, negative: [Batch, Embed_Dim]\n",
    "        \n",
    "        # 1. Liczymy dystanse\n",
    "        dist_pos = F.pairwise_distance(anchor, positive, p=2)\n",
    "        dist_neg = F.pairwise_distance(anchor, negative, p=2)\n",
    "        \n",
    "        # 2. Wz√≥r Hinge Loss\n",
    "        loss = torch.relu(dist_pos - dist_neg + self.margin)\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "# Test\n",
    "criterion_naive = NaiveTripletLoss()\n",
    "a = torch.randn(32, 128, requires_grad=True).to(DEVICE)\n",
    "p = torch.randn(32, 128, requires_grad=True).to(DEVICE)\n",
    "n = torch.randn(32, 128, requires_grad=True).to(DEVICE)\n",
    "\n",
    "loss = criterion_naive(a, p, n)\n",
    "print(f\"Naive Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb83dd2",
   "metadata": {},
   "source": [
    "## Wersja 2: Professional (Macierzowa)\n",
    "\n",
    "W zaawansowanych systemach (np. SimCLR, Metric Learning) czƒôsto musimy policzyƒá macierz odleg≈Ço≈õci **ka≈ºdy z ka≈ºdym** wewnƒÖtrz batcha.\n",
    "U≈ºycie pƒôtli jest tu zab√≥jcze.\n",
    "\n",
    "U≈ºyjemy wzoru skr√≥conego mno≈ºenia dla odleg≈Ço≈õci euklidesowej:\n",
    "$$ ||A - B||^2 = ||A||^2 + ||B||^2 - 2 \\cdot A \\cdot B^T $$\n",
    "\n",
    "Dziƒôki temu mo≈ºemy u≈ºyƒá ultraszybkiego mno≈ºenia macierzy (`@` lub `matmul`).\n",
    "\n",
    "**Pu≈Çapka NaN:**\n",
    "Pochodna z $\\sqrt{x}$ to $\\frac{1}{2\\sqrt{x}}$.\n",
    "Je≈õli $x=0$ (dystans wynosi zero, bo obrazy sƒÖ identyczne), mianownik wynosi 0 -> Gradient wybucha do `inf` -> Wagi stajƒÖ siƒô `NaN`.\n",
    "Musimy dodaƒá ma≈Çy $\\epsilon$ przed pierwiastkowaniem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea98a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zaawansowana funkcja kosztu gotowa.\n"
     ]
    }
   ],
   "source": [
    "def pairwise_distance_matrix(x, y):\n",
    "    \"\"\"\n",
    "    Oblicza dystans Euklidesowy miƒôdzy ka≈ºdym elementem x a ka≈ºdym elementem y.\n",
    "    x: [N, D]\n",
    "    y: [M, D]\n",
    "    Wynik: [N, M]\n",
    "    \"\"\"\n",
    "    # 1. Kwadraty norm\n",
    "    x_sq = torch.sum(x**2, dim=1, keepdim=True) # [N, 1]\n",
    "    y_sq = torch.sum(y**2, dim=1, keepdim=True) # [M, 1] -> transponujemy wirtualnie do [1, M]\n",
    "    \n",
    "    # 2. Iloczyn skalarny (2ab)\n",
    "    # [N, D] @ [D, M] -> [N, M]\n",
    "    prod = torch.matmul(x, y.t())\n",
    "    \n",
    "    # 3. Wz√≥r (a^2 + b^2 - 2ab)\n",
    "    # Broadcasting zadba o wymiary: [N, 1] + [1, M] - [N, M] -> [N, M]\n",
    "    dist_sq = x_sq + y_sq.t() - 2 * prod\n",
    "    \n",
    "    # 4. Zabezpieczenie przed ujemnymi zerami (b≈Çƒôdy float)\n",
    "    dist_sq = torch.clamp(dist_sq, min=1e-12)\n",
    "    \n",
    "    return torch.sqrt(dist_sq)\n",
    "\n",
    "class AdvancedTripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Tutaj liczymy tylko pary (i, i), ale dziƒôki funkcji macierzowej\n",
    "        # mogliby≈õmy ≈Çatwo zaimplementowaƒá \"Batch Hard Mining\" (najtrudniejszy negatyw w ca≈Çym batchu).\n",
    "        \n",
    "        # Obliczamy dystanse\n",
    "        # Uwaga: funkcja zwraca macierz NxN, my chcemy tylko przekƒÖtnƒÖ (odleg≈Ço≈õƒá pary i-i)\n",
    "        # Ale dla edukacji u≈ºyjemy tej funkcji.\n",
    "        \n",
    "        # Dystans A-P\n",
    "        dists_ap = pairwise_distance_matrix(anchor, positive)\n",
    "        # Bierzemy przekƒÖtnƒÖ (dystans miƒôdzy anchor[i] a positive[i])\n",
    "        d_ap = torch.diag(dists_ap)\n",
    "        \n",
    "        # Dystans A-N\n",
    "        dists_an = pairwise_distance_matrix(anchor, negative)\n",
    "        d_an = torch.diag(dists_an)\n",
    "        \n",
    "        loss = torch.relu(d_ap - d_an + self.margin)\n",
    "        return loss.mean()\n",
    "\n",
    "print(\"Zaawansowana funkcja kosztu gotowa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e686d0",
   "metadata": {},
   "source": [
    "## Weryfikacja: Gradienty i NaN\n",
    "\n",
    "Sprawd≈∫my, czy nasza funkcja jest stabilna.\n",
    "Stworzymy przypadek, gdzie `anchor == positive` (dystans = 0).\n",
    "W naiwnej implementacji (bez epsilora) `backward()` m√≥g≈Çby zwr√≥ciƒá `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "227b74e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss przy idealnym dopasowaniu: 0.0\n",
      "Gradient Anchora (norma): 0.0\n",
      "‚úÖ SUKCES: Gradient jest stabilny (dziƒôki clamp/epsilon).\n"
     ]
    }
   ],
   "source": [
    "criterion_adv = AdvancedTripletLoss()\n",
    "\n",
    "# --- POPRAWKA ---\n",
    "# Tworzymy tensor BEZPO≈öREDNIO na urzƒÖdzeniu (device=DEVICE).\n",
    "# Dziƒôki temu 'a_zero' jest Li≈õciem (Leaf Tensor) i jego .grad zostanie zachowany.\n",
    "a_zero = torch.randn(5, 10, device=DEVICE, requires_grad=True)\n",
    "\n",
    "# p_zero to klon a_zero.\n",
    "# Uwaga: p_zero nie jest li≈õciem (jest wynikiem klonowania), \n",
    "# ale nas interesuje gradient na 'a_zero', wiƒôc jest OK.\n",
    "p_zero = a_zero.clone() \n",
    "\n",
    "n_zero = torch.randn(5, 10, device=DEVICE, requires_grad=True)\n",
    "\n",
    "# Liczymy stratƒô\n",
    "loss = criterion_adv(a_zero, p_zero, n_zero)\n",
    "\n",
    "print(f\"Loss przy idealnym dopasowaniu: {loss.item()}\")\n",
    "\n",
    "# Pr√≥ba Backward\n",
    "try:\n",
    "    loss.backward()\n",
    "    \n",
    "    # Teraz a_zero.grad bƒôdzie istnia≈Ç i nie bƒôdzie ostrze≈ºenia\n",
    "    grad_norm = a_zero.grad.norm().item()\n",
    "    print(f\"Gradient Anchora (norma): {grad_norm}\")\n",
    "    \n",
    "    if torch.isnan(a_zero.grad).any():\n",
    "        print(\"‚ùå B≈ÅƒÑD: Gradient to NaN! (Dzielenie przez zero w pierwiastku)\")\n",
    "    else:\n",
    "        print(\"‚úÖ SUKCES: Gradient jest stabilny (dziƒôki clamp/epsilon).\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"B≈ÇƒÖd: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde688f3",
   "metadata": {},
   "source": [
    "## ü•ã Black Belt Summary\n",
    "\n",
    "1.  **Unikaj pƒôtli `for`** w funkcjach kosztu. Je≈õli masz batcha, u≈ºywaj operacji macierzowych (`matmul`, broadcasting).\n",
    "2.  **`clamp(min=1e-8)`**: Zawsze u≈ºywaj tego przed pierwiastkowaniem (`sqrt`) lub logarytmowaniem (`log`). W Deep Learningu zero jest Twoim wrogiem przy liczeniu pochodnych.\n",
    "3.  **Wz√≥r skr√≥conego mno≈ºenia:** $||a-b||^2 = a^2 + b^2 - 2ab$ to najszybszy spos√≥b na policzenie macierzy odleg≈Ço≈õci na GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
