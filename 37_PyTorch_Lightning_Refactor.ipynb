{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/37_PyTorch_Lightning_Refactor.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b80ec0",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 37: PyTorch Lightning (Koniec ze Spaghetti Code)\n",
    "\n",
    "PyTorch Lightning (PL) to nie jest nowa biblioteka ML. To **nak\u0142adka organizacyjna** na PyTorch.\n",
    "Wymusza na Tobie podzia\u0142 kodu na logiczne bloki:\n",
    "1.  **Co to jest model?** (`__init__`)\n",
    "2.  **Jak to liczy?** (`forward`)\n",
    "3.  **Jak to trenowa\u0107?** (`training_step`)\n",
    "4.  **Jakim optymalizatorem?** (`configure_optimizers`)\n",
    "\n",
    "Reszt\u0119 (p\u0119tle, urz\u0105dzenia, logowanie, paski post\u0119pu) bierze na siebie klasa **`Trainer`**.\n",
    "\n",
    "**Zysk:** Tw\u00f3j kod staje si\u0119 niezale\u017cny od sprz\u0119tu. Zmieniasz `accelerator=\"gpu\"` na `tpu` jedn\u0105 linijk\u0105, bez przepisywania ani jednego znaku w modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62fca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacja (standard w projektach pro)\n",
    "# !uv add lightning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lightning as L\n",
    "\n",
    "# W\u0142\u0105cza Tensor Cores (TF32) na nowszych GPU NVIDIA (Ampere+).\n",
    "# Zamienia precyzj\u0119 FP32 na TF32 dla mno\u017cenia macierzy.\n",
    "# Efekt: Du\u017ce przyspieszenie (nawet 3x) przy pomijalnej utracie dok\u0142adno\u015bci.\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Konfiguracja\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea595cb",
   "metadata": {},
   "source": [
    "## Krok 1: LightningModule\n",
    "\n",
    "Zamiast dziedziczy\u0107 po `nn.Module`, dziedziczymy po `L.LightningModule`.\n",
    "Zauwa\u017c, czego **NIE MA** w kodzie poni\u017cej:\n",
    "*   Nie ma `.to(device)` (PL robi to sam).\n",
    "*   Nie ma `.backward()` (PL robi to sam).\n",
    "*   Nie ma `optimizer.step()` i `zero_grad()` (PL robi to sam).\n",
    "\n",
    "Kod staje si\u0119 czyst\u0105 matematyk\u0105 problemu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47429f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Lightning zdefiniowany.\n"
     ]
    }
   ],
   "source": [
    "class LitModel(L.LightningModule):\n",
    "    def __init__(self, input_dim=10, hidden_dim=64, output_dim=1):\n",
    "        super().__init__()\n",
    "        # Definicja warstw (tak jak w zwyk\u0142ym PyTorch)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        # Zapisujemy hiperparametry (automatycznie logowane!)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # To jest wn\u0119trze p\u0119tli 'for batch in loader'\n",
    "        x, y = batch\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        # Loss\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        # Logowanie (pojawia si\u0119 na pasku post\u0119pu)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Definiujemy optymalizator (i ew. scheduler)\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Model Lightning zdefiniowany.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3944fb",
   "metadata": {},
   "source": [
    "## Krok 2: Dane (DataLoader)\n",
    "\n",
    "PL dzia\u0142a ze standardowymi `DataLoaderami` PyTorcha.\n",
    "Stw\u00f3rzmy proste dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19498f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader gotowy.\n"
     ]
    }
   ],
   "source": [
    "# Generujemy dane\n",
    "train_data = torch.randn(1000, 10)\n",
    "train_targets = torch.randn(1000, 1)\n",
    "\n",
    "dataset = TensorDataset(train_data, train_targets)\n",
    "# num_workers=0 dla bezpiecze\u0144stwa na Windows w notatniku\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=0)\n",
    "\n",
    "print(\"DataLoader gotowy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe89d8",
   "metadata": {},
   "source": [
    "## Krok 3: Trainer (Silnik)\n",
    "\n",
    "Tutaj dzieje si\u0119 magia. `Trainer` to Tw\u00f3j \"In\u017cynier ML\".\n",
    "M\u00f3wisz mu:\n",
    "*   `max_epochs=5`\n",
    "*   `accelerator=\"auto\"` (Sam wykryje czy masz GPU, CPU czy MPS).\n",
    "*   `devices=1`\n",
    "\n",
    "On sam zajmie si\u0119 paskiem post\u0119pu, obs\u0142ug\u0105 b\u0142\u0119d\u00f3w i p\u0119tl\u0105."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee09479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params | Mode  | FLOPs\n",
      "----------------------------------------------------\n",
      "0 | net  | Sequential | 769    | train | 0    \n",
      "----------------------------------------------------\n",
      "769       Trainable params\n",
      "0         Non-trainable params\n",
      "769       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 Start treningu z Lightning...\n",
      "Epoch 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00<00:00, 226.29it/s, train_loss=0.608]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00<00:00, 223.14it/s, train_loss=0.608]\n",
      "\u2705 Koniec.\n"
     ]
    }
   ],
   "source": [
    "# Inicjalizacja modelu\n",
    "model = LitModel()\n",
    "\n",
    "# Inicjalizacja Trenera\n",
    "# enable_checkpointing=False dla uproszczenia (\u017ceby nie tworzy\u0142 folder\u00f3w w demo)\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\", # Magia: sam znajdzie GPU\n",
    "    devices=1,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False # Wy\u0142\u0105czamy logowanie do plik\u00f3w dla czysto\u015bci demo\n",
    ")\n",
    "\n",
    "print(\"\ud83d\ude80 Start treningu z Lightning...\")\n",
    "trainer.fit(model=model, train_dataloaders=train_loader)\n",
    "print(\"\u2705 Koniec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8c733",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "Dlaczego Lightning to standard w pracy (a nie tylko czysty PyTorch)?\n",
    "\n",
    "1.  **Czytelno\u015b\u0107:** Kiedy wchodzisz do nowego projektu, od razu wiesz, gdzie szuka\u0107 modelu (`__init__`), a gdzie logiki uczenia (`training_step`).\n",
    "2.  **Skalowalno\u015b\u0107:** Chcesz uruchomi\u0107 ten sam kod na 8 GPU? Wystarczy zmieni\u0107 `devices=8` w Trainerze. W czystym PyTorch musia\u0142by\u015b u\u017cy\u0107 `DistributedDataParallel` i przepisa\u0107 po\u0142ow\u0119 kodu.\n",
    "3.  **Funkcjonalno\u015bci:** Mixed Precision (`precision=\"16-mixed\"`), Gradient Clipping, Profiler - wszystko to s\u0105 flagi w Trainerze.\n",
    "\n",
    "**Zasada:** Prototypuj w czystym PyTorch (\u017ceby zrozumie\u0107), wdra\u017caj w Lightning (\u017ceby utrzyma\u0107 porz\u0105dek)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}