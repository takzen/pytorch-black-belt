{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b80ec0",
   "metadata": {},
   "source": [
    "# ðŸ¥‹ Lekcja 37: PyTorch Lightning (Koniec ze Spaghetti Code)\n",
    "\n",
    "PyTorch Lightning (PL) to nie jest nowa biblioteka ML. To **nakÅ‚adka organizacyjna** na PyTorch.\n",
    "Wymusza na Tobie podziaÅ‚ kodu na logiczne bloki:\n",
    "1.  **Co to jest model?** (`__init__`)\n",
    "2.  **Jak to liczy?** (`forward`)\n",
    "3.  **Jak to trenowaÄ‡?** (`training_step`)\n",
    "4.  **Jakim optymalizatorem?** (`configure_optimizers`)\n",
    "\n",
    "ResztÄ™ (pÄ™tle, urzÄ…dzenia, logowanie, paski postÄ™pu) bierze na siebie klasa **`Trainer`**.\n",
    "\n",
    "**Zysk:** TwÃ³j kod staje siÄ™ niezaleÅ¼ny od sprzÄ™tu. Zmieniasz `accelerator=\"gpu\"` na `tpu` jednÄ… linijkÄ…, bez przepisywania ani jednego znaku w modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62fca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacja (standard w projektach pro)\n",
    "# !uv add lightning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lightning as L\n",
    "\n",
    "# WÅ‚Ä…cza Tensor Cores (TF32) na nowszych GPU NVIDIA (Ampere+).\n",
    "# Zamienia precyzjÄ™ FP32 na TF32 dla mnoÅ¼enia macierzy.\n",
    "# Efekt: DuÅ¼e przyspieszenie (nawet 3x) przy pomijalnej utracie dokÅ‚adnoÅ›ci.\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Konfiguracja\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea595cb",
   "metadata": {},
   "source": [
    "## Krok 1: LightningModule\n",
    "\n",
    "Zamiast dziedziczyÄ‡ po `nn.Module`, dziedziczymy po `L.LightningModule`.\n",
    "ZauwaÅ¼, czego **NIE MA** w kodzie poniÅ¼ej:\n",
    "*   Nie ma `.to(device)` (PL robi to sam).\n",
    "*   Nie ma `.backward()` (PL robi to sam).\n",
    "*   Nie ma `optimizer.step()` i `zero_grad()` (PL robi to sam).\n",
    "\n",
    "Kod staje siÄ™ czystÄ… matematykÄ… problemu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47429f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Lightning zdefiniowany.\n"
     ]
    }
   ],
   "source": [
    "class LitModel(L.LightningModule):\n",
    "    def __init__(self, input_dim=10, hidden_dim=64, output_dim=1):\n",
    "        super().__init__()\n",
    "        # Definicja warstw (tak jak w zwykÅ‚ym PyTorch)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        # Zapisujemy hiperparametry (automatycznie logowane!)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # To jest wnÄ™trze pÄ™tli 'for batch in loader'\n",
    "        x, y = batch\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        # Loss\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        # Logowanie (pojawia siÄ™ na pasku postÄ™pu)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Definiujemy optymalizator (i ew. scheduler)\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Model Lightning zdefiniowany.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3944fb",
   "metadata": {},
   "source": [
    "## Krok 2: Dane (DataLoader)\n",
    "\n",
    "PL dziaÅ‚a ze standardowymi `DataLoaderami` PyTorcha.\n",
    "StwÃ³rzmy proste dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19498f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader gotowy.\n"
     ]
    }
   ],
   "source": [
    "# Generujemy dane\n",
    "train_data = torch.randn(1000, 10)\n",
    "train_targets = torch.randn(1000, 1)\n",
    "\n",
    "dataset = TensorDataset(train_data, train_targets)\n",
    "# num_workers=0 dla bezpieczeÅ„stwa na Windows w notatniku\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=0)\n",
    "\n",
    "print(\"DataLoader gotowy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe89d8",
   "metadata": {},
   "source": [
    "## Krok 3: Trainer (Silnik)\n",
    "\n",
    "Tutaj dzieje siÄ™ magia. `Trainer` to TwÃ³j \"InÅ¼ynier ML\".\n",
    "MÃ³wisz mu:\n",
    "*   `max_epochs=5`\n",
    "*   `accelerator=\"auto\"` (Sam wykryje czy masz GPU, CPU czy MPS).\n",
    "*   `devices=1`\n",
    "\n",
    "On sam zajmie siÄ™ paskiem postÄ™pu, obsÅ‚ugÄ… bÅ‚Ä™dÃ³w i pÄ™tlÄ…."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee09479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params | Mode  | FLOPs\n",
      "----------------------------------------------------\n",
      "0 | net  | Sequential | 769    | train | 0    \n",
      "----------------------------------------------------\n",
      "769       Trainable params\n",
      "0         Non-trainable params\n",
      "769       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Start treningu z Lightning...\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 226.29it/s, train_loss=0.608]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 223.14it/s, train_loss=0.608]\n",
      "âœ… Koniec.\n"
     ]
    }
   ],
   "source": [
    "# Inicjalizacja modelu\n",
    "model = LitModel()\n",
    "\n",
    "# Inicjalizacja Trenera\n",
    "# enable_checkpointing=False dla uproszczenia (Å¼eby nie tworzyÅ‚ folderÃ³w w demo)\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\", # Magia: sam znajdzie GPU\n",
    "    devices=1,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False # WyÅ‚Ä…czamy logowanie do plikÃ³w dla czystoÅ›ci demo\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Start treningu z Lightning...\")\n",
    "trainer.fit(model=model, train_dataloaders=train_loader)\n",
    "print(\"âœ… Koniec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8c733",
   "metadata": {},
   "source": [
    "## ðŸ¥‹ Black Belt Summary\n",
    "\n",
    "Dlaczego Lightning to standard w pracy (a nie tylko czysty PyTorch)?\n",
    "\n",
    "1.  **CzytelnoÅ›Ä‡:** Kiedy wchodzisz do nowego projektu, od razu wiesz, gdzie szukaÄ‡ modelu (`__init__`), a gdzie logiki uczenia (`training_step`).\n",
    "2.  **SkalowalnoÅ›Ä‡:** Chcesz uruchomiÄ‡ ten sam kod na 8 GPU? Wystarczy zmieniÄ‡ `devices=8` w Trainerze. W czystym PyTorch musiaÅ‚byÅ› uÅ¼yÄ‡ `DistributedDataParallel` i przepisaÄ‡ poÅ‚owÄ™ kodu.\n",
    "3.  **FunkcjonalnoÅ›ci:** Mixed Precision (`precision=\"16-mixed\"`), Gradient Clipping, Profiler - wszystko to sÄ… flagi w Trainerze.\n",
    "\n",
    "**Zasada:** Prototypuj w czystym PyTorch (Å¼eby zrozumieÄ‡), wdraÅ¼aj w Lightning (Å¼eby utrzymaÄ‡ porzÄ…dek)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
