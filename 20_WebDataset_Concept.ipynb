{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00056c7",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/20_WebDataset_Concept.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f9661",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 20: WebDataset (Format TAR dla Big Data)\n",
    "\n",
    "Kiedy masz miliard plik\u00f3w, system plik\u00f3w staje si\u0119 w\u0105skim gard\u0142em.\n",
    "Otwarcie pliku (`open()`) trwa. Otwarcie miliarda plik\u00f3w trwa miliard razy d\u0142u\u017cej.\n",
    "\n",
    "**WebDataset (WDS)** to biblioteka i format oparty na standardowych archiwach **TAR**.\n",
    "*   Zamiast: folder z 1 000 000 plik\u00f3w `.jpg` i `.json`.\n",
    "*   Mamy: 100 plik\u00f3w `.tar`, a w ka\u017cdym po 10 000 par (obrazek + opis).\n",
    "\n",
    "**Zalety:**\n",
    "1.  **Sekwencyjny odczyt:** Dysk czyta jeden du\u017cy ci\u0105g bajt\u00f3w (maksymalna przepustowo\u015b\u0107).\n",
    "2.  **Streaming:** Mo\u017cesz trenowa\u0107 model na danych, kt\u00f3re le\u017c\u0105 na S3, nie pobieraj\u0105c ich na dysk! (Pipe mode).\n",
    "3.  **Shuffle:** Tasujemy w buforze RAM, a nie na dysku.\n",
    "\n",
    "Zrobimy symulacj\u0119: Stworzymy dataset w formacie TAR i odczytamy go strumieniowo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97cb172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m4 packages\u001b[0m \u001b[2min 1.19s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 175ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 55ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbraceexpand\u001b[0m\u001b[2m==0.1.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebdataset\u001b[0m\u001b[2m==1.0.2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katalog roboczy: data_wds\n"
     ]
    }
   ],
   "source": [
    "# Instalacja WebDataset\n",
    "!uv pip install webdataset\n",
    "\n",
    "import webdataset as wds\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Katalog na nasze dane\n",
    "DATA_DIR = \"data_wds\"\n",
    "if os.path.exists(DATA_DIR):\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "os.makedirs(DATA_DIR)\n",
    "\n",
    "print(f\"Katalog roboczy: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aef612",
   "metadata": {},
   "source": [
    "## Krok 1: Tworzenie Shard\u00f3w (Pisanie)\n",
    "\n",
    "Stworzymy syntetyczny dataset (obrazek + etykieta).\n",
    "Zapiszemy go jako seri\u0119 plik\u00f3w `.tar` (zwanych **Shardami**).\n",
    "\n",
    "U\u017cyjemy `wds.ShardWriter`.\n",
    "Wz\u00f3r nazwy: `dataset-%06d.tar` (dataset-000000.tar, dataset-000001.tar...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0aeb86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing data_wds\\mnist-dummy-000000.tar 0 0.0 GB 0\n",
      "# writing data_wds\\mnist-dummy-000001.tar 50 0.0 GB 50\n",
      "# writing data_wds\\mnist-dummy-000002.tar 50 0.0 GB 100\n",
      "# writing data_wds\\mnist-dummy-000003.tar 50 0.0 GB 150\n",
      "\u2705 Zapisano dane w formacie TAR.\n",
      "Lista plik\u00f3w:\n",
      " - mnist-dummy-000000.tar\n",
      " - mnist-dummy-000001.tar\n",
      " - mnist-dummy-000002.tar\n",
      " - mnist-dummy-000003.tar\n"
     ]
    }
   ],
   "source": [
    "# Wzorzec nazwy pliku (ograniczamy shard do 10MB lub 100 pr\u00f3bek)\n",
    "pattern = os.path.join(DATA_DIR, \"mnist-dummy-%06d.tar\")\n",
    "\n",
    "# Otwieramy pisarza\n",
    "# maxcount=50: Nowy plik .tar co 50 pr\u00f3bek\n",
    "with wds.ShardWriter(pattern, maxcount=50) as sink:\n",
    "    for i in range(200): # 200 pr\u00f3bek total (powstan\u0105 4 pliki tar)\n",
    "        \n",
    "        # Symulacja danych\n",
    "        # Obrazek: Losowy tensor, zapiszemy jako bajty (np. format .pth lub surowe)\n",
    "        # WDS lubi formaty standardowe (jpg, png, pyd), my u\u017cyjemy 'pth' dla tensora\n",
    "        image = torch.randn(3, 32, 32)\n",
    "        label = i % 10  # Klasa 0-9\n",
    "        \n",
    "        # Zapisujemy pr\u00f3bk\u0119 (S\u0142ownik)\n",
    "        sample = {\n",
    "            \"__key__\": f\"sample{i:05d}\",   # Unikalny klucz pliku wewn\u0105trz tara\n",
    "            \"input.pth\": image,            # Rozszerzenie m\u00f3wi, jak to odkodowa\u0107\n",
    "            \"label.cls\": label             # .cls to format dla liczby ca\u0142kowitej\n",
    "        }\n",
    "        \n",
    "        sink.write(sample)\n",
    "\n",
    "print(\"\u2705 Zapisano dane w formacie TAR.\")\n",
    "print(\"Lista plik\u00f3w:\")\n",
    "for f in sorted(os.listdir(DATA_DIR)):\n",
    "    print(f\" - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41997ebd",
   "metadata": {},
   "source": [
    "## Krok 2: Czytanie Strumieniowe (Pipeline)\n",
    "\n",
    "Teraz najwa\u017cniejsze. Jak to odczyta\u0107?\n",
    "`wds.WebDataset` dzia\u0142a jak ruroci\u0105g (Pipeline) w systemie Linux.\n",
    "\n",
    "1.  Wczytaj bajty z TAR-a.\n",
    "2.  Zdekoduj (np. zamie\u0144 bajty `.pth` z powrotem na Tensor).\n",
    "3.  Zmie\u0144 na krotk\u0119 `(input, label)`.\n",
    "\n",
    "To wszystko dzieje si\u0119 **w locie (on-the-fly)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd572da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista plik\u00f3w do wczytania:\n",
      "['data_wds\\\\mnist-dummy-000000.tar', 'data_wds\\\\mnist-dummy-000001.tar', 'data_wds\\\\mnist-dummy-000002.tar', 'data_wds\\\\mnist-dummy-000003.tar']\n",
      "Dataset zdefiniowany (Leniwy - nic jeszcze nie wczyta\u0142).\n"
     ]
    }
   ],
   "source": [
    "# Generujemy list\u0119 plik\u00f3w r\u0119cznie (bezpieczne na Windows/Linux)\n",
    "# Zamiast wzorca \"{..}\", tworzymy list\u0119 konkretnych \u015bcie\u017cek\n",
    "urls = [os.path.join(DATA_DIR, f\"mnist-dummy-{i:06d}.tar\") for i in range(4)]\n",
    "\n",
    "print(\"Lista plik\u00f3w do wczytania:\")\n",
    "print(urls)\n",
    "\n",
    "# Definicja Pipeline'u\n",
    "# WebDataset przyjmuje list\u0119 plik\u00f3w r\u00f3wnie ch\u0119tnie co wzorzec\n",
    "dataset = (\n",
    "    wds.WebDataset(urls)      # 1. Otw\u00f3rz strumie\u0144 z listy\n",
    "    .shuffle(100)             # 2. Tasuj w buforze (100 element\u00f3w w RAM)\n",
    "    .decode()                 # 3. Automatycznie dekoduj (.pth -> Tensor, .cls -> Int)\n",
    "    .to_tuple(\"input.pth\", \"label.cls\") # 4. Wybierz co chcesz zwr\u00f3ci\u0107\n",
    ")\n",
    "\n",
    "print(\"Dataset zdefiniowany (Leniwy - nic jeszcze nie wczyta\u0142).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf9642",
   "metadata": {},
   "source": [
    "## Integracja z DataLoaderem\n",
    "\n",
    "WebDataset jest typu **IterableDataset** (pami\u0119tasz Lekcj\u0119 15?).\n",
    "Dzia\u0142a \u015bwietnie z `DataLoader`, ale trzeba pami\u0119ta\u0107 o batchowaniu.\n",
    "\n",
    "WebDataset ma w\u0142asn\u0105 metod\u0119 `.batched(batch_size)`, kt\u00f3ra jest szybsza ni\u017c ta w DataLoaderze, bo skleja listy wewn\u0105trz C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2abbc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ODCZYT DANYCH ---\n",
      "Batch shape: torch.Size([16, 3, 32, 32])\n",
      "Labels: tensor([7, 3, 7, 5, 3, 3, 6, 0, 1, 9, 4, 4, 0, 5, 6, 4])\n",
      "Przetworzono 13 batchy.\n"
     ]
    }
   ],
   "source": [
    "# Dodajemy batchowanie do pipeline'u WDS\n",
    "batched_dataset = dataset.batched(16)\n",
    "\n",
    "# DataLoader s\u0142u\u017cy tu tylko do obs\u0142ugi worker\u00f3w i prefetchingu\n",
    "# batch_size=None, bo batchowanie zrobili\u015bmy ju\u017c wy\u017cej w WDS!\n",
    "loader = DataLoader(batched_dataset, batch_size=None, num_workers=0)\n",
    "\n",
    "print(\"--- ODCZYT DANYCH ---\")\n",
    "for i, (imgs, labels) in enumerate(loader):\n",
    "    if i == 0:\n",
    "        print(f\"Batch shape: {imgs.shape}\")\n",
    "        print(f\"Labels: {labels}\")\n",
    "    \n",
    "    # Symulacja treningu...\n",
    "    pass\n",
    "\n",
    "print(f\"Przetworzono {i+1} batchy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c71ee",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "To ko\u0144czy **Modu\u0142 3: In\u017cynieria Danych**.\n",
    "\n",
    "1.  **Dlaczego TAR?** System plik\u00f3w (OS) nie radzi sobie z milionami plik\u00f3w. TAR skleja je w du\u017ce bloki, co pozwala na sekwencyjny odczyt z maksymaln\u0105 pr\u0119dko\u015bci\u0105 dysku.\n",
    "2.  **WebDataset:** To standard w trenowaniu na klastrach (HPC). Pozwala na \"niesko\u0144czone\" zbiory danych, kt\u00f3re nie mieszcz\u0105 si\u0119 na dysku lokalnym (streaming).\n",
    "3.  **Struktura:** `Url -> Shuffle -> Decode -> Tuple -> Batch`.\n",
    "\n",
    "W nast\u0119pnym module (**Modu\u0142 4: Zaawansowana Architektura**) wejdziemy do \u015brodka `nn.Module`. Zrozumiemy cykl \u017cycia modelu, bufory i hooki."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}