{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017f9661",
   "metadata": {},
   "source": [
    "# ðŸ¥‹ Lekcja 20: WebDataset (Format TAR dla Big Data)\n",
    "\n",
    "Kiedy masz miliard plikÃ³w, system plikÃ³w staje siÄ™ wÄ…skim gardÅ‚em.\n",
    "Otwarcie pliku (`open()`) trwa. Otwarcie miliarda plikÃ³w trwa miliard razy dÅ‚uÅ¼ej.\n",
    "\n",
    "**WebDataset (WDS)** to biblioteka i format oparty na standardowych archiwach **TAR**.\n",
    "*   Zamiast: folder z 1 000 000 plikÃ³w `.jpg` i `.json`.\n",
    "*   Mamy: 100 plikÃ³w `.tar`, a w kaÅ¼dym po 10 000 par (obrazek + opis).\n",
    "\n",
    "**Zalety:**\n",
    "1.  **Sekwencyjny odczyt:** Dysk czyta jeden duÅ¼y ciÄ…g bajtÃ³w (maksymalna przepustowoÅ›Ä‡).\n",
    "2.  **Streaming:** MoÅ¼esz trenowaÄ‡ model na danych, ktÃ³re leÅ¼Ä… na S3, nie pobierajÄ…c ich na dysk! (Pipe mode).\n",
    "3.  **Shuffle:** Tasujemy w buforze RAM, a nie na dysku.\n",
    "\n",
    "Zrobimy symulacjÄ™: Stworzymy dataset w formacie TAR i odczytamy go strumieniowo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97cb172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m4 packages\u001b[0m \u001b[2min 1.19s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 175ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 55ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbraceexpand\u001b[0m\u001b[2m==0.1.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebdataset\u001b[0m\u001b[2m==1.0.2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katalog roboczy: data_wds\n"
     ]
    }
   ],
   "source": [
    "# Instalacja WebDataset\n",
    "!uv pip install webdataset\n",
    "\n",
    "import webdataset as wds\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Katalog na nasze dane\n",
    "DATA_DIR = \"data_wds\"\n",
    "if os.path.exists(DATA_DIR):\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "os.makedirs(DATA_DIR)\n",
    "\n",
    "print(f\"Katalog roboczy: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aef612",
   "metadata": {},
   "source": [
    "## Krok 1: Tworzenie ShardÃ³w (Pisanie)\n",
    "\n",
    "Stworzymy syntetyczny dataset (obrazek + etykieta).\n",
    "Zapiszemy go jako seriÄ™ plikÃ³w `.tar` (zwanych **Shardami**).\n",
    "\n",
    "UÅ¼yjemy `wds.ShardWriter`.\n",
    "WzÃ³r nazwy: `dataset-%06d.tar` (dataset-000000.tar, dataset-000001.tar...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0aeb86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing data_wds\\mnist-dummy-000000.tar 0 0.0 GB 0\n",
      "# writing data_wds\\mnist-dummy-000001.tar 50 0.0 GB 50\n",
      "# writing data_wds\\mnist-dummy-000002.tar 50 0.0 GB 100\n",
      "# writing data_wds\\mnist-dummy-000003.tar 50 0.0 GB 150\n",
      "âœ… Zapisano dane w formacie TAR.\n",
      "Lista plikÃ³w:\n",
      " - mnist-dummy-000000.tar\n",
      " - mnist-dummy-000001.tar\n",
      " - mnist-dummy-000002.tar\n",
      " - mnist-dummy-000003.tar\n"
     ]
    }
   ],
   "source": [
    "# Wzorzec nazwy pliku (ograniczamy shard do 10MB lub 100 prÃ³bek)\n",
    "pattern = os.path.join(DATA_DIR, \"mnist-dummy-%06d.tar\")\n",
    "\n",
    "# Otwieramy pisarza\n",
    "# maxcount=50: Nowy plik .tar co 50 prÃ³bek\n",
    "with wds.ShardWriter(pattern, maxcount=50) as sink:\n",
    "    for i in range(200): # 200 prÃ³bek total (powstanÄ… 4 pliki tar)\n",
    "        \n",
    "        # Symulacja danych\n",
    "        # Obrazek: Losowy tensor, zapiszemy jako bajty (np. format .pth lub surowe)\n",
    "        # WDS lubi formaty standardowe (jpg, png, pyd), my uÅ¼yjemy 'pth' dla tensora\n",
    "        image = torch.randn(3, 32, 32)\n",
    "        label = i % 10  # Klasa 0-9\n",
    "        \n",
    "        # Zapisujemy prÃ³bkÄ™ (SÅ‚ownik)\n",
    "        sample = {\n",
    "            \"__key__\": f\"sample{i:05d}\",   # Unikalny klucz pliku wewnÄ…trz tara\n",
    "            \"input.pth\": image,            # Rozszerzenie mÃ³wi, jak to odkodowaÄ‡\n",
    "            \"label.cls\": label             # .cls to format dla liczby caÅ‚kowitej\n",
    "        }\n",
    "        \n",
    "        sink.write(sample)\n",
    "\n",
    "print(\"âœ… Zapisano dane w formacie TAR.\")\n",
    "print(\"Lista plikÃ³w:\")\n",
    "for f in sorted(os.listdir(DATA_DIR)):\n",
    "    print(f\" - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41997ebd",
   "metadata": {},
   "source": [
    "## Krok 2: Czytanie Strumieniowe (Pipeline)\n",
    "\n",
    "Teraz najwaÅ¼niejsze. Jak to odczytaÄ‡?\n",
    "`wds.WebDataset` dziaÅ‚a jak rurociÄ…g (Pipeline) w systemie Linux.\n",
    "\n",
    "1.  Wczytaj bajty z TAR-a.\n",
    "2.  Zdekoduj (np. zamieÅ„ bajty `.pth` z powrotem na Tensor).\n",
    "3.  ZmieÅ„ na krotkÄ™ `(input, label)`.\n",
    "\n",
    "To wszystko dzieje siÄ™ **w locie (on-the-fly)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd572da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista plikÃ³w do wczytania:\n",
      "['data_wds\\\\mnist-dummy-000000.tar', 'data_wds\\\\mnist-dummy-000001.tar', 'data_wds\\\\mnist-dummy-000002.tar', 'data_wds\\\\mnist-dummy-000003.tar']\n",
      "Dataset zdefiniowany (Leniwy - nic jeszcze nie wczytaÅ‚).\n"
     ]
    }
   ],
   "source": [
    "# Generujemy listÄ™ plikÃ³w rÄ™cznie (bezpieczne na Windows/Linux)\n",
    "# Zamiast wzorca \"{..}\", tworzymy listÄ™ konkretnych Å›cieÅ¼ek\n",
    "urls = [os.path.join(DATA_DIR, f\"mnist-dummy-{i:06d}.tar\") for i in range(4)]\n",
    "\n",
    "print(\"Lista plikÃ³w do wczytania:\")\n",
    "print(urls)\n",
    "\n",
    "# Definicja Pipeline'u\n",
    "# WebDataset przyjmuje listÄ™ plikÃ³w rÃ³wnie chÄ™tnie co wzorzec\n",
    "dataset = (\n",
    "    wds.WebDataset(urls)      # 1. OtwÃ³rz strumieÅ„ z listy\n",
    "    .shuffle(100)             # 2. Tasuj w buforze (100 elementÃ³w w RAM)\n",
    "    .decode()                 # 3. Automatycznie dekoduj (.pth -> Tensor, .cls -> Int)\n",
    "    .to_tuple(\"input.pth\", \"label.cls\") # 4. Wybierz co chcesz zwrÃ³ciÄ‡\n",
    ")\n",
    "\n",
    "print(\"Dataset zdefiniowany (Leniwy - nic jeszcze nie wczytaÅ‚).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf9642",
   "metadata": {},
   "source": [
    "## Integracja z DataLoaderem\n",
    "\n",
    "WebDataset jest typu **IterableDataset** (pamiÄ™tasz LekcjÄ™ 15?).\n",
    "DziaÅ‚a Å›wietnie z `DataLoader`, ale trzeba pamiÄ™taÄ‡ o batchowaniu.\n",
    "\n",
    "WebDataset ma wÅ‚asnÄ… metodÄ™ `.batched(batch_size)`, ktÃ³ra jest szybsza niÅ¼ ta w DataLoaderze, bo skleja listy wewnÄ…trz C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2abbc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ODCZYT DANYCH ---\n",
      "Batch shape: torch.Size([16, 3, 32, 32])\n",
      "Labels: tensor([7, 3, 7, 5, 3, 3, 6, 0, 1, 9, 4, 4, 0, 5, 6, 4])\n",
      "Przetworzono 13 batchy.\n"
     ]
    }
   ],
   "source": [
    "# Dodajemy batchowanie do pipeline'u WDS\n",
    "batched_dataset = dataset.batched(16)\n",
    "\n",
    "# DataLoader sÅ‚uÅ¼y tu tylko do obsÅ‚ugi workerÃ³w i prefetchingu\n",
    "# batch_size=None, bo batchowanie zrobiliÅ›my juÅ¼ wyÅ¼ej w WDS!\n",
    "loader = DataLoader(batched_dataset, batch_size=None, num_workers=0)\n",
    "\n",
    "print(\"--- ODCZYT DANYCH ---\")\n",
    "for i, (imgs, labels) in enumerate(loader):\n",
    "    if i == 0:\n",
    "        print(f\"Batch shape: {imgs.shape}\")\n",
    "        print(f\"Labels: {labels}\")\n",
    "    \n",
    "    # Symulacja treningu...\n",
    "    pass\n",
    "\n",
    "print(f\"Przetworzono {i+1} batchy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c71ee",
   "metadata": {},
   "source": [
    "## ðŸ¥‹ Black Belt Summary\n",
    "\n",
    "To koÅ„czy **ModuÅ‚ 3: InÅ¼ynieria Danych**.\n",
    "\n",
    "1.  **Dlaczego TAR?** System plikÃ³w (OS) nie radzi sobie z milionami plikÃ³w. TAR skleja je w duÅ¼e bloki, co pozwala na sekwencyjny odczyt z maksymalnÄ… prÄ™dkoÅ›ciÄ… dysku.\n",
    "2.  **WebDataset:** To standard w trenowaniu na klastrach (HPC). Pozwala na \"nieskoÅ„czone\" zbiory danych, ktÃ³re nie mieszczÄ… siÄ™ na dysku lokalnym (streaming).\n",
    "3.  **Struktura:** `Url -> Shuffle -> Decode -> Tuple -> Batch`.\n",
    "\n",
    "W nastÄ™pnym module (**ModuÅ‚ 4: Zaawansowana Architektura**) wejdziemy do Å›rodka `nn.Module`. Zrozumiemy cykl Å¼ycia modelu, bufory i hooki."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
