{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/11_Jacobian_and_Hessian.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010d3b9",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 11: Jakobian i Hessian (Pochodne Wy\u017cszego Rz\u0119du)\n",
    "\n",
    "Standardowe `backward()` dzia\u0142a tylko wtedy, gdy wynik jest **skalarem** (jedn\u0105 liczb\u0105, np. Loss).\n",
    "Gdy wynik jest wektorem, PyTorch oblicza tzw. **vJP (vector-Jacobian Product)**, czyli od razu mno\u017cy macierz pochodnych przez wektor gradientu z g\u00f3ry.\n",
    "\n",
    "Ale czasem potrzebujemy **pe\u0142nej macierzy**:\n",
    "1.  **Jakobian (J):** Macierz pierwszych pochodnych dla funkcji wektorowych ($R^n \\to R^m$).\n",
    "2.  **Hessian (H):** Macierz drugich pochodnych ($R^n \\to R$). M\u00f3wi nam o **krzywi\u017anie** powierzchni b\u0142\u0119du.\n",
    "\n",
    "**Zastosowanie:**\n",
    "*   **MAML:** Optymalizacja parametr\u00f3w optymalizatora.\n",
    "*   **Influence Functions:** Sprawdzanie, kt\u00f3ry przyk\u0142ad treningowy najbardziej wp\u0142yn\u0105\u0142 na decyzj\u0119 modelu.\n",
    "*   **Newton's Method:** Szybsza optymalizacja (zamiast zgadywa\u0107 krok, skaczemy prosto do minimum paraboli)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3b1d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funkcje zdefiniowane.\n",
      "Dla wej\u015bcia tensor([2., 2.]):\n",
      "Wynik wektorowy: tensor([4., 8.])\n",
      "Wynik skalarny:  8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.functional import jacobian, hessian\n",
    "\n",
    "# Prosta funkcja wektorowa: wej\u015bcie (x, y) -> wyj\u015bcie (x^2, y^3)\n",
    "def func_vector(inputs):\n",
    "    x, y = inputs[0], inputs[1]\n",
    "    return torch.stack([x**2, y**3])\n",
    "\n",
    "# Prosta funkcja skalarna: wej\u015bcie (x, y) -> wyj\u015bcie x^2 + y^2 (Miseczka)\n",
    "def func_scalar(inputs):\n",
    "    x, y = inputs[0], inputs[1]\n",
    "    return x**2 + y**2\n",
    "\n",
    "inputs = torch.tensor([2.0, 2.0])\n",
    "\n",
    "print(\"Funkcje zdefiniowane.\")\n",
    "print(f\"Dla wej\u015bcia {inputs}:\")\n",
    "print(f\"Wynik wektorowy: {func_vector(inputs)}\") # [4, 8]\n",
    "print(f\"Wynik skalarny:  {func_scalar(inputs)}\") # 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a397a81",
   "metadata": {},
   "source": [
    "## 1. Macierz Jakobianu\n",
    "\n",
    "Funkcja: $f(x, y) = [x^2, y^3]$\n",
    "\n",
    "Jakobian to macierz pochodnych cz\u0105stkowych wszystkich wyj\u015b\u0107 po wszystkich wej\u015bciach:\n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "2x & 0 \\\\\n",
    "0 & 3y^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Dla $x=2, y=2$:\n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "4 & 0 \\\\\n",
    "0 & 12\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c79fefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MACIERZ JAKOBIANU ---\n",
      "tensor([[ 4.,  0.],\n",
      "        [ 0., 12.]])\n",
      "\u2705 Matematyka si\u0119 zgadza!\n"
     ]
    }
   ],
   "source": [
    "# Obliczamy Jakobian automatycznie\n",
    "J = jacobian(func_vector, inputs)\n",
    "\n",
    "print(\"--- MACIERZ JAKOBIANU ---\")\n",
    "print(J)\n",
    "\n",
    "# Weryfikacja\n",
    "expected = torch.tensor([[4., 0.], \n",
    "                         [0., 12.]])\n",
    "\n",
    "if torch.allclose(J, expected):\n",
    "    print(\"\u2705 Matematyka si\u0119 zgadza!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3694c783",
   "metadata": {},
   "source": [
    "## 2. Macierz Hessianu (Druga Pochodna)\n",
    "\n",
    "Hessian m\u00f3wi nam, jak bardzo \"zakrzywiona\" jest funkcja. Jest kluczowy, by wiedzie\u0107, czy jeste\u015bmy w do\u0142ku (minimum), na g\u00f3rce (maksimum) czy na siodle.\n",
    "\n",
    "Funkcja: $f(x, y) = x^2 + y^2$\n",
    "Pierwsza pochodna (Gradient): $[2x, 2y]$\n",
    "Druga pochodna (Hessian):\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Zauwa\u017c, \u017ce Hessian jest sta\u0142y (nie zale\u017cy od x, y), bo funkcja to idealna parabola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7be182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MACIERZ HESSIANU ---\n",
      "tensor([[2., 0.],\n",
      "        [0., 2.]])\n",
      "\n",
      "Warto\u015bci w\u0142asne: tensor([2., 2.])\n",
      "\ud83d\udc49 Dodatnio okre\u015blony: Jeste\u015bmy w 'miseczce' (lokalne minimum).\n"
     ]
    }
   ],
   "source": [
    "# Obliczamy Hessian\n",
    "H = hessian(func_scalar, inputs)\n",
    "\n",
    "print(\"--- MACIERZ HESSIANU ---\")\n",
    "print(H)\n",
    "\n",
    "# Analiza krzywizny\n",
    "# Warto\u015bci w\u0142asne Hessianu m\u00f3wi\u0105 o kszta\u0142cie\n",
    "eigenvalues = torch.linalg.eigvalsh(H)\n",
    "print(f\"\\nWarto\u015bci w\u0142asne: {eigenvalues}\")\n",
    "\n",
    "if (eigenvalues > 0).all():\n",
    "    print(\"\ud83d\udc49 Dodatnio okre\u015blony: Jeste\u015bmy w 'miseczce' (lokalne minimum).\")\n",
    "elif (eigenvalues < 0).all():\n",
    "    print(\"\ud83d\udc49 Ujemnie okre\u015blony: Jeste\u015bmy na 'g\u00f3rce' (lokalne maksimum).\")\n",
    "else:\n",
    "    print(\"\ud83d\udc49 Mieszane: Punkt siod\u0142owy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef7cdc",
   "metadata": {},
   "source": [
    "## In\u017cynieryjne Ostrze\u017cenie (Pami\u0119\u0107!)\n",
    "\n",
    "Dlaczego nie u\u017cywamy tego w ka\u017cdym treningu?\n",
    "Bo Hessian ma rozmiar $N \\times N$, gdzie $N$ to liczba parametr\u00f3w modelu.\n",
    "\n",
    "Je\u015bli Tw\u00f3j model ma 1 milion parametr\u00f3w (malutki):\n",
    "*   Gradient: 1 mln liczb (4 MB).\n",
    "*   Hessian: $10^{12}$ liczb (4 TB!). **To si\u0119 nie zmie\u015bci w \u017cadnym komputerze.**\n",
    "\n",
    "Dlatego w Deep Learningu u\u017cywa si\u0119 triku **Hessian-Vector Product (HvP)**.\n",
    "Mo\u017cemy policzy\u0107 iloczyn $H \\cdot v$ bez obliczania ca\u0142ej macierzy $H$!\n",
    "PyTorch robi to automatycznie, gdy policzysz gradient z gradientu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc32ea6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (pierwsza pochodna): tensor([4., 4.], grad_fn=<AddBackward0>)\n",
      "Hessian-Vector Product (H*v): tensor([2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# Przyk\u0142ad HvP (bez tworzenia wielkiej macierzy)\n",
    "\n",
    "x = torch.tensor([2.0, 2.0], requires_grad=True)\n",
    "y = x[0]**2 + x[1]**2  # x^2 + y^2\n",
    "\n",
    "# 1. Pierwszy Gradient (create_graph=True pozwala liczy\u0107 dalej!)\n",
    "grads = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print(f\"Gradient (pierwsza pochodna): {grads}\") # [4, 4]\n",
    "\n",
    "# 2. Drugi Gradient (rzutowany na wektor v)\n",
    "# Za\u0142\u00f3\u017cmy v = [1, 1]\n",
    "v = torch.tensor([1.0, 1.0])\n",
    "# Liczymy pochodn\u0105 z (Gradient * v)\n",
    "# To daje nam H * v\n",
    "hvp = torch.autograd.grad(grads, x, grad_outputs=v)[0]\n",
    "\n",
    "print(f\"Hessian-Vector Product (H*v): {hvp}\")\n",
    "# H to [[2,0],[0,2]], v to [1,1]. Wynik powinien by\u0107 [2, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf513a8",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  **`jacobian` i `hessian`**: \u015awietne do analizy matematycznej ma\u0142ych funkcji lub debugowania. Bezu\u017cyteczne dla pe\u0142nych sieci neuronowych (przez pami\u0119\u0107).\n",
    "2.  **`create_graph=True`**: Klucz do liczenia pochodnych wy\u017cszego rz\u0119du. M\u00f3wi PyTorchowi: \"Graf, kt\u00f3ry w\u0142a\u015bnie zbudowa\u0142e\u015b do policzenia gradientu? Nie wyrzucaj go! Chc\u0119 go r\u00f3\u017cniczkowa\u0107 jeszcze raz\".\n",
    "3.  **HvP (Hessian-Vector Product):** To jedyny spos\u00f3b na u\u017cywanie informacji o krzywi\u017anie w du\u017cych sieciach (np. w metodach optymalizacji drugiego rz\u0119du)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}