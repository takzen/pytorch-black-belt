{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7010d3b9",
   "metadata": {},
   "source": [
    "# ü•ã Lekcja 11: Jakobian i Hessian (Pochodne Wy≈ºszego Rzƒôdu)\n",
    "\n",
    "Standardowe `backward()` dzia≈Ça tylko wtedy, gdy wynik jest **skalarem** (jednƒÖ liczbƒÖ, np. Loss).\n",
    "Gdy wynik jest wektorem, PyTorch oblicza tzw. **vJP (vector-Jacobian Product)**, czyli od razu mno≈ºy macierz pochodnych przez wektor gradientu z g√≥ry.\n",
    "\n",
    "Ale czasem potrzebujemy **pe≈Çnej macierzy**:\n",
    "1.  **Jakobian (J):** Macierz pierwszych pochodnych dla funkcji wektorowych ($R^n \\to R^m$).\n",
    "2.  **Hessian (H):** Macierz drugich pochodnych ($R^n \\to R$). M√≥wi nam o **krzywi≈∫nie** powierzchni b≈Çƒôdu.\n",
    "\n",
    "**Zastosowanie:**\n",
    "*   **MAML:** Optymalizacja parametr√≥w optymalizatora.\n",
    "*   **Influence Functions:** Sprawdzanie, kt√≥ry przyk≈Çad treningowy najbardziej wp≈ÇynƒÖ≈Ç na decyzjƒô modelu.\n",
    "*   **Newton's Method:** Szybsza optymalizacja (zamiast zgadywaƒá krok, skaczemy prosto do minimum paraboli)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3b1d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funkcje zdefiniowane.\n",
      "Dla wej≈õcia tensor([2., 2.]):\n",
      "Wynik wektorowy: tensor([4., 8.])\n",
      "Wynik skalarny:  8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.functional import jacobian, hessian\n",
    "\n",
    "# Prosta funkcja wektorowa: wej≈õcie (x, y) -> wyj≈õcie (x^2, y^3)\n",
    "def func_vector(inputs):\n",
    "    x, y = inputs[0], inputs[1]\n",
    "    return torch.stack([x**2, y**3])\n",
    "\n",
    "# Prosta funkcja skalarna: wej≈õcie (x, y) -> wyj≈õcie x^2 + y^2 (Miseczka)\n",
    "def func_scalar(inputs):\n",
    "    x, y = inputs[0], inputs[1]\n",
    "    return x**2 + y**2\n",
    "\n",
    "inputs = torch.tensor([2.0, 2.0])\n",
    "\n",
    "print(\"Funkcje zdefiniowane.\")\n",
    "print(f\"Dla wej≈õcia {inputs}:\")\n",
    "print(f\"Wynik wektorowy: {func_vector(inputs)}\") # [4, 8]\n",
    "print(f\"Wynik skalarny:  {func_scalar(inputs)}\") # 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a397a81",
   "metadata": {},
   "source": [
    "## 1. Macierz Jakobianu\n",
    "\n",
    "Funkcja: $f(x, y) = [x^2, y^3]$\n",
    "\n",
    "Jakobian to macierz pochodnych czƒÖstkowych wszystkich wyj≈õƒá po wszystkich wej≈õciach:\n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "2x & 0 \\\\\n",
    "0 & 3y^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Dla $x=2, y=2$:\n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "4 & 0 \\\\\n",
    "0 & 12\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c79fefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MACIERZ JAKOBIANU ---\n",
      "tensor([[ 4.,  0.],\n",
      "        [ 0., 12.]])\n",
      "‚úÖ Matematyka siƒô zgadza!\n"
     ]
    }
   ],
   "source": [
    "# Obliczamy Jakobian automatycznie\n",
    "J = jacobian(func_vector, inputs)\n",
    "\n",
    "print(\"--- MACIERZ JAKOBIANU ---\")\n",
    "print(J)\n",
    "\n",
    "# Weryfikacja\n",
    "expected = torch.tensor([[4., 0.], \n",
    "                         [0., 12.]])\n",
    "\n",
    "if torch.allclose(J, expected):\n",
    "    print(\"‚úÖ Matematyka siƒô zgadza!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3694c783",
   "metadata": {},
   "source": [
    "## 2. Macierz Hessianu (Druga Pochodna)\n",
    "\n",
    "Hessian m√≥wi nam, jak bardzo \"zakrzywiona\" jest funkcja. Jest kluczowy, by wiedzieƒá, czy jeste≈õmy w do≈Çku (minimum), na g√≥rce (maksimum) czy na siodle.\n",
    "\n",
    "Funkcja: $f(x, y) = x^2 + y^2$\n",
    "Pierwsza pochodna (Gradient): $[2x, 2y]$\n",
    "Druga pochodna (Hessian):\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Zauwa≈º, ≈ºe Hessian jest sta≈Çy (nie zale≈ºy od x, y), bo funkcja to idealna parabola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7be182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MACIERZ HESSIANU ---\n",
      "tensor([[2., 0.],\n",
      "        [0., 2.]])\n",
      "\n",
      "Warto≈õci w≈Çasne: tensor([2., 2.])\n",
      "üëâ Dodatnio okre≈õlony: Jeste≈õmy w 'miseczce' (lokalne minimum).\n"
     ]
    }
   ],
   "source": [
    "# Obliczamy Hessian\n",
    "H = hessian(func_scalar, inputs)\n",
    "\n",
    "print(\"--- MACIERZ HESSIANU ---\")\n",
    "print(H)\n",
    "\n",
    "# Analiza krzywizny\n",
    "# Warto≈õci w≈Çasne Hessianu m√≥wiƒÖ o kszta≈Çcie\n",
    "eigenvalues = torch.linalg.eigvalsh(H)\n",
    "print(f\"\\nWarto≈õci w≈Çasne: {eigenvalues}\")\n",
    "\n",
    "if (eigenvalues > 0).all():\n",
    "    print(\"üëâ Dodatnio okre≈õlony: Jeste≈õmy w 'miseczce' (lokalne minimum).\")\n",
    "elif (eigenvalues < 0).all():\n",
    "    print(\"üëâ Ujemnie okre≈õlony: Jeste≈õmy na 'g√≥rce' (lokalne maksimum).\")\n",
    "else:\n",
    "    print(\"üëâ Mieszane: Punkt siod≈Çowy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef7cdc",
   "metadata": {},
   "source": [
    "## In≈ºynieryjne Ostrze≈ºenie (Pamiƒôƒá!)\n",
    "\n",
    "Dlaczego nie u≈ºywamy tego w ka≈ºdym treningu?\n",
    "Bo Hessian ma rozmiar $N \\times N$, gdzie $N$ to liczba parametr√≥w modelu.\n",
    "\n",
    "Je≈õli Tw√≥j model ma 1 milion parametr√≥w (malutki):\n",
    "*   Gradient: 1 mln liczb (4 MB).\n",
    "*   Hessian: $10^{12}$ liczb (4 TB!). **To siƒô nie zmie≈õci w ≈ºadnym komputerze.**\n",
    "\n",
    "Dlatego w Deep Learningu u≈ºywa siƒô triku **Hessian-Vector Product (HvP)**.\n",
    "Mo≈ºemy policzyƒá iloczyn $H \\cdot v$ bez obliczania ca≈Çej macierzy $H$!\n",
    "PyTorch robi to automatycznie, gdy policzysz gradient z gradientu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc32ea6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (pierwsza pochodna): tensor([4., 4.], grad_fn=<AddBackward0>)\n",
      "Hessian-Vector Product (H*v): tensor([2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# Przyk≈Çad HvP (bez tworzenia wielkiej macierzy)\n",
    "\n",
    "x = torch.tensor([2.0, 2.0], requires_grad=True)\n",
    "y = x[0]**2 + x[1]**2  # x^2 + y^2\n",
    "\n",
    "# 1. Pierwszy Gradient (create_graph=True pozwala liczyƒá dalej!)\n",
    "grads = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print(f\"Gradient (pierwsza pochodna): {grads}\") # [4, 4]\n",
    "\n",
    "# 2. Drugi Gradient (rzutowany na wektor v)\n",
    "# Za≈Ç√≥≈ºmy v = [1, 1]\n",
    "v = torch.tensor([1.0, 1.0])\n",
    "# Liczymy pochodnƒÖ z (Gradient * v)\n",
    "# To daje nam H * v\n",
    "hvp = torch.autograd.grad(grads, x, grad_outputs=v)[0]\n",
    "\n",
    "print(f\"Hessian-Vector Product (H*v): {hvp}\")\n",
    "# H to [[2,0],[0,2]], v to [1,1]. Wynik powinien byƒá [2, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf513a8",
   "metadata": {},
   "source": [
    "## ü•ã Black Belt Summary\n",
    "\n",
    "1.  **`jacobian` i `hessian`**: ≈öwietne do analizy matematycznej ma≈Çych funkcji lub debugowania. Bezu≈ºyteczne dla pe≈Çnych sieci neuronowych (przez pamiƒôƒá).\n",
    "2.  **`create_graph=True`**: Klucz do liczenia pochodnych wy≈ºszego rzƒôdu. M√≥wi PyTorchowi: \"Graf, kt√≥ry w≈Ça≈õnie zbudowa≈Çe≈õ do policzenia gradientu? Nie wyrzucaj go! Chcƒô go r√≥≈ºniczkowaƒá jeszcze raz\".\n",
    "3.  **HvP (Hessian-Vector Product):** To jedyny spos√≥b na u≈ºywanie informacji o krzywi≈∫nie w du≈ºych sieciach (np. w metodach optymalizacji drugiego rzƒôdu)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
