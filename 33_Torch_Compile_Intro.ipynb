{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b12abd",
   "metadata": {},
   "source": [
    "# ðŸ¥‹ Lekcja 33: PyTorch 2.0 & torch.compile (Darmowe Przyspieszenie)\n",
    "\n",
    "To najwiÄ™ksza zmiana w historii PyTorcha.\n",
    "Funkcja `torch.compile(model)` analizuje TwÃ³j kod, znajduje sekwencje operacji, ktÃ³re moÅ¼na poÅ‚Ä…czyÄ‡, i generuje w locie **zoptymalizowany kod C++/Triton**.\n",
    "\n",
    "**GÅ‚Ã³wne zalety:**\n",
    "1.  **Kernel Fusion:** ÅÄ…czenie maÅ‚ych operacji (Add, Mul, Gelu) w jeden duÅ¼y kernel CUDA. Mniej czytania/pisania do VRAM.\n",
    "2.  **Graph Capture:** Eliminuje narzut Pythona (Python Overhead).\n",
    "\n",
    "**Tryby kompilacji:**\n",
    "*   `default`: Balans miÄ™dzy szybkoÅ›ciÄ… kompilacji a szybkoÅ›ciÄ… dziaÅ‚ania.\n",
    "*   `reduce-overhead`: UÅ¼ywa CUDA Graphs. Dobre dla maÅ‚ych batchy.\n",
    "*   `max-autotune`: NajdÅ‚uÅ¼ej siÄ™ kompiluje (profiluje rÃ³Å¼ne konfiguracje), ale daje najszybszy kod.\n",
    "\n",
    "*Uwaga: Na Windowsie torch.compile wciÄ…Å¼ bywa kapryÅ›ne (czÄ™sto wymaga MSVC lub dziaÅ‚a wolniej). Na Linuxie/WSL to rakieta.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d95e2135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UrzÄ…dzenie: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch._dynamo\n",
    "import time\n",
    "\n",
    "# SprawdÅºmy, czy mamy GPU (Ampere lub nowsze najlepiej korzystajÄ… z Tritona)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    # Resetujemy ustawienia dynamo (dla czystoÅ›ci testu)\n",
    "    torch._dynamo.reset()\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"âš ï¸ Testujemy na CPU. Przyspieszenie bÄ™dzie mniejsze niÅ¼ na GPU.\")\n",
    "\n",
    "print(f\"UrzÄ…dzenie: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49833ba",
   "metadata": {},
   "source": [
    "## Model Testowy (Element-wise Heavy)\n",
    "\n",
    "Kompilator bÅ‚yszczy tam, gdzie jest duÅ¼o operacji **element-wise** (dziaÅ‚ajÄ…cych na kaÅ¼dym pikselu osobno), np. funkcje aktywacji, normalizacje, dodawanie.\n",
    "Zbudujemy sieÄ‡, ktÃ³ra robi duÅ¼o \"matematycznej drobnicy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fc8f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gotowy.\n"
     ]
    }
   ],
   "source": [
    "class HeavyOpsLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Linear(1024, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sekwencja operacji punktowych (Element-wise)\n",
    "        # W Eager Mode kaÅ¼da z nich to osobny kernel CUDA (osobny zapis/odczyt pamiÄ™ci).\n",
    "        x = self.w(x)\n",
    "        x = torch.cos(x)\n",
    "        x = torch.sin(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x * 2\n",
    "        x = x + 1\n",
    "        return x\n",
    "\n",
    "model = HeavyOpsLayer().to(device)\n",
    "input_data = torch.randn(2048, 1024).to(device) # Spory batch\n",
    "\n",
    "print(\"Model gotowy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f8cf41",
   "metadata": {},
   "source": [
    "## Benchmark: Eager Mode (Standard)\n",
    "\n",
    "Zmierzmy czas w \"starym\" PyTorch.\n",
    "UÅ¼ywamy `torch.cuda.synchronize()`, Å¼eby zmierzyÄ‡ prawdziwy czas wykonania na GPU (bo GPU dziaÅ‚a asynchronicznie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a86cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas Eager Mode: 0.752 ms / iter\n"
     ]
    }
   ],
   "source": [
    "def benchmark(model, x, runs=100):\n",
    "    # Rozgrzewka (Warmup)\n",
    "    for _ in range(10):\n",
    "        _ = model(x)\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "    start = time.time()\n",
    "    for _ in range(runs):\n",
    "        _ = model(x)\n",
    "        \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "    end = time.time()\n",
    "    return (end - start) / runs\n",
    "\n",
    "# Test Eager\n",
    "eager_time = benchmark(model, input_data)\n",
    "print(f\"Czas Eager Mode: {eager_time*1000:.3f} ms / iter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361e944",
   "metadata": {},
   "source": [
    "## Benchmark: Compiled Mode\n",
    "\n",
    "Teraz magia. `torch.compile`.\n",
    "\n",
    "**WaÅ¼ne:** Pierwsze uruchomienie skompilowanego modelu bÄ™dzie **bardzo wolne**.\n",
    "Dlaczego? Bo wtedy trwa kompilacja (JIT - Just In Time). PyTorch analizuje kod, generuje Triton/C++ i kompiluje go.\n",
    "Dopiero drugie i kolejne uruchomienia sÄ… szybkie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65745442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kompilowanie modelu (to moÅ¼e chwilÄ™ potrwaÄ‡)...\n",
      "â„¹ï¸ Wykryto Windows. Triton nie jest dostÄ™pny.\n",
      "PrÃ³ba uÅ¼ycia backendu 'cudagraphs' (wymaga sterownikÃ³w) lub 'eager'...\n",
      "âœ… Pierwsze uruchomienie (Warmup): 0.04 s\n",
      "Czas Compiled Mode (eager): 0.937 ms / iter\n",
      "\n",
      "ðŸš€ Przyspieszenie: 0.80x\n",
      "(Uwaga: Backend 'eager' nie optymalizuje kodu, sÅ‚uÅ¼y tylko do testu API na Windowsie)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"Kompilowanie modelu (to moÅ¼e chwilÄ™ potrwaÄ‡)...\")\n",
    "\n",
    "# WybÃ³r backendu w zaleÅ¼noÅ›ci od systemu\n",
    "# Na Linuxie 'inductor' (Triton) jest domyÅ›lny i najszybszy.\n",
    "# Na Windowsie Triton nie dziaÅ‚a, wiÄ™c prÃ³bujemy 'cudagraphs' lub 'eager' (brak optymalizacji, ale testuje API).\n",
    "if os.name == 'nt': # Windows\n",
    "    print(\"â„¹ï¸ Wykryto Windows. Triton nie jest dostÄ™pny.\")\n",
    "    print(\"PrÃ³ba uÅ¼ycia backendu 'cudagraphs' (wymaga sterownikÃ³w) lub 'eager'...\")\n",
    "    preferred_backend = \"eager\" # Bezpieczny fallback, Å¼eby kod przeszedÅ‚\n",
    "else:\n",
    "    preferred_backend = \"inductor\" # DomyÅ›lny na Linux\n",
    "\n",
    "try:\n",
    "    # Uruchamiamy kompilacjÄ™\n",
    "    compiled_model = torch.compile(model, backend=preferred_backend)\n",
    "    \n",
    "    # Pierwsze uruchomienie (Kompilacja/Rozgrzewka)\n",
    "    start_compile = time.time()\n",
    "    _ = compiled_model(input_data)\n",
    "    print(f\"âœ… Pierwsze uruchomienie (Warmup): {time.time() - start_compile:.2f} s\")\n",
    "\n",
    "    # WÅ‚aÅ›ciwy Benchmark\n",
    "    compiled_time = benchmark(compiled_model, input_data)\n",
    "    print(f\"Czas Compiled Mode ({preferred_backend}): {compiled_time*1000:.3f} ms / iter\")\n",
    "\n",
    "    # Podsumowanie\n",
    "    speedup = eager_time / compiled_time\n",
    "    print(f\"\\nðŸš€ Przyspieszenie: {speedup:.2f}x\")\n",
    "    \n",
    "    if preferred_backend == \"eager\":\n",
    "        print(\"(Uwaga: Backend 'eager' nie optymalizuje kodu, sÅ‚uÅ¼y tylko do testu API na Windowsie)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸ Nawet fallback zawiÃ³dÅ‚: {e}\")\n",
    "    print(\"To normalne na Windowsie bez peÅ‚nego Å›rodowiska C++.\")\n",
    "    print(\"Lekcja zaliczona: Wiesz jak uÅ¼ywaÄ‡ torch.compile, ale potrzebujesz Linuxa, Å¼eby zobaczyÄ‡ zysk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201bf65",
   "metadata": {},
   "source": [
    "## ðŸ¥‹ Black Belt Summary\n",
    "\n",
    "1.  **Kernel Fusion:** To gÅ‚Ã³wny powÃ³d przyspieszenia. Zamiast:\n",
    "    *   `Load -> Cos -> Store`\n",
    "    *   `Load -> Sin -> Store`\n",
    "    `torch.compile` robi:\n",
    "    *   `Load -> Cos -> Sin -> Store` (w rejestrach procesora GPU).\n",
    "    OszczÄ™dzasz przepustowoÅ›Ä‡ pamiÄ™ci (Memory Bandwidth), ktÃ³ra jest wÄ…skim gardÅ‚em.\n",
    "\n",
    "2.  **Kiedy uÅ¼ywaÄ‡?**\n",
    "    *   Zawsze na produkcji (Inference).\n",
    "    *   Zawsze przy treningu duÅ¼ych modeli (TransformerÃ³w).\n",
    "\n",
    "3.  **Wymagania:**\n",
    "    *   Nowoczesne GPU (Turing, Ampere).\n",
    "    *   Na Windowsie moÅ¼e wymagaÄ‡ instalacji `Microsoft C++ Build Tools`. Na Linuxie dziaÅ‚a \"out of the box\".\n",
    "    *   Kod nie moÅ¼e byÄ‡ zbyt dynamiczny (skomplikowane `if/else` zaleÅ¼ne od danych mogÄ… powodowaÄ‡ \"Graph Breaks\", co psuje optymalizacjÄ™)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
