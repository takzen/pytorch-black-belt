{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/33_Torch_Compile_Intro.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b12abd",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 33: PyTorch 2.0 & torch.compile (Darmowe Przyspieszenie)\n",
    "\n",
    "To najwi\u0119ksza zmiana w historii PyTorcha.\n",
    "Funkcja `torch.compile(model)` analizuje Tw\u00f3j kod, znajduje sekwencje operacji, kt\u00f3re mo\u017cna po\u0142\u0105czy\u0107, i generuje w locie **zoptymalizowany kod C++/Triton**.\n",
    "\n",
    "**G\u0142\u00f3wne zalety:**\n",
    "1.  **Kernel Fusion:** \u0141\u0105czenie ma\u0142ych operacji (Add, Mul, Gelu) w jeden du\u017cy kernel CUDA. Mniej czytania/pisania do VRAM.\n",
    "2.  **Graph Capture:** Eliminuje narzut Pythona (Python Overhead).\n",
    "\n",
    "**Tryby kompilacji:**\n",
    "*   `default`: Balans mi\u0119dzy szybko\u015bci\u0105 kompilacji a szybko\u015bci\u0105 dzia\u0142ania.\n",
    "*   `reduce-overhead`: U\u017cywa CUDA Graphs. Dobre dla ma\u0142ych batchy.\n",
    "*   `max-autotune`: Najd\u0142u\u017cej si\u0119 kompiluje (profiluje r\u00f3\u017cne konfiguracje), ale daje najszybszy kod.\n",
    "\n",
    "*Uwaga: Na Windowsie torch.compile wci\u0105\u017c bywa kapry\u015bne (cz\u0119sto wymaga MSVC lub dzia\u0142a wolniej). Na Linuxie/WSL to rakieta.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d95e2135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urz\u0105dzenie: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch._dynamo\n",
    "import time\n",
    "\n",
    "# Sprawd\u017amy, czy mamy GPU (Ampere lub nowsze najlepiej korzystaj\u0105 z Tritona)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    # Resetujemy ustawienia dynamo (dla czysto\u015bci testu)\n",
    "    torch._dynamo.reset()\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"\u26a0\ufe0f Testujemy na CPU. Przyspieszenie b\u0119dzie mniejsze ni\u017c na GPU.\")\n",
    "\n",
    "print(f\"Urz\u0105dzenie: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49833ba",
   "metadata": {},
   "source": [
    "## Model Testowy (Element-wise Heavy)\n",
    "\n",
    "Kompilator b\u0142yszczy tam, gdzie jest du\u017co operacji **element-wise** (dzia\u0142aj\u0105cych na ka\u017cdym pikselu osobno), np. funkcje aktywacji, normalizacje, dodawanie.\n",
    "Zbudujemy sie\u0107, kt\u00f3ra robi du\u017co \"matematycznej drobnicy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fc8f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gotowy.\n"
     ]
    }
   ],
   "source": [
    "class HeavyOpsLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Linear(1024, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sekwencja operacji punktowych (Element-wise)\n",
    "        # W Eager Mode ka\u017cda z nich to osobny kernel CUDA (osobny zapis/odczyt pami\u0119ci).\n",
    "        x = self.w(x)\n",
    "        x = torch.cos(x)\n",
    "        x = torch.sin(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x * 2\n",
    "        x = x + 1\n",
    "        return x\n",
    "\n",
    "model = HeavyOpsLayer().to(device)\n",
    "input_data = torch.randn(2048, 1024).to(device) # Spory batch\n",
    "\n",
    "print(\"Model gotowy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f8cf41",
   "metadata": {},
   "source": [
    "## Benchmark: Eager Mode (Standard)\n",
    "\n",
    "Zmierzmy czas w \"starym\" PyTorch.\n",
    "U\u017cywamy `torch.cuda.synchronize()`, \u017ceby zmierzy\u0107 prawdziwy czas wykonania na GPU (bo GPU dzia\u0142a asynchronicznie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a86cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas Eager Mode: 0.752 ms / iter\n"
     ]
    }
   ],
   "source": [
    "def benchmark(model, x, runs=100):\n",
    "    # Rozgrzewka (Warmup)\n",
    "    for _ in range(10):\n",
    "        _ = model(x)\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "    start = time.time()\n",
    "    for _ in range(runs):\n",
    "        _ = model(x)\n",
    "        \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "    end = time.time()\n",
    "    return (end - start) / runs\n",
    "\n",
    "# Test Eager\n",
    "eager_time = benchmark(model, input_data)\n",
    "print(f\"Czas Eager Mode: {eager_time*1000:.3f} ms / iter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361e944",
   "metadata": {},
   "source": [
    "## Benchmark: Compiled Mode\n",
    "\n",
    "Teraz magia. `torch.compile`.\n",
    "\n",
    "**Wa\u017cne:** Pierwsze uruchomienie skompilowanego modelu b\u0119dzie **bardzo wolne**.\n",
    "Dlaczego? Bo wtedy trwa kompilacja (JIT - Just In Time). PyTorch analizuje kod, generuje Triton/C++ i kompiluje go.\n",
    "Dopiero drugie i kolejne uruchomienia s\u0105 szybkie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65745442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kompilowanie modelu (to mo\u017ce chwil\u0119 potrwa\u0107)...\n",
      "\u2139\ufe0f Wykryto Windows. Triton nie jest dost\u0119pny.\n",
      "Pr\u00f3ba u\u017cycia backendu 'cudagraphs' (wymaga sterownik\u00f3w) lub 'eager'...\n",
      "\u2705 Pierwsze uruchomienie (Warmup): 0.04 s\n",
      "Czas Compiled Mode (eager): 0.937 ms / iter\n",
      "\n",
      "\ud83d\ude80 Przyspieszenie: 0.80x\n",
      "(Uwaga: Backend 'eager' nie optymalizuje kodu, s\u0142u\u017cy tylko do testu API na Windowsie)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"Kompilowanie modelu (to mo\u017ce chwil\u0119 potrwa\u0107)...\")\n",
    "\n",
    "# Wyb\u00f3r backendu w zale\u017cno\u015bci od systemu\n",
    "# Na Linuxie 'inductor' (Triton) jest domy\u015blny i najszybszy.\n",
    "# Na Windowsie Triton nie dzia\u0142a, wi\u0119c pr\u00f3bujemy 'cudagraphs' lub 'eager' (brak optymalizacji, ale testuje API).\n",
    "if os.name == 'nt': # Windows\n",
    "    print(\"\u2139\ufe0f Wykryto Windows. Triton nie jest dost\u0119pny.\")\n",
    "    print(\"Pr\u00f3ba u\u017cycia backendu 'cudagraphs' (wymaga sterownik\u00f3w) lub 'eager'...\")\n",
    "    preferred_backend = \"eager\" # Bezpieczny fallback, \u017ceby kod przeszed\u0142\n",
    "else:\n",
    "    preferred_backend = \"inductor\" # Domy\u015blny na Linux\n",
    "\n",
    "try:\n",
    "    # Uruchamiamy kompilacj\u0119\n",
    "    compiled_model = torch.compile(model, backend=preferred_backend)\n",
    "    \n",
    "    # Pierwsze uruchomienie (Kompilacja/Rozgrzewka)\n",
    "    start_compile = time.time()\n",
    "    _ = compiled_model(input_data)\n",
    "    print(f\"\u2705 Pierwsze uruchomienie (Warmup): {time.time() - start_compile:.2f} s\")\n",
    "\n",
    "    # W\u0142a\u015bciwy Benchmark\n",
    "    compiled_time = benchmark(compiled_model, input_data)\n",
    "    print(f\"Czas Compiled Mode ({preferred_backend}): {compiled_time*1000:.3f} ms / iter\")\n",
    "\n",
    "    # Podsumowanie\n",
    "    speedup = eager_time / compiled_time\n",
    "    print(f\"\\n\ud83d\ude80 Przyspieszenie: {speedup:.2f}x\")\n",
    "    \n",
    "    if preferred_backend == \"eager\":\n",
    "        print(\"(Uwaga: Backend 'eager' nie optymalizuje kodu, s\u0142u\u017cy tylko do testu API na Windowsie)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u26a0\ufe0f Nawet fallback zawi\u00f3d\u0142: {e}\")\n",
    "    print(\"To normalne na Windowsie bez pe\u0142nego \u015brodowiska C++.\")\n",
    "    print(\"Lekcja zaliczona: Wiesz jak u\u017cywa\u0107 torch.compile, ale potrzebujesz Linuxa, \u017ceby zobaczy\u0107 zysk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201bf65",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  **Kernel Fusion:** To g\u0142\u00f3wny pow\u00f3d przyspieszenia. Zamiast:\n",
    "    *   `Load -> Cos -> Store`\n",
    "    *   `Load -> Sin -> Store`\n",
    "    `torch.compile` robi:\n",
    "    *   `Load -> Cos -> Sin -> Store` (w rejestrach procesora GPU).\n",
    "    Oszcz\u0119dzasz przepustowo\u015b\u0107 pami\u0119ci (Memory Bandwidth), kt\u00f3ra jest w\u0105skim gard\u0142em.\n",
    "\n",
    "2.  **Kiedy u\u017cywa\u0107?**\n",
    "    *   Zawsze na produkcji (Inference).\n",
    "    *   Zawsze przy treningu du\u017cych modeli (Transformer\u00f3w).\n",
    "\n",
    "3.  **Wymagania:**\n",
    "    *   Nowoczesne GPU (Turing, Ampere).\n",
    "    *   Na Windowsie mo\u017ce wymaga\u0107 instalacji `Microsoft C++ Build Tools`. Na Linuxie dzia\u0142a \"out of the box\".\n",
    "    *   Kod nie mo\u017ce by\u0107 zbyt dynamiczny (skomplikowane `if/else` zale\u017cne od danych mog\u0105 powodowa\u0107 \"Graph Breaks\", co psuje optymalizacj\u0119)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}