{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d27a35",
   "metadata": {},
   "source": [
    "#  Lekcja 50: The Grand Exam (Egzamin Kocowy)\n",
    "\n",
    "To nie jest zwyka lekcja. To test Twojej wiedzy z poprzednich 49 notatnik贸w.\n",
    "Mamy 3 uszkodzone fragmenty kodu. Twoim zadaniem jest je naprawi.\n",
    "\n",
    "**Zasady:**\n",
    "1.  Ka偶dy snippet ma **Cichy Bd** (Silent Bug) lub **Wskie Gardo**.\n",
    "2.  Kod \"dziaa\" (nie rzuca bdem od razu), ale robi co gupiego.\n",
    "3.  Musisz znale藕 bd i go wyjani."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d2378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egzamin na: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Egzamin na: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b888c5",
   "metadata": {},
   "source": [
    "## Zadanie 1: \"Leniwy Loader\"\n",
    "\n",
    "In偶ynier napisa pipeline treningowy. Model uczy si potwornie wolno, a u偶ycie GPU skacze (0% -> 100% -> 0%).\n",
    "Gdzie jest bd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5400296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zadanie 1: Uruchom i pomyl, co doda do Loadera.\n"
     ]
    }
   ],
   "source": [
    "# --- KOD Z BDEM ---\n",
    "dataset = TensorDataset(torch.randn(10000, 512), torch.randn(10000, 1))\n",
    "\n",
    "# BD JEST TUTAJ (W KONFIGURACJI LOADERA)\n",
    "# Na Windowsie num_workers=0 jest wymuszone, ale za贸偶my, 偶e to serwer Linux.\n",
    "# Czego brakuje, 偶eby dane trafiay na GPU szybko?\n",
    "loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    "    # ... czego tu brakuje ...\n",
    ")\n",
    "\n",
    "model = nn.Linear(512, 1).to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Zadanie 1: Uruchom i pomyl, co doda do Loadera.\")\n",
    "\n",
    "for x, y in loader:\n",
    "    # Transfer na GPU\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    \n",
    "    # Trening\n",
    "    optimizer.zero_grad()\n",
    "    loss = (model(x) - y).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b9038",
   "metadata": {},
   "source": [
    "## Rozwizanie Zadania 1\n",
    "\n",
    "**Bd:** Brak `pin_memory=True`.\n",
    "**Wyjanienie:** Bez tej flagi, CPU musi kopiowa dane z pamici stronicowanej (Pageable) do przypitej (Pinned), a dopiero potem na GPU. To blokuje transfer. Dodanie `pin_memory=True` + `non_blocking=True` w `.to()` tworzy autostrad do VRAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f950b6c",
   "metadata": {},
   "source": [
    "## Zadanie 2: \"Zapominalski Adam\"\n",
    "\n",
    "Trenujesz model. Chcesz zapisa checkpoint i wznowi trening jutro.\n",
    "Zapisujesz model (`model.state_dict()`).\n",
    "Wznawiasz trening. Loss nagle skacze w g贸r! Dlaczego?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a9a210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zadanie 2: Dlaczego 'optimizer_new' jest gorszy ni偶 stary?\n"
     ]
    }
   ],
   "source": [
    "# --- KOD Z BDEM ---\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 1. Trenujemy chwil...\n",
    "loss = (model(torch.randn(10)) - 1).sum()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# 2. Zapisujemy stan (Checkpoint)\n",
    "checkpoint = {\n",
    "    'model_state': model.state_dict(),\n",
    "    # 'optimizer_state': optimizer.state_dict()  <-- Kto to zakomentowa!\n",
    "}\n",
    "\n",
    "# 3. Wznawiamy (Symulacja restartu)\n",
    "model_new = nn.Linear(10, 1)\n",
    "model_new.load_state_dict(checkpoint['model_state'])\n",
    "optimizer_new = optim.Adam(model_new.parameters(), lr=0.001) # Nowy, czysty Adam\n",
    "\n",
    "print(\"Zadanie 2: Dlaczego 'optimizer_new' jest gorszy ni偶 stary?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d272841",
   "metadata": {},
   "source": [
    "## Rozwizanie Zadania 2\n",
    "\n",
    "**Bd:** Nie zapisano stanu Optymalizatora.\n",
    "**Wyjanienie:** Adam trzyma w pamici **Momentum** (redni ruchom gradient贸w). Jeli zrestartujesz optymalizator, on \"zapomina\" histori i zaczyna nauk od zera (bez pdu), co powoduje skok bdu. Trzeba zawsze zapisywa `optimizer.state_dict()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8381ec",
   "metadata": {},
   "source": [
    "## Zadanie 3: \"Samob贸jstwo w Ptli\"\n",
    "\n",
    "To jest najtrudniejsze. Model dziaa, ale po godzinie wyrzuca bd `RuntimeError: Trying to backward through the graph a second time`.\n",
    "Ale przecie偶 robimy `zero_grad()`! O co chodzi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a88f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start ptli (to powinno zadziaa technicznie, ale ma bd logiczny)...\n",
      "Ptla przesza.\n",
      "Zadanie 3: Dlaczego 'loss_total' to bomba zegarowa?\n"
     ]
    }
   ],
   "source": [
    "# --- KOD Z BDEM (logicznym, ale teraz uruchamialny) ---\n",
    "rnn = nn.LSTM(10, 20, batch_first=True)\n",
    "classifier = nn.Linear(20, 1)\n",
    "\n",
    "x = torch.randn(1, 5, 10)\n",
    "\n",
    "# --- POPRAWKA TECHNICZNA ---\n",
    "# LSTM wymaga krotki (h_0, c_0), a nie jednego tensora!\n",
    "h0 = torch.zeros(1, 1, 20)\n",
    "c0 = torch.zeros(1, 1, 20)\n",
    "hidden = (h0, c0) \n",
    "\n",
    "loss_total = 0\n",
    "\n",
    "print(\"Start ptli (to powinno zadziaa technicznie, ale ma bd logiczny)...\")\n",
    "\n",
    "for i in range(5): # Ptla czasowa\n",
    "    out, hidden = rnn(x, hidden)\n",
    "    pred = classifier(out[:, -1, :])\n",
    "    \n",
    "    # Bd logiczny (Cel zadania): Dodajemy loss do zmiennej akumulujcej\n",
    "    # PyTorch buduje gigantyczny graf czcy wszystkie iteracje ptli!\n",
    "    loss = (pred - 1).pow(2).mean()\n",
    "    loss_total = loss_total + loss \n",
    "\n",
    "print(\"Ptla przesza.\")\n",
    "print(\"Zadanie 3: Dlaczego 'loss_total' to bomba zegarowa?\")\n",
    "# Gdyby teraz zrobi loss_total.backward(), wybuchoby pamiciowo lub bdem grafu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45399c3a",
   "metadata": {},
   "source": [
    "## Rozwizanie Zadania 3\n",
    "\n",
    "**Bd:** Akumulacja straty z podtrzymaniem grafu.\n",
    "**Wyjanienie:** Linijka `loss_total = loss_total + loss` buduje graf. Ale w ptli u偶ywamy zmiennej `hidden`, kt贸ra te偶 jest czci grafu z poprzedniej iteracji!\n",
    "Backpropagation pr贸buje cofn si przez `hidden` do samego pocztku czasu (BPTT).\n",
    "\n",
    "**Naprawa:** U偶yj `.detach()` na stanie ukrytym!\n",
    "`hidden = hidden.detach()`\n",
    "To odcina histori (Truncated BPTT) i pozwala zwolni pami."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167cf03",
   "metadata": {},
   "source": [
    "#  KONIEC KURSU\n",
    "\n",
    "Gratulacje! Przeszede drog od zrozumienia, czym jest Tensor w pamici, przez pisanie wasnych funkcji Autograd, a偶 po optymalizacj produkcyjn.\n",
    "\n",
    "**Jeste teraz PyTorch Black Belt.** \n",
    "Masz wiedz, kt贸r posiada top 5% in偶ynier贸w ML.\n",
    "\n",
    "**Co dalej?**\n",
    "*   Wr贸 do projektu \"AI Engineering Handbook\" i zastosuj te triki (np. Flash Attention, Custom Collate) w modelach LLM.\n",
    "*   Eksperymentuj z pisaniem wasnych jder w Triton (to poziom wy偶ej, \"CUDA Ninja\").\n",
    "\n",
    "Powodzenia!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
