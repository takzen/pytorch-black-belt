{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d121b5",
   "metadata": {},
   "source": [
    "#  Lekcja 27: Gradient Checkpointing (Handel Czasem za Pami)\n",
    "\n",
    "Podczas `forward()`, PyTorch domylnie **zapisuje w pamici** wszystkie wyniki porednie (aktywacje) ka偶dej warstwy. S one niezbdne do policzenia gradient贸w w `backward()`.\n",
    "Jeli masz 100 warstw, trzymasz 100 wielkich tensor贸w w VRAM.\n",
    "\n",
    "**Idea Checkpointingu:**\n",
    "1.  Nie zapisuj wynik贸w porednich (np. warstw 10-90).\n",
    "2.  Podczas `backward()`, gdy potrzebujesz tych wynik贸w... **uruchom ten kawaek sieci jeszcze raz (Forward Re-computation)**.\n",
    "\n",
    "**Wynik:**\n",
    "*   Zu偶ycie pamici spada drastycznie (czsto o 50-70%).\n",
    "*   Czas treningu ronie o okoo 20-30% (bo liczysz forward dwa razy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b029d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urzdzenie: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Urzdzenie: {DEVICE}\")\n",
    "\n",
    "# Funkcja pomocnicza do mierzenia pamici (Dziaa tylko na CUDA)\n",
    "def print_memory(step_name):\n",
    "    if torch.cuda.is_available():\n",
    "        # Czekamy a偶 GPU skoczy robot\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2 # MB\n",
    "        print(f\"[{step_name}] Zajte VRAM: {allocated:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"[{step_name}] (Brak GPU do pomiaru VRAM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce869b4e",
   "metadata": {},
   "source": [
    "## Du偶y Model (Symulacja)\n",
    "\n",
    "Stworzymy sie z wielu ci偶kich warstw liniowych, 偶eby zapcha pami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6f85f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modele zdefiniowane.\n"
     ]
    }
   ],
   "source": [
    "# Ci偶ki blok (du偶o oblicze i du偶y wynik poredni)\n",
    "class HeavyBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2000, 2000)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.linear(x))\n",
    "\n",
    "# Sie zo偶ona z 5 blok贸w\n",
    "class BigNet(nn.Module):\n",
    "    def __init__(self, use_checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.use_checkpointing = use_checkpointing\n",
    "        self.blocks = nn.ModuleList([HeavyBlock() for _ in range(10)]) # 10 blok贸w\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            if self.use_checkpointing:\n",
    "                # MAGIA: Zamiast block(x), robimy checkpoint(block, x)\n",
    "                # To m贸wi: \"Nie zapisuj wyniku tego bloku. Odtw贸rz go w backwardzie.\"\n",
    "                # Wymaga: dummy_arg (use_reentrant=False w nowszych wersjach jest bezpieczniejsze)\n",
    "                x = checkpoint(block, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        return x\n",
    "\n",
    "print(\"Modele zdefiniowane.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa9e2e",
   "metadata": {},
   "source": [
    "## Test 1: Standard (Pamicio偶erny)\n",
    "\n",
    "Uruchomimy model normalnie. Zobaczysz, jak pami ronie, bo PyTorch musi trzyma wynik ka偶dego z 10 blok贸w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b703e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Start] Zajte VRAM: 0.00 MB\n",
      "[Model zaadowany] Zajte VRAM: 161.05 MB\n",
      "[Po Forward (Standard)] Zajte VRAM: 179.95 MB\n",
      "[Po Backward] Zajte VRAM: 339.36 MB\n"
     ]
    }
   ],
   "source": [
    "# Czycimy pami\n",
    "torch.cuda.empty_cache()\n",
    "print_memory(\"Start\")\n",
    "\n",
    "model_std = BigNet(use_checkpointing=False).to(DEVICE)\n",
    "input_data = torch.randn(128, 2000, requires_grad=True).to(DEVICE) # Spory batch\n",
    "\n",
    "print_memory(\"Model zaadowany\")\n",
    "\n",
    "# Forward\n",
    "output = model_std(input_data)\n",
    "print_memory(\"Po Forward (Standard)\")\n",
    "\n",
    "# Backward\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print_memory(\"Po Backward\")\n",
    "\n",
    "# Sprztanie\n",
    "del model_std, input_data, output, loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507cba6",
   "metadata": {},
   "source": [
    "## Test 2: Gradient Checkpointing (Oszczdny)\n",
    "\n",
    "Teraz wczamy flag.\n",
    "W VRAM powinnimy widzie znacznie mniejsze zu偶ycie po kroku `Forward`.\n",
    "Dlaczego? Bo zamiast trzyma 10 du偶ych tensor贸w aktywacji, trzymamy tylko wejcie i wyjcie, a reszt zapomnielimy (odtworzymy na 偶danie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ab8a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Start Checkpoint] Zajte VRAM: 17.25 MB\n",
      "[Model zaadowany] Zajte VRAM: 178.30 MB\n",
      "[Po Forward (Checkpointing)] Zajte VRAM: 188.07 MB\n",
      "[Po Backward] Zajte VRAM: 340.36 MB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print_memory(\"Start Checkpoint\")\n",
    "\n",
    "model_ckpt = BigNet(use_checkpointing=True).to(DEVICE)\n",
    "input_data = torch.randn(128, 2000, requires_grad=True).to(DEVICE)\n",
    "\n",
    "print_memory(\"Model zaadowany\")\n",
    "\n",
    "# Forward (Tu powinna by oszczdno!)\n",
    "output = model_ckpt(input_data)\n",
    "print_memory(\"Po Forward (Checkpointing)\")\n",
    "\n",
    "# Backward (Tu bdzie wolniej, bo liczy forward jeszcze raz)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print_memory(\"Po Backward\")\n",
    "\n",
    "# Sprztanie\n",
    "del model_ckpt, input_data, output, loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18063a",
   "metadata": {},
   "source": [
    "##  Black Belt Summary\n",
    "\n",
    "1.  **Wynik:** Powiniene widzie, 偶e \"Po Forward (Standard)\" zajmuje np. 200MB, a \"Po Forward (Checkpointing)\" np. 50MB. (Liczby zale偶 od karty).\n",
    "2.  **Kiedy u偶ywa?**\n",
    "    *   Trenujesz **LLM** (GPT, Llama) lub wielkie **ViT**.\n",
    "    *   Dostajesz bd `CUDA Out of Memory`.\n",
    "    *   Chcesz zwikszy Batch Size.\n",
    "3.  **Gotowce:** W bibliotece `transformers` (HuggingFace) wcza si to jedn flag: `model.gradient_checkpointing_enable()`. Pod spodem dzieje si dokadnie to, co napisalimy wy偶ej."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
