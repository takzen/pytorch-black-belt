{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/27_Gradient_Checkpointing.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d121b5",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 27: Gradient Checkpointing (Handel Czasem za Pami\u0119\u0107)\n",
    "\n",
    "Podczas `forward()`, PyTorch domy\u015blnie **zapisuje w pami\u0119ci** wszystkie wyniki po\u015brednie (aktywacje) ka\u017cdej warstwy. S\u0105 one niezb\u0119dne do policzenia gradient\u00f3w w `backward()`.\n",
    "Je\u015bli masz 100 warstw, trzymasz 100 wielkich tensor\u00f3w w VRAM.\n",
    "\n",
    "**Idea Checkpointingu:**\n",
    "1.  Nie zapisuj wynik\u00f3w po\u015brednich (np. warstw 10-90).\n",
    "2.  Podczas `backward()`, gdy potrzebujesz tych wynik\u00f3w... **uruchom ten kawa\u0142ek sieci jeszcze raz (Forward Re-computation)**.\n",
    "\n",
    "**Wynik:**\n",
    "*   Zu\u017cycie pami\u0119ci spada drastycznie (cz\u0119sto o 50-70%).\n",
    "*   Czas treningu ro\u015bnie o oko\u0142o 20-30% (bo liczysz forward dwa razy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b029d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urz\u0105dzenie: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Urz\u0105dzenie: {DEVICE}\")\n",
    "\n",
    "# Funkcja pomocnicza do mierzenia pami\u0119ci (Dzia\u0142a tylko na CUDA)\n",
    "def print_memory(step_name):\n",
    "    if torch.cuda.is_available():\n",
    "        # Czekamy a\u017c GPU sko\u0144czy robot\u0119\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2 # MB\n",
    "        print(f\"[{step_name}] Zaj\u0119te VRAM: {allocated:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"[{step_name}] (Brak GPU do pomiaru VRAM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce869b4e",
   "metadata": {},
   "source": [
    "## Du\u017cy Model (Symulacja)\n",
    "\n",
    "Stworzymy sie\u0107 z wielu ci\u0119\u017ckich warstw liniowych, \u017ceby zapcha\u0107 pami\u0119\u0107."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6f85f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modele zdefiniowane.\n"
     ]
    }
   ],
   "source": [
    "# Ci\u0119\u017cki blok (du\u017co oblicze\u0144 i du\u017cy wynik po\u015bredni)\n",
    "class HeavyBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2000, 2000)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.linear(x))\n",
    "\n",
    "# Sie\u0107 z\u0142o\u017cona z 5 blok\u00f3w\n",
    "class BigNet(nn.Module):\n",
    "    def __init__(self, use_checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.use_checkpointing = use_checkpointing\n",
    "        self.blocks = nn.ModuleList([HeavyBlock() for _ in range(10)]) # 10 blok\u00f3w\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            if self.use_checkpointing:\n",
    "                # MAGIA: Zamiast block(x), robimy checkpoint(block, x)\n",
    "                # To m\u00f3wi: \"Nie zapisuj wyniku tego bloku. Odtw\u00f3rz go w backwardzie.\"\n",
    "                # Wymaga: dummy_arg (use_reentrant=False w nowszych wersjach jest bezpieczniejsze)\n",
    "                x = checkpoint(block, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        return x\n",
    "\n",
    "print(\"Modele zdefiniowane.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa9e2e",
   "metadata": {},
   "source": [
    "## Test 1: Standard (Pami\u0119cio\u017cerny)\n",
    "\n",
    "Uruchomimy model normalnie. Zobaczysz, jak pami\u0119\u0107 ro\u015bnie, bo PyTorch musi trzyma\u0107 wynik ka\u017cdego z 10 blok\u00f3w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b703e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Start] Zaj\u0119te VRAM: 0.00 MB\n",
      "[Model za\u0142adowany] Zaj\u0119te VRAM: 161.05 MB\n",
      "[Po Forward (Standard)] Zaj\u0119te VRAM: 179.95 MB\n",
      "[Po Backward] Zaj\u0119te VRAM: 339.36 MB\n"
     ]
    }
   ],
   "source": [
    "# Czy\u015bcimy pami\u0119\u0107\n",
    "torch.cuda.empty_cache()\n",
    "print_memory(\"Start\")\n",
    "\n",
    "model_std = BigNet(use_checkpointing=False).to(DEVICE)\n",
    "input_data = torch.randn(128, 2000, requires_grad=True).to(DEVICE) # Spory batch\n",
    "\n",
    "print_memory(\"Model za\u0142adowany\")\n",
    "\n",
    "# Forward\n",
    "output = model_std(input_data)\n",
    "print_memory(\"Po Forward (Standard)\")\n",
    "\n",
    "# Backward\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print_memory(\"Po Backward\")\n",
    "\n",
    "# Sprz\u0105tanie\n",
    "del model_std, input_data, output, loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507cba6",
   "metadata": {},
   "source": [
    "## Test 2: Gradient Checkpointing (Oszcz\u0119dny)\n",
    "\n",
    "Teraz w\u0142\u0105czamy flag\u0119.\n",
    "W VRAM powinni\u015bmy widzie\u0107 znacznie mniejsze zu\u017cycie po kroku `Forward`.\n",
    "Dlaczego? Bo zamiast trzyma\u0107 10 du\u017cych tensor\u00f3w aktywacji, trzymamy tylko wej\u015bcie i wyj\u015bcie, a reszt\u0119 zapomnieli\u015bmy (odtworzymy na \u017c\u0105danie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ab8a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Start Checkpoint] Zaj\u0119te VRAM: 17.25 MB\n",
      "[Model za\u0142adowany] Zaj\u0119te VRAM: 178.30 MB\n",
      "[Po Forward (Checkpointing)] Zaj\u0119te VRAM: 188.07 MB\n",
      "[Po Backward] Zaj\u0119te VRAM: 340.36 MB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print_memory(\"Start Checkpoint\")\n",
    "\n",
    "model_ckpt = BigNet(use_checkpointing=True).to(DEVICE)\n",
    "input_data = torch.randn(128, 2000, requires_grad=True).to(DEVICE)\n",
    "\n",
    "print_memory(\"Model za\u0142adowany\")\n",
    "\n",
    "# Forward (Tu powinna by\u0107 oszcz\u0119dno\u015b\u0107!)\n",
    "output = model_ckpt(input_data)\n",
    "print_memory(\"Po Forward (Checkpointing)\")\n",
    "\n",
    "# Backward (Tu b\u0119dzie wolniej, bo liczy forward jeszcze raz)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print_memory(\"Po Backward\")\n",
    "\n",
    "# Sprz\u0105tanie\n",
    "del model_ckpt, input_data, output, loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18063a",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  **Wynik:** Powiniene\u015b widzie\u0107, \u017ce \"Po Forward (Standard)\" zajmuje np. 200MB, a \"Po Forward (Checkpointing)\" np. 50MB. (Liczby zale\u017c\u0105 od karty).\n",
    "2.  **Kiedy u\u017cywa\u0107?**\n",
    "    *   Trenujesz **LLM** (GPT, Llama) lub wielkie **ViT**.\n",
    "    *   Dostajesz b\u0142\u0105d `CUDA Out of Memory`.\n",
    "    *   Chcesz zwi\u0119kszy\u0107 Batch Size.\n",
    "3.  **Gotowce:** W bibliotece `transformers` (HuggingFace) w\u0142\u0105cza si\u0119 to jedn\u0105 flag\u0105: `model.gradient_checkpointing_enable()`. Pod spodem dzieje si\u0119 dok\u0142adnie to, co napisali\u015bmy wy\u017cej."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}