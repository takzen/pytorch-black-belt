{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1edb4cd",
   "metadata": {},
   "source": [
    "# ğŸ¥‹ Lekcja 12: Retain Graph (Å»ycie po Å›mierci grafu)\n",
    "\n",
    "DomyÅ›lny cykl Å¼ycia w PyTorch:\n",
    "1.  **Forward:** Budujemy graf, zapisujemy tensory poÅ›rednie.\n",
    "2.  **Backward:** UÅ¼ywamy grafu do policzenia gradientÃ³w.\n",
    "3.  **Destrukcja:** Graf jest usuwany z pamiÄ™ci (Free Memory).\n",
    "\n",
    "JeÅ›li sprÃ³bujesz zrobiÄ‡ `.backward()` drugi raz na tym samym wyjÅ›ciu, dostaniesz bÅ‚Ä…d, bo \"mapa drogowa\" juÅ¼ nie istnieje.\n",
    "\n",
    "**Kiedy potrzebujemy `retain_graph=True`?**\n",
    "1.  **Multi-Task Learning:** Masz jednÄ… sieÄ‡, ale dwie rÃ³Å¼ne funkcje straty (Loss A i Loss B), ktÃ³re chcesz aplikowaÄ‡ sekwencyjnie.\n",
    "2.  **Wizualizacja:** Chcesz podejrzeÄ‡ gradienty przed wykonaniem \"prawdziwego\" kroku.\n",
    "3.  **GANy:** Czasami przy skomplikowanych pÄ™tlach treningowych Dyskryminatora i Generatora.\n",
    "\n",
    "Zasymulujemy ten bÅ‚Ä…d i go naprawimy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220dece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graf zbudowany.\n",
      "Loss fn: <SumBackward0 object at 0x000001F0FBFB0520>\n",
      "Pierwszy backward: SUKCES\n",
      "\n",
      "ğŸš« BÅÄ„D (Zgodnie z planem):\n",
      "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Prosta sieÄ‡\n",
    "x = torch.randn(1, 10)\n",
    "w = torch.randn(10, 1, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = x @ w\n",
    "loss = y.sum()\n",
    "\n",
    "print(\"Graf zbudowany.\")\n",
    "print(f\"Loss fn: {loss.grad_fn}\")\n",
    "\n",
    "# Pierwszy Backward - Standardowy\n",
    "loss.backward()\n",
    "print(\"Pierwszy backward: SUKCES\")\n",
    "\n",
    "# Drugi Backward - Na tym samym grafie\n",
    "try:\n",
    "    loss.backward()\n",
    "except RuntimeError as e:\n",
    "    print(\"\\nğŸš« BÅÄ„D (Zgodnie z planem):\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3041f",
   "metadata": {},
   "source": [
    "## Scenariusz: Dwie niezaleÅ¼ne straty\n",
    "\n",
    "WyobraÅº sobie, Å¼e trenujesz model, ktÃ³ry ma:\n",
    "1.  Dobrze klasyfikowaÄ‡ obrazki (`Loss_Main`).\n",
    "2.  MieÄ‡ maÅ‚e wagi (Regularyzacja L2 - `Loss_Reg`).\n",
    "\n",
    "Chcesz policzyÄ‡ wpÅ‚yw obu tych strat oddzielnie (np. Å¼eby zalogowaÄ‡ gradienty dla kaÅ¼dej z nich osobno przed zsumowaniem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c960e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamy dwie straty wiszÄ…ce na jednym grafie.\n",
      "Gradient po Loss Main: tensor([-2.3429, -5.5241, -4.7889])... (czÄ™Å›Ä‡)\n",
      "Gradient po obu stratach: tensor([-1.1643, -7.6936, -4.6166])... (suma)\n"
     ]
    }
   ],
   "source": [
    "# Resetujemy wagi i gradienty\n",
    "w = torch.randn(10, 1, requires_grad=True)\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "# 1. Forward\n",
    "y = x @ w\n",
    "\n",
    "# 2. Definiujemy dwie rÃ³Å¼ne straty na podstawie tego samego 'y'\n",
    "loss_main = (y - 1).pow(2).sum()  # Chcemy, Å¼eby wynik byÅ‚ 1\n",
    "loss_reg  = w.pow(2).sum()        # Chcemy, Å¼eby wagi byÅ‚y maÅ‚e\n",
    "\n",
    "print(\"Mamy dwie straty wiszÄ…ce na jednym grafie.\")\n",
    "\n",
    "# 3. Backward dla pierwszej straty\n",
    "# WAÅ»NE: retain_graph=True\n",
    "# MÃ³wimy: \"Policz gradienty dla loss_main, ale NIE NISZCZ grafu (x@w), bo loss_reg teÅ¼ go potrzebuje!\"\n",
    "loss_main.backward(retain_graph=True)\n",
    "\n",
    "print(f\"Gradient po Loss Main: {w.grad.view(-1)[:3]}... (czÄ™Å›Ä‡)\")\n",
    "\n",
    "# 4. Backward dla drugiej straty\n",
    "# Teraz moÅ¼emy zniszczyÄ‡ graf (domyÅ›lnie retain_graph=False)\n",
    "loss_reg.backward()\n",
    "\n",
    "# Gradienty siÄ™ ZSUMOWAÅY (Accumulation)\n",
    "print(f\"Gradient po obu stratach: {w.grad.view(-1)[:3]}... (suma)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000f3db",
   "metadata": {},
   "source": [
    "## `retain_graph` vs `create_graph`\n",
    "\n",
    "To czÄ™ste nieporozumienie na rekrutacjach.\n",
    "\n",
    "1.  **`retain_graph=True`**:\n",
    "    *   \"Nie kasuj buforÃ³w poÅ›rednich po backwardzie\".\n",
    "    *   Potrzebne, gdy robisz **wiele backwardÃ³w na tym samym forwardzie**.\n",
    "\n",
    "2.  **`create_graph=True`**:\n",
    "    *   \"Traktuj proces liczenia gradientu jako operacjÄ™, ktÃ³rÄ… teÅ¼ moÅ¼na rÃ³Å¼niczkowaÄ‡\".\n",
    "    *   Buduje graf pochodnej.\n",
    "    *   Potrzebne do **pochodnych wyÅ¼szego rzÄ™du** (Hessian, MAML - notatnik 11 i 75).\n",
    "\n",
    "**PrzykÅ‚ad:** Czy `retain_graph` zuÅ¼ywa duÅ¼o pamiÄ™ci?\n",
    "Tak! Trzyma w VRAM caÅ‚Ä… historiÄ™ aktywacji. Dlatego uÅ¼ywaj tego tylko wtedy, gdy musisz. W 99% przypadkÃ³w lepiej zsumowaÄ‡ straty (`total_loss = loss1 + loss2`) i zrobiÄ‡ jeden `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c96ef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PamiÄ™Ä‡ zwolniona.\n"
     ]
    }
   ],
   "source": [
    "# TEST PAMIÄ˜CI (Zrozumienie ryzyka)\n",
    "\n",
    "# DuÅ¼y tensor\n",
    "huge = torch.randn(1000, 1000, requires_grad=True)\n",
    "y = huge * 2\n",
    "loss = y.sum()\n",
    "\n",
    "# Backward z zatrzymaniem grafu\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# Tutaj graf (i tensor 'huge' w pamiÄ™ci grafu) NADAL WISI w RAM.\n",
    "# Dopiero gdy zrobimy kolejny backward bez retain, albo usuniemy zmiennÄ…, pamiÄ™Ä‡ zostanie zwolniona.\n",
    "\n",
    "loss.backward(retain_graph=False) \n",
    "# Teraz graf posprzÄ…tany.\n",
    "print(\"PamiÄ™Ä‡ zwolniona.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f257a81",
   "metadata": {},
   "source": [
    "## ğŸ¥‹ Black Belt Summary\n",
    "\n",
    "1.  DomyÅ›lnie PyTorch jest **agresywny w sprzÄ…taniu**. Po `.backward()` bufory znikajÄ….\n",
    "2.  UÅ¼ywaj `retain_graph=True` **TYLKO** wtedy, gdy musisz wywoÅ‚aÄ‡ `.backward()` wielokrotnie na tym samym pod-grafie.\n",
    "3.  **Alternatywa:** Zamiast robiÄ‡ dwa backwardy:\n",
    "    ```python\n",
    "    loss1.backward(retain_graph=True)\n",
    "    loss2.backward()\n",
    "    ```\n",
    "    Zazwyczaj lepiej (szybciej i lÅ¼ej dla pamiÄ™ci) jest zrobiÄ‡:\n",
    "    ```python\n",
    "    total_loss = loss1 + loss2\n",
    "    total_loss.backward()\n",
    "    ```\n",
    "    Wtedy PyTorch sam ogarnie graf raz, a porzÄ…dnie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
