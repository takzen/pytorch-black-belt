{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c2ea86",
   "metadata": {},
   "source": [
    "#  Lekcja 8: Anatomia Grafu Obliczeniowego (DAG)\n",
    "\n",
    "Kiedy wykonujesz operacje na tensorach z `requires_grad=True`, PyTorch nie tylko liczy wynik.\n",
    "Buduje w pamici drzewo (graf), kt贸re zapamituje **histori operacji**.\n",
    "\n",
    "**Kluczowe pojcia:**\n",
    "1.  **Leaf Node (Li):** Tensor stworzony przez u偶ytkownika (np. Wagi, Dane). Nie ma historii (`grad_fn` jest None).\n",
    "2.  **Root Node (Korze):** Wynik kocowy (np. Loss).\n",
    "3.  **`grad_fn`:** Funkcja odwrotna. Jeli zrobie mno偶enie (`Mul`), w grafie powstaje wze `MulBackward0`, kt贸ry wie, jak policzy pochodn mno偶enia.\n",
    "\n",
    "W tej lekcji przeledzimy ten graf rcznie, cofajc si od Wyniku do Wejcia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9efc9ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czy 'a' jest liciem? True\n",
      "Czy 'a' ma grad_fn? None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. TWORZYMY LICIE (Wejcie)\n",
    "# requires_grad=True oznacza: \"Zacznij ledzi histori od tego momentu\"\n",
    "a = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "print(f\"Czy 'a' jest liciem? {a.is_leaf}\")\n",
    "print(f\"Czy 'a' ma grad_fn? {a.grad_fn}\")  # None, bo to pocztek historii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20128f",
   "metadata": {},
   "source": [
    "## Budowanie Grafu (Forward Pass)\n",
    "\n",
    "Wykonajmy proste dziaanie:\n",
    "$$ c = a \\times b $$\n",
    "$$ d = c + 5 $$\n",
    "$$ out = d \\times 2 $$\n",
    "\n",
    "Ka偶da z tych operacji dodaje nowy wze do grafu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0db52ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: tensor([6.], grad_fn=<MulBackward0>)\n",
      "Funkcja tworzca c: <MulBackward0 object at 0x00000200F474DAE0>\n",
      "Funkcja tworzca d: <AddBackward0 object at 0x00000200F474DAE0>\n",
      "Funkcja tworzca out: <MulBackward0 object at 0x00000200F474DAE0>\n"
     ]
    }
   ],
   "source": [
    "# Krok 1: Mno偶enie\n",
    "c = a * b\n",
    "print(f\"c: {c}\")\n",
    "print(f\"Funkcja tworzca c: {c.grad_fn}\") \n",
    "# MulBackward0 -> M贸wi: \"Powstaem z mno偶enia\"\n",
    "\n",
    "# Krok 2: Dodawanie\n",
    "d = c + 5\n",
    "print(f\"Funkcja tworzca d: {d.grad_fn}\")\n",
    "# AddBackward0\n",
    "\n",
    "# Krok 3: Wynik kocowy\n",
    "out = d * 2\n",
    "print(f\"Funkcja tworzca out: {out.grad_fn}\")\n",
    "# MulBackward0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770d2f1",
   "metadata": {},
   "source": [
    "## Spacer po Grafie (Traversing the Graph)\n",
    "\n",
    "Skoro `out` wie, 偶e powsta z mno偶enia `d * 2`, to musi mie wska藕nik do `d`.\n",
    "Mo偶emy u偶y metody `.next_functions`, 偶eby rcznie cofn si w historii a偶 do `a` i `b`.\n",
    "\n",
    "To jest dokadnie to, co robi silnik Autograd podczas `backward()`, tylko my zrobimy to \"na piechot\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbe7659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LEDZTWO WSTECZNE ---\n",
      "KROK 0 (Out): <MulBackward0 object at 0x00000200F474E710>\n",
      "KROK 1 (Rodzice Out): ((<AddBackward0 object at 0x00000200F474D450>, 0), (None, 0))\n",
      "KROK 2 (Rodzice d): ((<MulBackward0 object at 0x00000200F474E710>, 0), (None, 0))\n",
      "KROK 3 (Rodzice c): ((<AccumulateGrad object at 0x00000200F474F370>, 0), (<AccumulateGrad object at 0x00000200F474E320>, 0))\n"
     ]
    }
   ],
   "source": [
    "print(\"--- LEDZTWO WSTECZNE ---\")\n",
    "\n",
    "# Krok 0: Jestemy w 'out'\n",
    "print(f\"KROK 0 (Out): {out.grad_fn}\")\n",
    "\n",
    "# Krok 1: Z czego powsta out?\n",
    "# next_functions zwraca list krotek (funkcja, indeks)\n",
    "parents = out.grad_fn.next_functions\n",
    "print(f\"KROK 1 (Rodzice Out): {parents}\")\n",
    "# Widzimy AddBackward0 (to jest nasze 'd'). \n",
    "# Drugi element to None (bo mno偶ylimy przez sta '2', kt贸ra nie ma historii)\n",
    "\n",
    "# Wycigamy funkcj, kt贸ra stworzya 'd'\n",
    "d_fn = parents[0][0]\n",
    "\n",
    "# Krok 2: Z czego powstao d?\n",
    "grandparents = d_fn.next_functions\n",
    "print(f\"KROK 2 (Rodzice d): {grandparents}\")\n",
    "# Widzimy MulBackward0 (to jest nasze 'c')\n",
    "\n",
    "# Wycigamy funkcj, kt贸ra stworzya 'c'\n",
    "c_fn = grandparents[0][0]\n",
    "\n",
    "# Krok 3: Z czego powstao c?\n",
    "great_grandparents = c_fn.next_functions\n",
    "print(f\"KROK 3 (Rodzice c): {great_grandparents}\")\n",
    "# Widzimy dwa obiekty AccumulateGrad.\n",
    "# AccumulateGrad to \"opakowanie\" na nasze Licie (a i b).\n",
    "# To tutaj gromadz si gradienty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2db79",
   "metadata": {},
   "source": [
    "## Wizualizacja Wasna (ASCII Tree)\n",
    "\n",
    "Zamiast polega na zewntrznych bibliotekach (jak `torchviz`, kt贸ry czsto sprawia problemy na Windowsie przez brak plik贸w systemowych), zachowamy si jak in偶ynierowie i **napiszemy wasne narzdzie**.\n",
    "\n",
    "Stworzymy prost funkcj rekurencyjn, kt贸ra przejdzie po grafie (u偶ywajc atrybutu `next_functions`, kt贸ry odkrylimy wczeniej) i wypisze go w formie drzewa tekstowego.\n",
    "\n",
    "To rozwizanie jest:\n",
    "1.  **Lekkie:** Czysty Python, zero zale偶noci.\n",
    "2.  **Niezawodne:** Zadziaa zawsze, nawet na serwerze bez graficznego interfejsu.\n",
    "3.  **Edukacyjne:** Poka偶e dokadnie struktur `Mul` -> `Add` -> `Leaf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a6ce09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WIZUALIZACJA GRAFU (ASCII) ---\n",
      "ROOT (Wynik kocowy)\n",
      "★ MulBackward0\n",
      "    ★ AddBackward0\n",
      "        ★ MulBackward0\n",
      "            ★ AccumulateGrad\n",
      "                 To jest LI (Leaf Node) - np. Wagi\n",
      "            ★ AccumulateGrad\n",
      "                 To jest LI (Leaf Node) - np. Wagi\n",
      "         (Staa / Brak historii)\n",
      "     (Staa / Brak historii)\n"
     ]
    }
   ],
   "source": [
    "# Zamiast polega na zewntrznym programie, napiszmy wasn funkcj rekurencyjn.\n",
    "# To przechodzi po grafie i rysuje go w konsoli.\n",
    "\n",
    "def print_graph(grad_fn, level=0):\n",
    "    # Wcicie dla wizualizacji poziomu\n",
    "    indent = \"    \" * level\n",
    "    \n",
    "    # Nazwa funkcji (np. MulBackward0)\n",
    "    name = grad_fn.__class__.__name__\n",
    "    \n",
    "    print(f\"{indent}★ {name}\")\n",
    "    \n",
    "    # Jeli funkcja ma \"rodzic贸w\" (next_functions), idziemy gbiej\n",
    "    if hasattr(grad_fn, 'next_functions'):\n",
    "        for parent, _ in grad_fn.next_functions:\n",
    "            if parent is not None:\n",
    "                print_graph(parent, level + 1)\n",
    "            else:\n",
    "                # To oznacza, 偶e dotarlimy do staej lub tensora bez gradientu\n",
    "                print(f\"{indent}     (Staa / Brak historii)\")\n",
    "    \n",
    "    # Jeli to AccumulateGrad, to znaczy, 偶e dotarlimy do Licia (Zmiennej)\n",
    "    if \"AccumulateGrad\" in name:\n",
    "        # Mo偶emy spr贸bowa wycign nazw zmiennej, jeli j ma\n",
    "        print(f\"{indent}     To jest LI (Leaf Node) - np. Wagi\")\n",
    "\n",
    "print(\"--- WIZUALIZACJA GRAFU (ASCII) ---\")\n",
    "print(\"ROOT (Wynik kocowy)\")\n",
    "print_graph(out.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2cbe70",
   "metadata": {},
   "source": [
    "##  Black Belt Summary\n",
    "\n",
    "1.  **Graf jest dynamiczny:** Graf powstaje w momencie wykonywania operacji (np. `+`, `*`). Jeli w kodzie masz `if x > 0: y = x * 2`, to graf zmienia sw贸j ksztat w ka偶dej iteracji (Define-by-Run).\n",
    "2.  **`grad_fn` to mapa:** Ka偶dy tensor (opr贸cz lici) ma w plecaku map, jak wr贸ci do domu.\n",
    "3.  **Licie (`is_leaf`):** To parametry (Wagi), kt贸re chcemy aktualizowa. Tylko dla nich PyTorch gromadzi `.grad`.\n",
    "\n",
    "W nastpnej lekcji zobaczymy, jak manipulowa t histori (odcinanie grafu i tryby inferencji)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
