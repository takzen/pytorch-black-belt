{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/03_Einsum_Is_All_You_Need.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477cff1",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 3: Einsum (Jeden by wszystkimi rz\u0105dzi\u0107)\n",
    "\n",
    "Wi\u0119kszo\u015b\u0107 operacji w PyTorch (`sum`, `transpose`, `mm`, `bmm`) to tylko specjalne przypadki **Konwencji Sumacyjnej Einsteina**.\n",
    "\n",
    "Funkcja `torch.einsum('wz\u00f3r', a, b)` pozwala zdefiniowa\u0107 operacj\u0119 za pomoc\u0105 indeks\u00f3w literowych.\n",
    "\n",
    "**Zasady Gry:**\n",
    "1.  Ka\u017cdy wymiar oznaczamy liter\u0105 (np. `i`, `j`, `k`).\n",
    "2.  **Po lewej stronie strza\u0142ki (`->`):** Nazywamy wymiary wej\u015bciowe.\n",
    "3.  **Po prawej stronie strza\u0142ki:** Nazywamy wymiary wyj\u015bciowe.\n",
    "    *   Je\u015bli litera znikn\u0119\u0142a -> **Sumujemy** po tym wymiarze.\n",
    "    *   Je\u015bli litery zmieni\u0142y kolejno\u015b\u0107 -> **Transponujemy**.\n",
    "    *   Je\u015bli litera si\u0119 powtarza w wej\u015bciu (np. `i, i`) -> **Mno\u017cymy** elementy (Hadamard).\n",
    "\n",
    "To brzmi abstrakcyjnie, ale w praktyce jest genialnie proste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974a2126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: torch.Size([2, 3])\n",
      "B shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Dane testowe\n",
    "A = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]]) # (2, 3) -> i, j\n",
    "\n",
    "B = torch.tensor([[7, 8, 9], \n",
    "                  [10, 11, 12]]) # (2, 3) -> i, j\n",
    "\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e31507",
   "metadata": {},
   "source": [
    "## Poziom 1: Podstawy (Suma i Transpozycja)\n",
    "\n",
    "Zapiszmy proste operacje w j\u0119zyku Einsum.\n",
    "\n",
    "1.  **Transpozycja:** Zamieniamy wiersze (`i`) z kolumnami (`j`).\n",
    "    *   Wz\u00f3r: `ij -> ji`\n",
    "2.  **Suma wszystkich element\u00f3w:** Wszystkie wymiary znikaj\u0105.\n",
    "    *   Wz\u00f3r: `ij ->` (pusty wynik oznacza skalar)\n",
    "3.  **Suma po kolumnach:** Sumujemy wymiar `j`, zostaje `i`.\n",
    "    *   Wz\u00f3r: `ij -> i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212dfcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Transpozycja (ij -> ji) ---\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "\n",
      "--- Suma Ca\u0142kowita (ij -> ) ---\n",
      "21\n",
      "\n",
      "--- Suma po wierszach (ij -> i) ---\n",
      "tensor([ 6, 15])\n"
     ]
    }
   ],
   "source": [
    "# 1. Transpozycja (A.T)\n",
    "# i=2, j=3 -> j=3, i=2\n",
    "transposed = torch.einsum('ij -> ji', A)\n",
    "print(\"--- Transpozycja (ij -> ji) ---\")\n",
    "print(transposed)\n",
    "\n",
    "# 2. Suma ca\u0142kowita (torch.sum(A))\n",
    "# i, j znikaj\u0105 -> sumujemy po obu\n",
    "total_sum = torch.einsum('ij ->', A)\n",
    "print(f\"\\n--- Suma Ca\u0142kowita (ij -> ) ---\\n{total_sum}\")\n",
    "\n",
    "# 3. Suma wierszy (torch.sum(A, dim=1))\n",
    "# j znika -> sumujemy po j (kolumnach), zostaj\u0105 wiersze i\n",
    "row_sum = torch.einsum('ij -> i', A)\n",
    "print(f\"\\n--- Suma po wierszach (ij -> i) ---\\n{row_sum}\")\n",
    "# 1+2+3=6, 4+5+6=15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd344c",
   "metadata": {},
   "source": [
    "## Poziom 2: Mno\u017cenie Macierzy (Matrix Multiplication)\n",
    "\n",
    "To tutaj `einsum` b\u0142yszczy. Klasyczne mno\u017cenie macierzy $A \\times C$.\n",
    "*   $A$: (2, 3) -> `ik`\n",
    "*   $C$: (3, 5) -> `kj`\n",
    "*   Wynik: (2, 5) -> `ij`\n",
    "\n",
    "Wymiar `k` (wewn\u0119trzny) znika, wi\u0119c po nim sumujemy. To definicja mno\u017cenia macierzy.\n",
    "Wz\u00f3r: `ik, kj -> ij`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dffae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Matrix Multiplication ---\n",
      "Kszta\u0142t wyniku: torch.Size([2, 5])\n",
      "Czy identyczne? True\n"
     ]
    }
   ],
   "source": [
    "# Nowa macierz C (3 wiersze, 5 kolumn)\n",
    "C = torch.randn(3, 5)\n",
    "\n",
    "# Tradycyjne mno\u017cenie (mm)\n",
    "res_mm = torch.mm(A.float(), C)\n",
    "\n",
    "# Einsum\n",
    "# i=2, k=3 | k=3, j=5 -> i=2, j=5\n",
    "res_ein = torch.einsum('ik, kj -> ij', A.float(), C)\n",
    "\n",
    "print(\"--- Matrix Multiplication ---\")\n",
    "print(f\"Kszta\u0142t wyniku: {res_ein.shape}\")\n",
    "\n",
    "# Sprawd\u017amy czy to to samo\n",
    "print(f\"Czy identyczne? {torch.allclose(res_mm, res_ein)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf168160",
   "metadata": {},
   "source": [
    "## Poziom 3: Batch Matrix Multiplication (BMM)\n",
    "\n",
    "To operacja, kt\u00f3r\u0105 wykonuje ka\u017cdy Transformer (GPT) miliardy razy.\n",
    "Mamy Batch (`b`) macierzy. Chcemy pomno\u017cy\u0107 ka\u017cd\u0105 macierz z batcha przez odpowiadaj\u0105c\u0105 jej macierz z drugiego batcha.\n",
    "\n",
    "*   Input 1: `(b, i, k)`\n",
    "*   Input 2: `(b, k, j)`\n",
    "*   Output: `(b, i, j)`\n",
    "\n",
    "Tradycyjnie: `torch.bmm`.\n",
    "Einsum: `bik, bkj -> bij` (Po prostu dodajemy `b` na pocz\u0105tku, kt\u00f3re \"przechodzi dalej\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e03c9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch Matrix Multiplication ---\n",
      "Czy identyczne? True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "i, k, j = 20, 30, 40\n",
    "\n",
    "X = torch.randn(batch_size, i, k)\n",
    "Y = torch.randn(batch_size, k, j)\n",
    "\n",
    "# Tradycyjne BMM\n",
    "res_bmm = torch.bmm(X, Y)\n",
    "\n",
    "# Einsum BMM\n",
    "# b przechodzi bez zmian. k znika (sumowanie).\n",
    "res_ein_bmm = torch.einsum('bik, bkj -> bij', X, Y)\n",
    "\n",
    "print(\"--- Batch Matrix Multiplication ---\")\n",
    "print(f\"Czy identyczne? {torch.allclose(res_bmm, res_ein_bmm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75905cb",
   "metadata": {},
   "source": [
    "## Boss Level: Attention Mechanism\n",
    "\n",
    "Wz\u00f3r na Attention to:\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V $$\n",
    "\n",
    "W kodzie PyTorch (bez einsum) to koszmar z wymiarami:\n",
    "`Q` ma kszta\u0142t `(Batch, Heads, Seq, Dim)`.\n",
    "\u017beby pomno\u017cy\u0107 $Q$ i $K^T$, musimy robi\u0107 transpozycje, uwa\u017ca\u0107 na wymiary g\u0142\u00f3w...\n",
    "\n",
    "Z `einsum` to jedna linijka.\n",
    "*   $Q$: `bhqd` (batch, heads, query_len, dim)\n",
    "*   $K$: `bhkd` (batch, heads, key_len, dim)\n",
    "*   Wynik ($Q K^T$): `bhqk` (batch, heads, query_len, key_len)\n",
    "\n",
    "Wymiar `d` znika (sumujemy po nim - iloczyn skalarny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "042cdcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([2, 4, 8, 16])\n",
      "K shape: torch.Size([2, 4, 8, 16])\n",
      "\n",
      "--- ATTENTION SCORES ---\n",
      "Wynik shape: torch.Size([2, 4, 8, 8])\n",
      "Czy identyczne? True\n",
      "Context shape: torch.Size([2, 4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# Symulacja danych do Attention\n",
    "batch = 2\n",
    "heads = 4\n",
    "seq_len = 8\n",
    "dim = 16\n",
    "\n",
    "Q = torch.randn(batch, heads, seq_len, dim)\n",
    "K = torch.randn(batch, heads, seq_len, dim)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "\n",
    "# Tradycyjnie (b\u00f3l g\u0142owy)\n",
    "# Musimy transponowa\u0107 K na (batch, heads, dim, seq_len) przed mno\u017ceniem\n",
    "scores_manual = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "# EINSUM (Czysta poezja)\n",
    "# bhqd - Q\n",
    "# bhkd - K (u\u017cywamy 'k' zamiast 'q' dla d\u0142ugo\u015bci, cho\u0107 tu s\u0105 r\u00f3wne)\n",
    "# Wynik ma by\u0107 macierz\u0105 podobie\u0144stwa query-to-key: bhqk\n",
    "scores_einsum = torch.einsum('bhqd, bhkd -> bhqk', Q, K)\n",
    "\n",
    "print(\"\\n--- ATTENTION SCORES ---\")\n",
    "print(f\"Wynik shape: {scores_einsum.shape}\")\n",
    "print(f\"Czy identyczne? {torch.allclose(scores_manual, scores_einsum)}\")\n",
    "\n",
    "# A teraz mno\u017cenie przez V (Value)\n",
    "V = torch.randn(batch, heads, seq_len, dim)\n",
    "\n",
    "# Wynik Attention * V\n",
    "# Scores: bhqk\n",
    "# V:      bhkd (k to ten sam wymiar co w scores - length)\n",
    "# Wynik:  bhqd (wracamy do oryginalnego kszta\u0142tu)\n",
    "# Sumujemy po 'k'\n",
    "context = torch.einsum('bhqk, bhkd -> bhqd', scores_einsum, V)\n",
    "\n",
    "print(f\"Context shape: {context.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8921b07",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "Dlaczego warto u\u017cywa\u0107 `einsum`?\n",
    "1.  **Czytelno\u015b\u0107:** Wida\u0107 dok\u0142adnie, kt\u00f3re wymiary s\u0105 mno\u017cone, a kt\u00f3re sumowane. Nie musisz zgadywa\u0107, co robi `transpose(1, 2)`.\n",
    "2.  **Bezpiecze\u0144stwo:** Nie musisz robi\u0107 `reshape` ani `view`, co jest cz\u0119stym \u017ar\u00f3d\u0142em b\u0142\u0119d\u00f3w w wymiarach.\n",
    "3.  **Wydajno\u015b\u0107:** PyTorch kompiluje `einsum` do zoptymalizowanych j\u0105der CUDA (cz\u0119sto \u0142\u0105czy operacje, np. transpozycj\u0119 i mno\u017cenie w jednym kroku).\n",
    "\n",
    "**Zasada kciuka:**\n",
    "Je\u015bli masz w kodzie wi\u0119cej ni\u017c jeden `.permute()` lub `.transpose()` przed mno\u017ceniem -> zamie\u0144 to na `einsum`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}