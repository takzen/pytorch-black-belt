{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7477cff1",
   "metadata": {},
   "source": [
    "#  Lekcja 3: Einsum (Jeden by wszystkimi rzdzi)\n",
    "\n",
    "Wikszo operacji w PyTorch (`sum`, `transpose`, `mm`, `bmm`) to tylko specjalne przypadki **Konwencji Sumacyjnej Einsteina**.\n",
    "\n",
    "Funkcja `torch.einsum('wz贸r', a, b)` pozwala zdefiniowa operacj za pomoc indeks贸w literowych.\n",
    "\n",
    "**Zasady Gry:**\n",
    "1.  Ka偶dy wymiar oznaczamy liter (np. `i`, `j`, `k`).\n",
    "2.  **Po lewej stronie strzaki (`->`):** Nazywamy wymiary wejciowe.\n",
    "3.  **Po prawej stronie strzaki:** Nazywamy wymiary wyjciowe.\n",
    "    *   Jeli litera znikna -> **Sumujemy** po tym wymiarze.\n",
    "    *   Jeli litery zmieniy kolejno -> **Transponujemy**.\n",
    "    *   Jeli litera si powtarza w wejciu (np. `i, i`) -> **Mno偶ymy** elementy (Hadamard).\n",
    "\n",
    "To brzmi abstrakcyjnie, ale w praktyce jest genialnie proste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974a2126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: torch.Size([2, 3])\n",
      "B shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Dane testowe\n",
    "A = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]]) # (2, 3) -> i, j\n",
    "\n",
    "B = torch.tensor([[7, 8, 9], \n",
    "                  [10, 11, 12]]) # (2, 3) -> i, j\n",
    "\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e31507",
   "metadata": {},
   "source": [
    "## Poziom 1: Podstawy (Suma i Transpozycja)\n",
    "\n",
    "Zapiszmy proste operacje w jzyku Einsum.\n",
    "\n",
    "1.  **Transpozycja:** Zamieniamy wiersze (`i`) z kolumnami (`j`).\n",
    "    *   Wz贸r: `ij -> ji`\n",
    "2.  **Suma wszystkich element贸w:** Wszystkie wymiary znikaj.\n",
    "    *   Wz贸r: `ij ->` (pusty wynik oznacza skalar)\n",
    "3.  **Suma po kolumnach:** Sumujemy wymiar `j`, zostaje `i`.\n",
    "    *   Wz贸r: `ij -> i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212dfcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Transpozycja (ij -> ji) ---\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "\n",
      "--- Suma Cakowita (ij -> ) ---\n",
      "21\n",
      "\n",
      "--- Suma po wierszach (ij -> i) ---\n",
      "tensor([ 6, 15])\n"
     ]
    }
   ],
   "source": [
    "# 1. Transpozycja (A.T)\n",
    "# i=2, j=3 -> j=3, i=2\n",
    "transposed = torch.einsum('ij -> ji', A)\n",
    "print(\"--- Transpozycja (ij -> ji) ---\")\n",
    "print(transposed)\n",
    "\n",
    "# 2. Suma cakowita (torch.sum(A))\n",
    "# i, j znikaj -> sumujemy po obu\n",
    "total_sum = torch.einsum('ij ->', A)\n",
    "print(f\"\\n--- Suma Cakowita (ij -> ) ---\\n{total_sum}\")\n",
    "\n",
    "# 3. Suma wierszy (torch.sum(A, dim=1))\n",
    "# j znika -> sumujemy po j (kolumnach), zostaj wiersze i\n",
    "row_sum = torch.einsum('ij -> i', A)\n",
    "print(f\"\\n--- Suma po wierszach (ij -> i) ---\\n{row_sum}\")\n",
    "# 1+2+3=6, 4+5+6=15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd344c",
   "metadata": {},
   "source": [
    "## Poziom 2: Mno偶enie Macierzy (Matrix Multiplication)\n",
    "\n",
    "To tutaj `einsum` byszczy. Klasyczne mno偶enie macierzy $A \\times C$.\n",
    "*   $A$: (2, 3) -> `ik`\n",
    "*   $C$: (3, 5) -> `kj`\n",
    "*   Wynik: (2, 5) -> `ij`\n",
    "\n",
    "Wymiar `k` (wewntrzny) znika, wic po nim sumujemy. To definicja mno偶enia macierzy.\n",
    "Wz贸r: `ik, kj -> ij`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dffae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Matrix Multiplication ---\n",
      "Ksztat wyniku: torch.Size([2, 5])\n",
      "Czy identyczne? True\n"
     ]
    }
   ],
   "source": [
    "# Nowa macierz C (3 wiersze, 5 kolumn)\n",
    "C = torch.randn(3, 5)\n",
    "\n",
    "# Tradycyjne mno偶enie (mm)\n",
    "res_mm = torch.mm(A.float(), C)\n",
    "\n",
    "# Einsum\n",
    "# i=2, k=3 | k=3, j=5 -> i=2, j=5\n",
    "res_ein = torch.einsum('ik, kj -> ij', A.float(), C)\n",
    "\n",
    "print(\"--- Matrix Multiplication ---\")\n",
    "print(f\"Ksztat wyniku: {res_ein.shape}\")\n",
    "\n",
    "# Sprawd藕my czy to to samo\n",
    "print(f\"Czy identyczne? {torch.allclose(res_mm, res_ein)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf168160",
   "metadata": {},
   "source": [
    "## Poziom 3: Batch Matrix Multiplication (BMM)\n",
    "\n",
    "To operacja, kt贸r wykonuje ka偶dy Transformer (GPT) miliardy razy.\n",
    "Mamy Batch (`b`) macierzy. Chcemy pomno偶y ka偶d macierz z batcha przez odpowiadajc jej macierz z drugiego batcha.\n",
    "\n",
    "*   Input 1: `(b, i, k)`\n",
    "*   Input 2: `(b, k, j)`\n",
    "*   Output: `(b, i, j)`\n",
    "\n",
    "Tradycyjnie: `torch.bmm`.\n",
    "Einsum: `bik, bkj -> bij` (Po prostu dodajemy `b` na pocztku, kt贸re \"przechodzi dalej\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e03c9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch Matrix Multiplication ---\n",
      "Czy identyczne? True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "i, k, j = 20, 30, 40\n",
    "\n",
    "X = torch.randn(batch_size, i, k)\n",
    "Y = torch.randn(batch_size, k, j)\n",
    "\n",
    "# Tradycyjne BMM\n",
    "res_bmm = torch.bmm(X, Y)\n",
    "\n",
    "# Einsum BMM\n",
    "# b przechodzi bez zmian. k znika (sumowanie).\n",
    "res_ein_bmm = torch.einsum('bik, bkj -> bij', X, Y)\n",
    "\n",
    "print(\"--- Batch Matrix Multiplication ---\")\n",
    "print(f\"Czy identyczne? {torch.allclose(res_bmm, res_ein_bmm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75905cb",
   "metadata": {},
   "source": [
    "## Boss Level: Attention Mechanism\n",
    "\n",
    "Wz贸r na Attention to:\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V $$\n",
    "\n",
    "W kodzie PyTorch (bez einsum) to koszmar z wymiarami:\n",
    "`Q` ma ksztat `(Batch, Heads, Seq, Dim)`.\n",
    "呕eby pomno偶y $Q$ i $K^T$, musimy robi transpozycje, uwa偶a na wymiary g贸w...\n",
    "\n",
    "Z `einsum` to jedna linijka.\n",
    "*   $Q$: `bhqd` (batch, heads, query_len, dim)\n",
    "*   $K$: `bhkd` (batch, heads, key_len, dim)\n",
    "*   Wynik ($Q K^T$): `bhqk` (batch, heads, query_len, key_len)\n",
    "\n",
    "Wymiar `d` znika (sumujemy po nim - iloczyn skalarny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "042cdcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([2, 4, 8, 16])\n",
      "K shape: torch.Size([2, 4, 8, 16])\n",
      "\n",
      "--- ATTENTION SCORES ---\n",
      "Wynik shape: torch.Size([2, 4, 8, 8])\n",
      "Czy identyczne? True\n",
      "Context shape: torch.Size([2, 4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# Symulacja danych do Attention\n",
    "batch = 2\n",
    "heads = 4\n",
    "seq_len = 8\n",
    "dim = 16\n",
    "\n",
    "Q = torch.randn(batch, heads, seq_len, dim)\n",
    "K = torch.randn(batch, heads, seq_len, dim)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "\n",
    "# Tradycyjnie (b贸l gowy)\n",
    "# Musimy transponowa K na (batch, heads, dim, seq_len) przed mno偶eniem\n",
    "scores_manual = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "# EINSUM (Czysta poezja)\n",
    "# bhqd - Q\n",
    "# bhkd - K (u偶ywamy 'k' zamiast 'q' dla dugoci, cho tu s r贸wne)\n",
    "# Wynik ma by macierz podobiestwa query-to-key: bhqk\n",
    "scores_einsum = torch.einsum('bhqd, bhkd -> bhqk', Q, K)\n",
    "\n",
    "print(\"\\n--- ATTENTION SCORES ---\")\n",
    "print(f\"Wynik shape: {scores_einsum.shape}\")\n",
    "print(f\"Czy identyczne? {torch.allclose(scores_manual, scores_einsum)}\")\n",
    "\n",
    "# A teraz mno偶enie przez V (Value)\n",
    "V = torch.randn(batch, heads, seq_len, dim)\n",
    "\n",
    "# Wynik Attention * V\n",
    "# Scores: bhqk\n",
    "# V:      bhkd (k to ten sam wymiar co w scores - length)\n",
    "# Wynik:  bhqd (wracamy do oryginalnego ksztatu)\n",
    "# Sumujemy po 'k'\n",
    "context = torch.einsum('bhqk, bhkd -> bhqd', scores_einsum, V)\n",
    "\n",
    "print(f\"Context shape: {context.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8921b07",
   "metadata": {},
   "source": [
    "##  Black Belt Summary\n",
    "\n",
    "Dlaczego warto u偶ywa `einsum`?\n",
    "1.  **Czytelno:** Wida dokadnie, kt贸re wymiary s mno偶one, a kt贸re sumowane. Nie musisz zgadywa, co robi `transpose(1, 2)`.\n",
    "2.  **Bezpieczestwo:** Nie musisz robi `reshape` ani `view`, co jest czstym 藕r贸dem bd贸w w wymiarach.\n",
    "3.  **Wydajno:** PyTorch kompiluje `einsum` do zoptymalizowanych jder CUDA (czsto czy operacje, np. transpozycj i mno偶enie w jednym kroku).\n",
    "\n",
    "**Zasada kciuka:**\n",
    "Jeli masz w kodzie wicej ni偶 jeden `.permute()` lub `.transpose()` przed mno偶eniem -> zamie to na `einsum`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
