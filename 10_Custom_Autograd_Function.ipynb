{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/10_Custom_Autograd_Function.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ac420",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 10: Custom Autograd Function (R\u0119czne Pochodne)\n",
    "\n",
    "PyTorch zna pochodne wi\u0119kszo\u015bci operacji matematycznych (`+`, `*`, `sin`, `exp`).\n",
    "Ale czasem musisz stworzy\u0107 w\u0142asn\u0105.\n",
    "\n",
    "Aby to zrobi\u0107, tworzymy klas\u0119 dziedzicz\u0105c\u0105 po `torch.autograd.Function` i implementujemy dwie metody statyczne:\n",
    "\n",
    "1.  **`forward(ctx, input)`**:\n",
    "    *   Robi obliczenia (np. $y = x^3$).\n",
    "    *   Zapisuje dane potrzebne do pochodnej w **kontek\u015bcie** (`ctx.save_for_backward`).\n",
    "2.  **`backward(ctx, grad_output)`**:\n",
    "    *   Dostaje gradient \"z g\u00f3ry\" (`grad_output`).\n",
    "    *   Mno\u017cy go przez nasz\u0105 lokaln\u0105 pochodn\u0105 (Chain Rule).\n",
    "    *   Zwraca gradient dla wej\u015bcia.\n",
    "\n",
    "Zaimplementujemy w\u0142asn\u0105 funkcj\u0119 sze\u015bcienn\u0105: $f(x) = x^3$.\n",
    "Pochodna to $f'(x) = 3x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09659a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\u0142asna funkcja zdefiniowana.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Klasa musi dziedziczy\u0107 po torch.autograd.Function\n",
    "class MyCube(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # 1. Obliczenia Forward\n",
    "        result = x ** 3\n",
    "        \n",
    "        # 2. Zapisywanie do pami\u0119ci (Context)\n",
    "        # Musimy zapisa\u0107 'x', bo b\u0119dzie potrzebne do policzenia pochodnej (3x^2)\n",
    "        ctx.save_for_backward(x)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # grad_output: Gradient, kt\u00f3ry przyszed\u0142 z g\u00f3ry (od Loss function)\n",
    "        \n",
    "        # 1. Odzyskujemy zapisane tensory\n",
    "        x, = ctx.saved_tensors\n",
    "        \n",
    "        # 2. Liczymy nasz\u0105 lokaln\u0105 pochodn\u0105: 3x^2\n",
    "        local_grad = 3 * x ** 2\n",
    "        \n",
    "        # 3. Chain Rule: Mno\u017cymy gradient z g\u00f3ry przez nasz\n",
    "        grad_input = grad_output * local_grad\n",
    "        \n",
    "        return grad_input\n",
    "\n",
    "# Tworzymy alias dla wygody (jak F.relu)\n",
    "my_cube = MyCube.apply\n",
    "\n",
    "print(\"W\u0142asna funkcja zdefiniowana.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4cf532",
   "metadata": {},
   "source": [
    "## Testowanie w Boju\n",
    "\n",
    "Sprawd\u017amy, czy to dzia\u0142a.\n",
    "1.  Przepu\u015bcimy dane przez nasz\u0105 funkcj\u0119.\n",
    "2.  Wywo\u0142amy `.backward()`.\n",
    "3.  Sprawdzimy, czy `x.grad` zgadza si\u0119 z matematyk\u0105.\n",
    "\n",
    "Dla $x=2$:\n",
    "*   Forward: $2^3 = 8$.\n",
    "*   Backward: $3 \\cdot 2^2 = 12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29306bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward (2^3): 8.0\n",
      "Gradient (3*2^2): 12.0\n",
      "\u2705 Matematyka si\u0119 zgadza!\n"
     ]
    }
   ],
   "source": [
    "# Dane wej\u015bciowe (wymagaj\u0105 gradientu)\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Forward\n",
    "y = my_cube(x)\n",
    "print(f\"Forward (2^3): {y.item()}\")\n",
    "\n",
    "# Backward\n",
    "# Symulujemy, \u017ce to koniec sieci (Loss), wi\u0119c gradient pocz\u0105tkowy to 1.0\n",
    "y.backward()\n",
    "\n",
    "print(f\"Gradient (3*2^2): {x.grad.item()}\")\n",
    "\n",
    "if x.grad.item() == 12.0:\n",
    "    print(\"\u2705 Matematyka si\u0119 zgadza!\")\n",
    "else:\n",
    "    print(\"\u274c Co\u015b posz\u0142o nie tak.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5fac2",
   "metadata": {},
   "source": [
    "## `gradcheck`: Ostateczny Egzamin\n",
    "\n",
    "Jako ludzie, mylimy si\u0119 przy liczeniu pochodnych.\n",
    "PyTorch ma wbudowane narz\u0119dzie **`gradcheck`**.\n",
    "\n",
    "Robi ono dwie rzeczy:\n",
    "1.  Liczy gradient Twoj\u0105 metod\u0105 `backward` (Analitycznie).\n",
    "2.  Liczy gradient numerycznie (metod\u0105 r\u00f3\u017cnic sko\u0144czonych: $\\frac{f(x+h) - f(x)}{h}$).\n",
    "3.  Por\u00f3wnuje wyniki.\n",
    "\n",
    "Je\u015bli napiszesz z\u0142y wz\u00f3r w `backward`, `gradcheck` to wykryje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c568c484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czy gradcheck przeszed\u0142? True\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# Dane testowe (musi by\u0107 double precision dla gradcheck)\n",
    "test_input = torch.randn(20, 20, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "# Uruchamiamy test\n",
    "# eps=1e-6 (ma\u0142e przesuni\u0119cie h)\n",
    "# atol=1e-4 (tolerancja b\u0142\u0119du)\n",
    "try:\n",
    "    is_ok = gradcheck(my_cube, test_input, eps=1e-6, atol=1e-4)\n",
    "    print(f\"Czy gradcheck przeszed\u0142? {is_ok}\")\n",
    "except Exception as e:\n",
    "    print(f\"\ud83d\udeab B\u0142\u0105d: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1066c87",
   "metadata": {},
   "source": [
    "## Hackowanie Gradient\u00f3w (Gradient Reversal Layer)\n",
    "\n",
    "Po co to robi\u0107, skoro `x**3` dzia\u0142a automatycznie?\n",
    "Bo czasami chcemy **oszuka\u0107** matematyk\u0119.\n",
    "\n",
    "Przyk\u0142ad: **Gradient Reversal Layer (GRL)**.\n",
    "U\u017cywany w Domain Adaptation.\n",
    "*   Forward: Zachowuje si\u0119 jak identyczno\u015b\u0107 ($y = x$).\n",
    "*   Backward: Odwraca znak gradientu ($grad_{in} = -grad_{out}$).\n",
    "\n",
    "Dzi\u0119ki temu jedna cz\u0119\u015b\u0107 sieci uczy si\u0119 dobrze klasyfikowa\u0107, a druga cz\u0119\u015b\u0107 sieci \"og\u0142upia\" si\u0119 celowo (np. \u017ceby nie rozpoznawa\u0107, z jakiej domeny pochodzi zdj\u0119cie).\n",
    "\n",
    "Bez `autograd.Function` by\u015b tego nie zrobi\u0142."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c412aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient po odwr\u00f3ceniu: -1.0\n"
     ]
    }
   ],
   "source": [
    "class GradientReversal(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # Forward: Nic nie robimy (Identity)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward: Odwracamy znak!\n",
    "        # To sprawia, \u017ce wagi b\u0119d\u0105 aktualizowane w PRZECIWN\u0104 stron\u0119 ni\u017c powinny (uciekaj\u0105 od optimum)\n",
    "        return -grad_output\n",
    "\n",
    "grad_reverse = GradientReversal.apply\n",
    "\n",
    "# Test\n",
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "y = grad_reverse(x)\n",
    "\n",
    "# Loss = y (chcemy zminimalizowa\u0107 y)\n",
    "# Normalnie gradient by\u0142by +1 (zmniejsz x).\n",
    "# Tutaj gradient powinien by\u0107 -1 (zwi\u0119ksz x).\n",
    "y.backward()\n",
    "\n",
    "print(f\"Gradient po odwr\u00f3ceniu: {x.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17f26a8",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "Pisanie w\u0142asnych funkcji Autograd jest konieczne, gdy:\n",
    "1.  **Nied\u00f3\u017cniczkowalno\u015b\u0107:** U\u017cywasz biblioteki C++/Numpy w \u015brodku sieci, kt\u00f3ra nie jest PyTorchem (musisz r\u0119cznie powiedzie\u0107 sieci, jak policzy\u0107 gradient przez t\u0119 \"czarn\u0105 skrzynk\u0119\").\n",
    "2.  **Stabilno\u015b\u0107 numeryczna:** Standardowy wz\u00f3r wybucha (NaN), a Ty znasz wz\u00f3r uproszczony (np. LogSoftmax).\n",
    "3.  **Hackowanie:** Chcesz zmieni\u0107 fizyk\u0119 uczenia (GRL, Gradient Clipping wewn\u0105trz warstwy).\n",
    "\n",
    "Pami\u0119taj o `ctx.save_for_backward`! Bez tego `backward` nie b\u0119dzie mia\u0142 dost\u0119pu do danych z `forward`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}