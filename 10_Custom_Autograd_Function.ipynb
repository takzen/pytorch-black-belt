{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875ac420",
   "metadata": {},
   "source": [
    "# ğŸ¥‹ Lekcja 10: Custom Autograd Function (RÄ™czne Pochodne)\n",
    "\n",
    "PyTorch zna pochodne wiÄ™kszoÅ›ci operacji matematycznych (`+`, `*`, `sin`, `exp`).\n",
    "Ale czasem musisz stworzyÄ‡ wÅ‚asnÄ….\n",
    "\n",
    "Aby to zrobiÄ‡, tworzymy klasÄ™ dziedziczÄ…cÄ… po `torch.autograd.Function` i implementujemy dwie metody statyczne:\n",
    "\n",
    "1.  **`forward(ctx, input)`**:\n",
    "    *   Robi obliczenia (np. $y = x^3$).\n",
    "    *   Zapisuje dane potrzebne do pochodnej w **kontekÅ›cie** (`ctx.save_for_backward`).\n",
    "2.  **`backward(ctx, grad_output)`**:\n",
    "    *   Dostaje gradient \"z gÃ³ry\" (`grad_output`).\n",
    "    *   MnoÅ¼y go przez naszÄ… lokalnÄ… pochodnÄ… (Chain Rule).\n",
    "    *   Zwraca gradient dla wejÅ›cia.\n",
    "\n",
    "Zaimplementujemy wÅ‚asnÄ… funkcjÄ™ szeÅ›ciennÄ…: $f(x) = x^3$.\n",
    "Pochodna to $f'(x) = 3x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09659a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WÅ‚asna funkcja zdefiniowana.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Klasa musi dziedziczyÄ‡ po torch.autograd.Function\n",
    "class MyCube(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # 1. Obliczenia Forward\n",
    "        result = x ** 3\n",
    "        \n",
    "        # 2. Zapisywanie do pamiÄ™ci (Context)\n",
    "        # Musimy zapisaÄ‡ 'x', bo bÄ™dzie potrzebne do policzenia pochodnej (3x^2)\n",
    "        ctx.save_for_backward(x)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # grad_output: Gradient, ktÃ³ry przyszedÅ‚ z gÃ³ry (od Loss function)\n",
    "        \n",
    "        # 1. Odzyskujemy zapisane tensory\n",
    "        x, = ctx.saved_tensors\n",
    "        \n",
    "        # 2. Liczymy naszÄ… lokalnÄ… pochodnÄ…: 3x^2\n",
    "        local_grad = 3 * x ** 2\n",
    "        \n",
    "        # 3. Chain Rule: MnoÅ¼ymy gradient z gÃ³ry przez nasz\n",
    "        grad_input = grad_output * local_grad\n",
    "        \n",
    "        return grad_input\n",
    "\n",
    "# Tworzymy alias dla wygody (jak F.relu)\n",
    "my_cube = MyCube.apply\n",
    "\n",
    "print(\"WÅ‚asna funkcja zdefiniowana.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4cf532",
   "metadata": {},
   "source": [
    "## Testowanie w Boju\n",
    "\n",
    "SprawdÅºmy, czy to dziaÅ‚a.\n",
    "1.  PrzepuÅ›cimy dane przez naszÄ… funkcjÄ™.\n",
    "2.  WywoÅ‚amy `.backward()`.\n",
    "3.  Sprawdzimy, czy `x.grad` zgadza siÄ™ z matematykÄ….\n",
    "\n",
    "Dla $x=2$:\n",
    "*   Forward: $2^3 = 8$.\n",
    "*   Backward: $3 \\cdot 2^2 = 12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29306bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward (2^3): 8.0\n",
      "Gradient (3*2^2): 12.0\n",
      "âœ… Matematyka siÄ™ zgadza!\n"
     ]
    }
   ],
   "source": [
    "# Dane wejÅ›ciowe (wymagajÄ… gradientu)\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Forward\n",
    "y = my_cube(x)\n",
    "print(f\"Forward (2^3): {y.item()}\")\n",
    "\n",
    "# Backward\n",
    "# Symulujemy, Å¼e to koniec sieci (Loss), wiÄ™c gradient poczÄ…tkowy to 1.0\n",
    "y.backward()\n",
    "\n",
    "print(f\"Gradient (3*2^2): {x.grad.item()}\")\n",
    "\n",
    "if x.grad.item() == 12.0:\n",
    "    print(\"âœ… Matematyka siÄ™ zgadza!\")\n",
    "else:\n",
    "    print(\"âŒ CoÅ› poszÅ‚o nie tak.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5fac2",
   "metadata": {},
   "source": [
    "## `gradcheck`: Ostateczny Egzamin\n",
    "\n",
    "Jako ludzie, mylimy siÄ™ przy liczeniu pochodnych.\n",
    "PyTorch ma wbudowane narzÄ™dzie **`gradcheck`**.\n",
    "\n",
    "Robi ono dwie rzeczy:\n",
    "1.  Liczy gradient TwojÄ… metodÄ… `backward` (Analitycznie).\n",
    "2.  Liczy gradient numerycznie (metodÄ… rÃ³Å¼nic skoÅ„czonych: $\\frac{f(x+h) - f(x)}{h}$).\n",
    "3.  PorÃ³wnuje wyniki.\n",
    "\n",
    "JeÅ›li napiszesz zÅ‚y wzÃ³r w `backward`, `gradcheck` to wykryje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c568c484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czy gradcheck przeszedÅ‚? True\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# Dane testowe (musi byÄ‡ double precision dla gradcheck)\n",
    "test_input = torch.randn(20, 20, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "# Uruchamiamy test\n",
    "# eps=1e-6 (maÅ‚e przesuniÄ™cie h)\n",
    "# atol=1e-4 (tolerancja bÅ‚Ä™du)\n",
    "try:\n",
    "    is_ok = gradcheck(my_cube, test_input, eps=1e-6, atol=1e-4)\n",
    "    print(f\"Czy gradcheck przeszedÅ‚? {is_ok}\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš« BÅ‚Ä…d: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1066c87",
   "metadata": {},
   "source": [
    "## Hackowanie GradientÃ³w (Gradient Reversal Layer)\n",
    "\n",
    "Po co to robiÄ‡, skoro `x**3` dziaÅ‚a automatycznie?\n",
    "Bo czasami chcemy **oszukaÄ‡** matematykÄ™.\n",
    "\n",
    "PrzykÅ‚ad: **Gradient Reversal Layer (GRL)**.\n",
    "UÅ¼ywany w Domain Adaptation.\n",
    "*   Forward: Zachowuje siÄ™ jak identycznoÅ›Ä‡ ($y = x$).\n",
    "*   Backward: Odwraca znak gradientu ($grad_{in} = -grad_{out}$).\n",
    "\n",
    "DziÄ™ki temu jedna czÄ™Å›Ä‡ sieci uczy siÄ™ dobrze klasyfikowaÄ‡, a druga czÄ™Å›Ä‡ sieci \"ogÅ‚upia\" siÄ™ celowo (np. Å¼eby nie rozpoznawaÄ‡, z jakiej domeny pochodzi zdjÄ™cie).\n",
    "\n",
    "Bez `autograd.Function` byÅ› tego nie zrobiÅ‚."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c412aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient po odwrÃ³ceniu: -1.0\n"
     ]
    }
   ],
   "source": [
    "class GradientReversal(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # Forward: Nic nie robimy (Identity)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward: Odwracamy znak!\n",
    "        # To sprawia, Å¼e wagi bÄ™dÄ… aktualizowane w PRZECIWNÄ„ stronÄ™ niÅ¼ powinny (uciekajÄ… od optimum)\n",
    "        return -grad_output\n",
    "\n",
    "grad_reverse = GradientReversal.apply\n",
    "\n",
    "# Test\n",
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "y = grad_reverse(x)\n",
    "\n",
    "# Loss = y (chcemy zminimalizowaÄ‡ y)\n",
    "# Normalnie gradient byÅ‚by +1 (zmniejsz x).\n",
    "# Tutaj gradient powinien byÄ‡ -1 (zwiÄ™ksz x).\n",
    "y.backward()\n",
    "\n",
    "print(f\"Gradient po odwrÃ³ceniu: {x.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17f26a8",
   "metadata": {},
   "source": [
    "## ğŸ¥‹ Black Belt Summary\n",
    "\n",
    "Pisanie wÅ‚asnych funkcji Autograd jest konieczne, gdy:\n",
    "1.  **NiedÃ³Å¼niczkowalnoÅ›Ä‡:** UÅ¼ywasz biblioteki C++/Numpy w Å›rodku sieci, ktÃ³ra nie jest PyTorchem (musisz rÄ™cznie powiedzieÄ‡ sieci, jak policzyÄ‡ gradient przez tÄ™ \"czarnÄ… skrzynkÄ™\").\n",
    "2.  **StabilnoÅ›Ä‡ numeryczna:** Standardowy wzÃ³r wybucha (NaN), a Ty znasz wzÃ³r uproszczony (np. LogSoftmax).\n",
    "3.  **Hackowanie:** Chcesz zmieniÄ‡ fizykÄ™ uczenia (GRL, Gradient Clipping wewnÄ…trz warstwy).\n",
    "\n",
    "PamiÄ™taj o `ctx.save_for_backward`! Bez tego `backward` nie bÄ™dzie miaÅ‚ dostÄ™pu do danych z `forward`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
