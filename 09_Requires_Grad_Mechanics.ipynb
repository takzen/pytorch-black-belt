{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/09_Requires_Grad_Mechanics.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9a512",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 9: Requires Grad & Inference Mode (Zarz\u0105dzanie Pami\u0119ci\u0105)\n",
    "\n",
    "Ka\u017cdy tensor w PyTorch ma flag\u0119 `requires_grad`.\n",
    "*   `True`: PyTorch alokuje dodatkow\u0105 pami\u0119\u0107 na graf obliczeniowy.\n",
    "*   `False`: Tensor zachowuje si\u0119 jak zwyk\u0142a macierz NumPy (lekki).\n",
    "\n",
    "Podczas **treningu** chcemy `True` (dla wag).\n",
    "Podczas **u\u017cywania (Inference)** chcemy `False` (\u017ceby nie zapcha\u0107 pami\u0119ci).\n",
    "\n",
    "Mamy trzy sposoby na kontrolowanie tego:\n",
    "1.  **.detach():** Fizyczne oderwanie tensora od grafu.\n",
    "2.  **with torch.no_grad():** Klasyczny spos\u00f3b na wy\u0142\u0105czenie silnika Autograd.\n",
    "3.  **with torch.inference_mode():** Nowoczesny, ekstremalnie zoptymalizowany tryb dla produkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54dccf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi (w) requires_grad: True\n",
      "Dane (x) requires_grad: False\n",
      "Wynik (y) requires_grad: True\n",
      "Funkcja y: <MmBackward0 object at 0x0000014F58561780>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Tensor, kt\u00f3ry chce si\u0119 uczy\u0107 (Wagi)\n",
    "w = torch.randn(5, 5, requires_grad=True)\n",
    "\n",
    "# 2. Tensor, kt\u00f3ry jest danymi (Input)\n",
    "x = torch.randn(5, 5) # Domy\u015blnie requires_grad=False\n",
    "\n",
    "print(f\"Wagi (w) requires_grad: {w.requires_grad}\")\n",
    "print(f\"Dane (x) requires_grad: {x.requires_grad}\")\n",
    "\n",
    "# Operacja\n",
    "y = w @ x\n",
    "\n",
    "# Wynik \"dziedziczy\" ch\u0119\u0107 uczenia si\u0119 po rodzicach!\n",
    "# Skoro 'w' wymaga\u0142o gradientu, to 'y' te\u017c go wymaga (\u017ceby policzy\u0107 pochodn\u0105 dla 'w').\n",
    "print(f\"Wynik (y) requires_grad: {y.requires_grad}\")\n",
    "print(f\"Funkcja y: {y.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1519d",
   "metadata": {},
   "source": [
    "## 1. Metoda `.detach()` (Chirurgiczne ci\u0119cie)\n",
    "\n",
    "U\u017cywamy tego, gdy chcemy przerwa\u0107 przep\u0142yw gradient\u00f3w w po\u0142owie sieci.\n",
    "Cz\u0119sty przypadek: **GAN-y** lub **Transfer Learning** (zamra\u017canie cz\u0119\u015bci sieci).\n",
    "\n",
    "`.detach()` zwraca nowy tensor, kt\u00f3ry dzieli t\u0119 sam\u0105 pami\u0119\u0107, ale **nie ma historii**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c275a081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orygina\u0142 y: <MmBackward0 object at 0x0000014F58562E90> (Ma histori\u0119)\n",
      "Odci\u0119ty z:  None (Brak historii)\n",
      "Czy z wymaga gradientu? False\n",
      "\n",
      "--- TEST PAMI\u0118CI ---\n",
      "Zmienili\u015bmy z[0,0]. Sprawd\u017amy y[0,0]: 999.0\n",
      "Uwaga: .detach() nie kopiuje danych! Zmiana w 'z' zmienia 'y'.\n"
     ]
    }
   ],
   "source": [
    "# Mamy y, kt\u00f3re jest cz\u0119\u015bci\u0105 grafu\n",
    "print(f\"Orygina\u0142 y: {y.grad_fn} (Ma histori\u0119)\")\n",
    "\n",
    "# Odcinamy\n",
    "z = y.detach()\n",
    "\n",
    "print(f\"Odci\u0119ty z:  {z.grad_fn} (Brak historii)\")\n",
    "print(f\"Czy z wymaga gradientu? {z.requires_grad}\")\n",
    "\n",
    "# Dow\u00f3d na wsp\u00f3\u0142dzielenie pami\u0119ci (Uwa\u017caj na in-place!)\n",
    "print(\"\\n--- TEST PAMI\u0118CI ---\")\n",
    "z[0, 0] = 999\n",
    "print(f\"Zmienili\u015bmy z[0,0]. Sprawd\u017amy y[0,0]: {y[0,0]}\")\n",
    "print(\"Uwaga: .detach() nie kopiuje danych! Zmiana w 'z' zmienia 'y'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe4ef2",
   "metadata": {},
   "source": [
    "## 2. `no_grad()` vs `inference_mode()`\n",
    "\n",
    "To s\u0105 mened\u017cery kontekstu (`with ...`).\n",
    "\n",
    "*   **`torch.no_grad()`**: Tylko ustawia `requires_grad=False` dla nowych tensor\u00f3w. Ale nadal pozwala na pewne operacje, kt\u00f3re mog\u0105 by\u0107 potrzebne do `backward` w przysz\u0142o\u015bci.\n",
    "*   **`torch.inference_mode()` (ZALECANE)**: Wy\u0142\u0105cza wszystko. Nie mo\u017cna u\u017cy\u0107 wyniku z tego bloku do \u017cadnego treningu. Dzi\u0119ki temu PyTorch mo\u017ce pomin\u0105\u0107 np. \u015bledzenie wersji (Version Counter), co przyspiesza kod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d339b96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- NO GRAD ---\n",
      "Czy wymaga gradientu? False\n",
      "\n",
      "--- INFERENCE MODE (Black Belt Choice) ---\n",
      "Czy wymaga gradientu? False\n",
      "B\u0142\u0105d no_grad: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n",
      "\ud83d\udeab Backward na inference_mode: ZABLOKOWANE.\n",
      "B\u0142\u0105d: element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "print(\"--- NO GRAD ---\")\n",
    "with torch.no_grad():\n",
    "    y_no_grad = w @ x\n",
    "    print(f\"Czy wymaga gradientu? {y_no_grad.requires_grad}\")\n",
    "    # W no_grad() wci\u0105\u017c mo\u017cemy robi\u0107 in-place operations, kt\u00f3re s\u0105 \u015bledzone przez Version Counter\n",
    "    \n",
    "print(\"\\n--- INFERENCE MODE (Black Belt Choice) ---\")\n",
    "with torch.inference_mode():\n",
    "    y_inf = w @ x\n",
    "    print(f\"Czy wymaga gradientu? {y_inf.requires_grad}\")\n",
    "    \n",
    "    # R\u00f3\u017cnica jest subtelna, ale kluczowa.\n",
    "    # Spr\u00f3bujmy u\u017cy\u0107 tych wynik\u00f3w do dalszego treningu poza blokiem.\n",
    "    \n",
    "try:\n",
    "    # To zadzia\u0142a (cho\u0107 nie policzy gradientu dla w, bo y_no_grad nie ma historii)\n",
    "    loss = y_no_grad.sum()\n",
    "    loss.backward() \n",
    "    print(\"Backward na no_grad: Przesz\u0142o (ale gradient\u00f3w brak).\")\n",
    "except Exception as e:\n",
    "    print(f\"B\u0142\u0105d no_grad: {e}\")\n",
    "\n",
    "try:\n",
    "    # To wyrzuci b\u0142\u0105d! Inference Mode zabrania tworzenia grafu nawet p\u00f3\u017aniej.\n",
    "    loss = y_inf.sum()\n",
    "    loss.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\n\ud83d\udeab Backward na inference_mode: ZABLOKOWANE.\")\n",
    "    print(f\"B\u0142\u0105d: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1518b",
   "metadata": {},
   "source": [
    "## Dlaczego `inference_mode` jest szybsze?\n",
    "\n",
    "W `inference_mode` PyTorch pomija tworzenie struktur `ViewTracking` i `VersionCounter`.\n",
    "Przy ma\u0142ych modelach (MLP) r\u00f3\u017cnica jest znikoma.\n",
    "Przy gigantycznych modelach (LLM, ViT) i bardzo ma\u0142ych batchach (np. obs\u0142uga zapyta\u0144 HTTP w czasie rzeczywistym), narzut Pythona na obs\u0142ug\u0119 grafu mo\u017ce by\u0107 zauwa\u017calny.\n",
    "\n",
    "**Zasada in\u017cynierska:**\n",
    "*   Trenujesz? -> Nic nie r\u00f3b.\n",
    "*   Walidujesz/Testujesz/Wdra\u017casz? -> **`@torch.inference_mode()`** (jako dekorator funkcji)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d53d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wynik predykcji: torch.Size([5, 5])\n",
      "Czy ma histori\u0119? None\n"
     ]
    }
   ],
   "source": [
    "# Wzorzec projektowy: Dekorator\n",
    "@torch.inference_mode()\n",
    "def predict(model_weights, inputs):\n",
    "    # Ca\u0142a ta funkcja jest chroniona i zoptymalizowana\n",
    "    return model_weights @ inputs\n",
    "\n",
    "# Symulacja \"Modelu\" na produkcji\n",
    "result = predict(w, x)\n",
    "\n",
    "print(\"Wynik predykcji:\", result.shape)\n",
    "print(\"Czy ma histori\u0119?\", result.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859054c6",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  **`x.requires_grad`**: W\u0142asno\u015b\u0107 wirusowa. Je\u015bli jeden sk\u0142adnik r\u00f3wnania tego wymaga, wynik te\u017c b\u0119dzie tego wymaga\u0142.\n",
    "2.  **`.detach()`**: U\u017cywaj, gdy chcesz \"urwa\u0107\" histori\u0119 (np. przekazuj\u0105c stan ukryty w LSTM do nowej sekwencji albo wizualizuj\u0105c tensor w matplotlib).\n",
    "3.  **`torch.no_grad()`**: Stare, dobre, ale wolniejsze. U\u017cywaj tylko, je\u015bli musisz manipulowa\u0107 tensorami w specyficzny spos\u00f3b.\n",
    "4.  **`torch.inference_mode()`**: **Nowy standard.** U\u017cywaj zawsze na produkcji i podczas walidacji. Jest szybsze i bezpieczniejsze (gwarantuje, \u017ce nie zrobisz b\u0142\u0119du w logice gradient\u00f3w)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}