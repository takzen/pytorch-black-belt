{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfec8521",
   "metadata": {},
   "source": [
    "#  Lekcja 16: Custom Collate Fn (Obsuga danych o r贸偶nej dugoci)\n",
    "\n",
    "W In偶ynierii Danych PyTorch `DataLoader` dziaa w dw贸ch krokach:\n",
    "1.  **Sampler** wybiera indeksy (np. `[0, 5, 2]`).\n",
    "2.  **Dataset** zwraca surowe obiekty dla tych indeks贸w.\n",
    "3.  **Collate Fn** (Sklejacz) bierze list tych obiekt贸w i zamienia je w jeden Tensor (Batch).\n",
    "\n",
    "Domylny `default_collate` robi po prostu `torch.stack()`.\n",
    "Dla tekst贸w o r贸偶nej dugoci musimy napisa wasny `collate_fn`, kt贸ry u偶ywa **Paddingu** (wypeniania zerami)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82ec676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SUROWE DANE ---\n",
      "Pr贸bka 0: tensor([1, 2, 3]) (Dugo: 3)\n",
      "Pr贸bka 1: tensor([4, 5, 6, 7, 8]) (Dugo: 5)\n",
      "Pr贸bka 2: tensor([9]) (Dugo: 1)\n",
      "Pr贸bka 3: tensor([10, 11, 12, 13]) (Dugo: 4)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 1. DANE (Symulacja zda o r贸偶nej dugoci)\n",
    "# Wyobra藕 sobie, 偶e to s ztokenizowane zdania (IDs s贸w).\n",
    "raw_data = [\n",
    "    torch.tensor([1, 2, 3]),             # Zdanie A (dugo 3)\n",
    "    torch.tensor([4, 5, 6, 7, 8]),       # Zdanie B (dugo 5)\n",
    "    torch.tensor([9]),                   # Zdanie C (dugo 1)\n",
    "    torch.tensor([10, 11, 12, 13])       # Zdanie D (dugo 4)\n",
    "]\n",
    "\n",
    "print(\"--- SUROWE DANE ---\")\n",
    "for i, seq in enumerate(raw_data):\n",
    "    print(f\"Pr贸bka {i}: {seq} (Dugo: {len(seq)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e8567",
   "metadata": {},
   "source": [
    "## Problem: Domylny Collate\n",
    "\n",
    "Spr贸bujmy wrzuci to do Loadera bez 偶adnej konfiguracji.\n",
    "Oczekujemy bdu `RuntimeError`, poniewa偶 PyTorch nie potrafi uo偶y \"schodk贸w\" w r贸wn macierz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d69e304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr贸ba uruchomienia domylnego loadera...\n",
      "\n",
      " BD (Zgodnie z planem):\n",
      "stack expects each tensor to be equal size, but got [3] at entry 0 and [5] at entry 1\n",
      "\n",
      "Wyjanienie: stack expects each tensor to be equal size.\n"
     ]
    }
   ],
   "source": [
    "# batch_size=2, 偶eby pr贸bowa sklei przynajmniej dwa elementy\n",
    "loader_broken = DataLoader(raw_data, batch_size=2, shuffle=False)\n",
    "\n",
    "print(\"Pr贸ba uruchomienia domylnego loadera...\")\n",
    "\n",
    "try:\n",
    "    for batch in loader_broken:\n",
    "        print(batch)\n",
    "except RuntimeError as e:\n",
    "    print(\"\\n BD (Zgodnie z planem):\")\n",
    "    print(e)\n",
    "    print(\"\\nWyjanienie: stack expects each tensor to be equal size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7dd0d1",
   "metadata": {},
   "source": [
    "## Rozwizanie: Padding Collate\n",
    "\n",
    "Napiszemy funkcj, kt贸ra:\n",
    "1.  Przyjmuje list tensor贸w (`batch`).\n",
    "2.  Znajduje najdu偶szy tensor.\n",
    "3.  Wypenia kr贸tsze tensory zerami (`padding_value=0`) do tej dugoci.\n",
    "4.  Zwraca idealny prostokt.\n",
    "\n",
    "U偶yjemy do tego `pad_sequence` z biblioteki `torch.nn.utils.rnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24fdcced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WYNIK Z WASNYM COLLATE ---\n",
      "\n",
      "Batch 0:\n",
      "Ksztat: torch.Size([2, 5])\n",
      "tensor([[1, 2, 3, 0, 0],\n",
      "        [4, 5, 6, 7, 8]])\n",
      "Prawdziwe dugoci: [3, 5]\n",
      "\n",
      "Batch 1:\n",
      "Ksztat: torch.Size([2, 4])\n",
      "tensor([[ 9,  0,  0,  0],\n",
      "        [10, 11, 12, 13]])\n",
      "Prawdziwe dugoci: [1, 4]\n",
      "\n",
      "Widzisz zera? To jest Padding. Macierz jest prostoktna!\n"
     ]
    }
   ],
   "source": [
    "def my_padding_collate(batch):\n",
    "    \"\"\"\n",
    "    batch: lista tensor贸w [tensor([1,2,3]), tensor([4,5])]\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Padding\n",
    "    # batch_first=True -> Wymiary [Batch, Time]\n",
    "    # padding_value=0  -> Czym wypenia braki? (Zazwyczaj 0 to ID dla <PAD>)\n",
    "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # 2. (Opcjonalnie) Zwracamy te偶 oryginalne dugoci\n",
    "    # To przydaje si np. w sieciach rekurencyjnych (pack_padded_sequence),\n",
    "    # 偶eby sie wiedziaa, 偶e te zera na kocu to mieci.\n",
    "    lengths = torch.tensor([len(x) for x in batch])\n",
    "    \n",
    "    return padded_batch, lengths\n",
    "\n",
    "# Testujemy\n",
    "# num_workers=0 (Dla bezpieczestwa na Windows)\n",
    "loader_fixed = DataLoader(raw_data, batch_size=2, collate_fn=my_padding_collate, shuffle=False)\n",
    "\n",
    "print(\"--- WYNIK Z WASNYM COLLATE ---\")\n",
    "\n",
    "for i, (batch, lens) in enumerate(loader_fixed):\n",
    "    print(f\"\\nBatch {i}:\")\n",
    "    print(f\"Ksztat: {batch.shape}\")\n",
    "    print(batch)\n",
    "    print(f\"Prawdziwe dugoci: {lens.tolist()}\")\n",
    "\n",
    "print(\"\\nWidzisz zera? To jest Padding. Macierz jest prostoktna!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b622bc96",
   "metadata": {},
   "source": [
    "##  Black Belt Summary\n",
    "\n",
    "1.  **Kiedy u偶ywa?** Zawsze, gdy Twoje dane wejciowe nie s sztywn macierz (Tekst, Audio o r贸偶nym czasie trwania, Grafy o r贸偶nej liczbie wz贸w, Detekcja obiekt贸w z r贸偶n liczb ramek).\n",
    "2.  **`pad_sequence`:** Najlepszy przyjaciel in偶yniera NLP. Pamitaj o `batch_first=True`.\n",
    "3.  **Maskowanie:** W Transformerach bdziesz musia u偶y tych zer, 偶eby stworzy `Attention Mask` (偶eby model nie zwraca uwagi na puste wypeniacze).\n",
    "\n",
    "W nastpnej lekcji zajmiemy si **Samplerami**. Co zrobi, gdy masz 99% zdrowych pacjent贸w i 1% chorych? (Imbalanced Dataset)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
