{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e5bbfd",
   "metadata": {},
   "source": [
    "# ü•ã Lekcja 14: Forward Mode AD (Przysz≈Ço≈õƒá r√≥≈ºniczkowania)\n",
    "\n",
    "Klasyczne uczenie maszynowe (Backpropagation) to **Reverse Mode**.\n",
    "*   ≈öwietne, gdy: `Inputs >> Outputs` (np. Sieƒá neuronowa: milion wag -> 1 Loss).\n",
    "*   Wada: Musi trzymaƒá ca≈Çy graf w pamiƒôci.\n",
    "\n",
    "**Forward Mode AD:**\n",
    "*   ≈öwietne, gdy: `Outputs >> Inputs` (np. Generative AI, Fizyka, Jacobiany).\n",
    "*   Zaleta: Liczy pochodnƒÖ \"w locie\" (w trakcie forward pass). Nie zajmuje pamiƒôci na graf!\n",
    "\n",
    "**Matematyka (Liczby Dualne):**\n",
    "Zamiast liczby $x$, wprowadzamy parƒô $(x, \\dot{x})$, gdzie $\\dot{x}$ to \"ziarno\" gradientu (tangent).\n",
    "Ka≈ºda operacja liczy wynik i od razu jego pochodnƒÖ:\n",
    "$$ f(x, \\dot{x}) = (f(x), f'(x) \\cdot \\dot{x}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62a6d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wej≈õcie: tensor([3.])\n",
      "Wyj≈õcie: tensor([[ 3.],\n",
      "        [ 9.],\n",
      "        [27.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd.forward_ad as fwAD # Nowy modu≈Ç w PyTorch\n",
    "\n",
    "# Funkcja wektorowa: 1 wej≈õcie -> 3 wyj≈õcia\n",
    "# f(x) = [x, x^2, x^3]\n",
    "def func(x):\n",
    "    return torch.stack([x, x**2, x**3])\n",
    "\n",
    "x = torch.tensor([3.0])\n",
    "\n",
    "print(f\"Wej≈õcie: {x}\")\n",
    "print(f\"Wyj≈õcie: {func(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb71062",
   "metadata": {},
   "source": [
    "## Podej≈õcie 1: Klasyczne (Reverse Mode)\n",
    "\n",
    "Gdyby≈õmy chcieli policzyƒá pochodnƒÖ `func` wzglƒôdem `x` tradycyjnie, musieliby≈õmy zrobiƒá `backward` dla ka≈ºdego elementu wyj≈õcia osobno (lub u≈ºyƒá `jacobian`).\n",
    "\n",
    "Dla funkcji $f: \\mathbb{R}^1 \\to \\mathbb{R}^{1000}$, Reverse Mode jest 1000x wolniejszy ni≈º Forward Mode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59f7949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- REVERSE MODE JACOBIAN ---\n",
      "tensor([ 1.,  6., 27.])\n"
     ]
    }
   ],
   "source": [
    "# Klasyczne podej≈õcie (wymaga requires_grad)\n",
    "x_rev = torch.tensor([3.0], requires_grad=True)\n",
    "y_rev = func(x_rev)\n",
    "\n",
    "# Musimy liczyƒá gradienty dla ka≈ºdego wyj≈õcia osobno?\n",
    "# Albo u≈ºyƒá sztuczki z sumƒÖ, albo obliczyƒá Jakobian.\n",
    "# Zobaczmy Jakobian (macierz pochodnych)\n",
    "J_rev = torch.autograd.functional.jacobian(func, x_rev)\n",
    "\n",
    "print(\"--- REVERSE MODE JACOBIAN ---\")\n",
    "print(J_rev.squeeze())\n",
    "# Oczekujemy: [1, 2x, 3x^2] -> dla x=3: [1, 6, 27]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb76432",
   "metadata": {},
   "source": [
    "## Podej≈õcie 2: Forward Mode AD\n",
    "\n",
    "U≈ºywamy `fwAD`.\n",
    "1.  Otwieramy kontekst `dual_level`.\n",
    "2.  Tworzymy **Liczbƒô DualnƒÖ** (`make_dual`): pakujemy warto≈õƒá $x$ i stycznƒÖ $\\dot{x}$ (tangent). Zazwyczaj $\\dot{x}=1$.\n",
    "3.  Uruchamiamy funkcjƒô raz.\n",
    "4.  Rozpakowujemy wynik (`unpack_dual`). Dostajemy od razu wynik i gradient!\n",
    "\n",
    "Zero budowania grafu wstecznego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c3cf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FORWARD MODE RESULT ---\n",
      "Wynik funkcji: tensor([[ 3.],\n",
      "        [ 9.],\n",
      "        [27.]])\n",
      "Gradient (JVP): tensor([[ 1.],\n",
      "        [ 6.],\n",
      "        [27.]])\n"
     ]
    }
   ],
   "source": [
    "# Krok 1: Kontekst\n",
    "with fwAD.dual_level():\n",
    "    \n",
    "    # Krok 2: Tworzymy Dual Tensor (Warto≈õƒá + Tangent)\n",
    "    # Tangent = 1.0 oznacza, ≈ºe liczymy pochodnƒÖ df/dx\n",
    "    dual_input = fwAD.make_dual(x, torch.ones_like(x))\n",
    "    \n",
    "    # Krok 3: Forward (Obliczenia lecƒÖ normalnie)\n",
    "    dual_output = func(dual_input)\n",
    "    \n",
    "    # Krok 4: Rozpakowanie\n",
    "    result, jvp = fwAD.unpack_dual(dual_output)\n",
    "\n",
    "print(\"--- FORWARD MODE RESULT ---\")\n",
    "print(f\"Wynik funkcji: {result}\")\n",
    "print(f\"Gradient (JVP): {jvp}\") # Jacobian-Vector Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1427c85",
   "metadata": {},
   "source": [
    "## Benchmark: Reverse vs Forward\n",
    "\n",
    "Zr√≥bmy ekstremalny przyk≈Çad.\n",
    "Funkcja bierze 1 liczbƒô i zwraca 10 000 liczb.\n",
    "$$ f(x) = [x^0, x^1, ..., x^{9999}] $$\n",
    "\n",
    "*   **Reverse Mode:** Musi przej≈õƒá wstecz przez graf 10 000 razy (dla ka≈ºdego wyj≈õcia), ≈ºeby zbudowaƒá pe≈Çny Jakobian.\n",
    "*   **Forward Mode:** Przechodzi raz w prz√≥d i niesie gradient ze sobƒÖ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92fec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse Mode Time: 1.3660 s\n",
      "Forward Mode Time: 0.7635 s\n",
      "\n",
      "Czy wyniki sƒÖ takie same? True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Funkcja 1 -> 10000\n",
    "def massive_expansion(x):\n",
    "    # Tworzymy potƒôgi od 0 do 9999\n",
    "    powers = torch.arange(10000, device=x.device)\n",
    "    return x ** powers\n",
    "\n",
    "x_bench = torch.tensor([1.0001], requires_grad=True) # Ma≈Ça liczba > 1\n",
    "\n",
    "# 1. Reverse Mode (Jacobian)\n",
    "start = time.time()\n",
    "# jacobian w PyTorch u≈ºywa reverse mode domy≈õlnie\n",
    "J_rev = torch.autograd.functional.jacobian(massive_expansion, x_bench)\n",
    "print(f\"Reverse Mode Time: {time.time() - start:.4f} s\")\n",
    "\n",
    "# 2. Forward Mode\n",
    "start = time.time()\n",
    "with fwAD.dual_level():\n",
    "    dual_x = fwAD.make_dual(x_bench, torch.ones_like(x_bench))\n",
    "    dual_y = massive_expansion(dual_x)\n",
    "    _, J_fwd = fwAD.unpack_dual(dual_y)\n",
    "print(f\"Forward Mode Time: {time.time() - start:.4f} s\")\n",
    "\n",
    "# Sprawd≈∫my poprawno≈õƒá\n",
    "print(f\"\\nCzy wyniki sƒÖ takie same? {torch.allclose(J_rev.squeeze(), J_fwd.squeeze())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb860d",
   "metadata": {},
   "source": [
    "## ü•ã Black Belt Summary\n",
    "\n",
    "1.  **Reverse Mode (`backward`)**: U≈ºywaj, gdy trenujesz sieci neuronowe (Loss to jedna liczba, a wag sƒÖ miliony). Koszt zale≈ºy od liczby wej≈õƒá.\n",
    "2.  **Forward Mode (`fwAD`)**: U≈ºywaj, gdy liczysz Jakobian funkcji, kt√≥ra ma ma≈Ço wej≈õƒá, a du≈ºo wyj≈õƒá (lub gdy output jest wektorem). Koszt zale≈ºy od liczby wej≈õƒá (w naszym te≈õcie 1 wej≈õcie = super szybko).\n",
    "\n",
    "**Gdzie to spotkasz?**\n",
    "*   **PINN (Physics-Informed Neural Networks):** RozwiƒÖzywanie r√≥wna≈Ñ r√≥≈ºniczkowych.\n",
    "*   **Optymalizacja drugiego rzƒôdu:** Liczenie Hessianu (jako Jacobiana z Gradientu) mo≈ºna przyspieszyƒá, mieszajƒÖc Reverse i Forward mode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
