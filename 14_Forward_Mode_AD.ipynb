{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/14_Forward_Mode_AD.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5bbfd",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 14: Forward Mode AD (Przysz\u0142o\u015b\u0107 r\u00f3\u017cniczkowania)\n",
    "\n",
    "Klasyczne uczenie maszynowe (Backpropagation) to **Reverse Mode**.\n",
    "*   \u015awietne, gdy: `Inputs >> Outputs` (np. Sie\u0107 neuronowa: milion wag -> 1 Loss).\n",
    "*   Wada: Musi trzyma\u0107 ca\u0142y graf w pami\u0119ci.\n",
    "\n",
    "**Forward Mode AD:**\n",
    "*   \u015awietne, gdy: `Outputs >> Inputs` (np. Generative AI, Fizyka, Jacobiany).\n",
    "*   Zaleta: Liczy pochodn\u0105 \"w locie\" (w trakcie forward pass). Nie zajmuje pami\u0119ci na graf!\n",
    "\n",
    "**Matematyka (Liczby Dualne):**\n",
    "Zamiast liczby $x$, wprowadzamy par\u0119 $(x, \\dot{x})$, gdzie $\\dot{x}$ to \"ziarno\" gradientu (tangent).\n",
    "Ka\u017cda operacja liczy wynik i od razu jego pochodn\u0105:\n",
    "$$ f(x, \\dot{x}) = (f(x), f'(x) \\cdot \\dot{x}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62a6d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wej\u015bcie: tensor([3.])\n",
      "Wyj\u015bcie: tensor([[ 3.],\n",
      "        [ 9.],\n",
      "        [27.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd.forward_ad as fwAD # Nowy modu\u0142 w PyTorch\n",
    "\n",
    "# Funkcja wektorowa: 1 wej\u015bcie -> 3 wyj\u015bcia\n",
    "# f(x) = [x, x^2, x^3]\n",
    "def func(x):\n",
    "    return torch.stack([x, x**2, x**3])\n",
    "\n",
    "x = torch.tensor([3.0])\n",
    "\n",
    "print(f\"Wej\u015bcie: {x}\")\n",
    "print(f\"Wyj\u015bcie: {func(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb71062",
   "metadata": {},
   "source": [
    "## Podej\u015bcie 1: Klasyczne (Reverse Mode)\n",
    "\n",
    "Gdyby\u015bmy chcieli policzy\u0107 pochodn\u0105 `func` wzgl\u0119dem `x` tradycyjnie, musieliby\u015bmy zrobi\u0107 `backward` dla ka\u017cdego elementu wyj\u015bcia osobno (lub u\u017cy\u0107 `jacobian`).\n",
    "\n",
    "Dla funkcji $f: \\mathbb{R}^1 \\to \\mathbb{R}^{1000}$, Reverse Mode jest 1000x wolniejszy ni\u017c Forward Mode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59f7949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- REVERSE MODE JACOBIAN ---\n",
      "tensor([ 1.,  6., 27.])\n"
     ]
    }
   ],
   "source": [
    "# Klasyczne podej\u015bcie (wymaga requires_grad)\n",
    "x_rev = torch.tensor([3.0], requires_grad=True)\n",
    "y_rev = func(x_rev)\n",
    "\n",
    "# Musimy liczy\u0107 gradienty dla ka\u017cdego wyj\u015bcia osobno?\n",
    "# Albo u\u017cy\u0107 sztuczki z sum\u0105, albo obliczy\u0107 Jakobian.\n",
    "# Zobaczmy Jakobian (macierz pochodnych)\n",
    "J_rev = torch.autograd.functional.jacobian(func, x_rev)\n",
    "\n",
    "print(\"--- REVERSE MODE JACOBIAN ---\")\n",
    "print(J_rev.squeeze())\n",
    "# Oczekujemy: [1, 2x, 3x^2] -> dla x=3: [1, 6, 27]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb76432",
   "metadata": {},
   "source": [
    "## Podej\u015bcie 2: Forward Mode AD\n",
    "\n",
    "U\u017cywamy `fwAD`.\n",
    "1.  Otwieramy kontekst `dual_level`.\n",
    "2.  Tworzymy **Liczb\u0119 Dualn\u0105** (`make_dual`): pakujemy warto\u015b\u0107 $x$ i styczn\u0105 $\\dot{x}$ (tangent). Zazwyczaj $\\dot{x}=1$.\n",
    "3.  Uruchamiamy funkcj\u0119 raz.\n",
    "4.  Rozpakowujemy wynik (`unpack_dual`). Dostajemy od razu wynik i gradient!\n",
    "\n",
    "Zero budowania grafu wstecznego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c3cf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FORWARD MODE RESULT ---\n",
      "Wynik funkcji: tensor([[ 3.],\n",
      "        [ 9.],\n",
      "        [27.]])\n",
      "Gradient (JVP): tensor([[ 1.],\n",
      "        [ 6.],\n",
      "        [27.]])\n"
     ]
    }
   ],
   "source": [
    "# Krok 1: Kontekst\n",
    "with fwAD.dual_level():\n",
    "    \n",
    "    # Krok 2: Tworzymy Dual Tensor (Warto\u015b\u0107 + Tangent)\n",
    "    # Tangent = 1.0 oznacza, \u017ce liczymy pochodn\u0105 df/dx\n",
    "    dual_input = fwAD.make_dual(x, torch.ones_like(x))\n",
    "    \n",
    "    # Krok 3: Forward (Obliczenia lec\u0105 normalnie)\n",
    "    dual_output = func(dual_input)\n",
    "    \n",
    "    # Krok 4: Rozpakowanie\n",
    "    result, jvp = fwAD.unpack_dual(dual_output)\n",
    "\n",
    "print(\"--- FORWARD MODE RESULT ---\")\n",
    "print(f\"Wynik funkcji: {result}\")\n",
    "print(f\"Gradient (JVP): {jvp}\") # Jacobian-Vector Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1427c85",
   "metadata": {},
   "source": [
    "## Benchmark: Reverse vs Forward\n",
    "\n",
    "Zr\u00f3bmy ekstremalny przyk\u0142ad.\n",
    "Funkcja bierze 1 liczb\u0119 i zwraca 10 000 liczb.\n",
    "$$ f(x) = [x^0, x^1, ..., x^{9999}] $$\n",
    "\n",
    "*   **Reverse Mode:** Musi przej\u015b\u0107 wstecz przez graf 10 000 razy (dla ka\u017cdego wyj\u015bcia), \u017ceby zbudowa\u0107 pe\u0142ny Jakobian.\n",
    "*   **Forward Mode:** Przechodzi raz w prz\u00f3d i niesie gradient ze sob\u0105."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92fec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse Mode Time: 1.3660 s\n",
      "Forward Mode Time: 0.7635 s\n",
      "\n",
      "Czy wyniki s\u0105 takie same? True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Funkcja 1 -> 10000\n",
    "def massive_expansion(x):\n",
    "    # Tworzymy pot\u0119gi od 0 do 9999\n",
    "    powers = torch.arange(10000, device=x.device)\n",
    "    return x ** powers\n",
    "\n",
    "x_bench = torch.tensor([1.0001], requires_grad=True) # Ma\u0142a liczba > 1\n",
    "\n",
    "# 1. Reverse Mode (Jacobian)\n",
    "start = time.time()\n",
    "# jacobian w PyTorch u\u017cywa reverse mode domy\u015blnie\n",
    "J_rev = torch.autograd.functional.jacobian(massive_expansion, x_bench)\n",
    "print(f\"Reverse Mode Time: {time.time() - start:.4f} s\")\n",
    "\n",
    "# 2. Forward Mode\n",
    "start = time.time()\n",
    "with fwAD.dual_level():\n",
    "    dual_x = fwAD.make_dual(x_bench, torch.ones_like(x_bench))\n",
    "    dual_y = massive_expansion(dual_x)\n",
    "    _, J_fwd = fwAD.unpack_dual(dual_y)\n",
    "print(f\"Forward Mode Time: {time.time() - start:.4f} s\")\n",
    "\n",
    "# Sprawd\u017amy poprawno\u015b\u0107\n",
    "print(f\"\\nCzy wyniki s\u0105 takie same? {torch.allclose(J_rev.squeeze(), J_fwd.squeeze())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb860d",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  **Reverse Mode (`backward`)**: U\u017cywaj, gdy trenujesz sieci neuronowe (Loss to jedna liczba, a wag s\u0105 miliony). Koszt zale\u017cy od liczby wej\u015b\u0107.\n",
    "2.  **Forward Mode (`fwAD`)**: U\u017cywaj, gdy liczysz Jakobian funkcji, kt\u00f3ra ma ma\u0142o wej\u015b\u0107, a du\u017co wyj\u015b\u0107 (lub gdy output jest wektorem). Koszt zale\u017cy od liczby wej\u015b\u0107 (w naszym te\u015bcie 1 wej\u015bcie = super szybko).\n",
    "\n",
    "**Gdzie to spotkasz?**\n",
    "*   **PINN (Physics-Informed Neural Networks):** Rozwi\u0105zywanie r\u00f3wna\u0144 r\u00f3\u017cniczkowych.\n",
    "*   **Optymalizacja drugiego rz\u0119du:** Liczenie Hessianu (jako Jacobiana z Gradientu) mo\u017cna przyspieszy\u0107, mieszaj\u0105c Reverse i Forward mode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}