{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/22_Buffers_vs_Parameters.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397ff6d",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 22: Buffers vs Parameters (Stan nietrenowalny)\n",
    "\n",
    "W PyTorch `nn.Module` ma dwa g\u0142\u00f3wne magazyny:\n",
    "1.  **Parameters:** Tensory, kt\u00f3re maj\u0105 gradient i s\u0105 aktualizowane przez optymalizator (Wagi, Biasy).\n",
    "2.  **Buffers:** Tensory, kt\u00f3re s\u0105 cz\u0119\u015bci\u0105 stanu modelu (zapisuj\u0105 si\u0119 w `state_dict`, przenosz\u0105 si\u0119 na GPU), ale **nie s\u0105 trenowane** przez gradient.\n",
    "\n",
    "**Typowy b\u0142\u0105d:**\n",
    "Przypisanie tensora jako zwyk\u0142ego atrybutu (`self.t = torch.randn(5)`).\n",
    "*   Skutek 1: Nie przeniesie si\u0119 na GPU przy `model.cuda()`.\n",
    "*   Skutek 2: Nie zapisze si\u0119 przy `torch.save()`.\n",
    "\n",
    "W tej lekcji naprawimy ten b\u0142\u0105d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7367026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zainicjowany.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Definiujemy modu\u0142 z 3 rodzajami zmiennych\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Parameter (Standard)\n",
    "        # To b\u0119dzie trenowane.\n",
    "        self.weight = nn.Parameter(torch.randn(3, 3))\n",
    "        \n",
    "        # 2. Zwyk\u0142y atrybut (B\u0141\u0104D!)\n",
    "        # To jest \"niewidzialne\" dla silnika PyTorch.\n",
    "        self.mistake_tensor = torch.randn(3, 3)\n",
    "        \n",
    "        # 3. Buffer (POPRAWNIE)\n",
    "        # To jest stan nietrenowalny (np. \u015brednia w BatchNorm).\n",
    "        # Musimy u\u017cy\u0107 register_buffer(nazwa, tensor)\n",
    "        self.register_buffer('running_mean', torch.randn(3, 3))\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "print(\"Model zainicjowany.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b6f34",
   "metadata": {},
   "source": [
    "## Test 1: Przenoszenie na GPU (`.to(device)`)\n",
    "\n",
    "To jest najcz\u0119stsza przyczyna b\u0142\u0119du `RuntimeError: Expected all tensors to be on the same device`.\n",
    "U\u017cytkownik pisze `model.to('cuda')`, ale jego pomocniczy tensor zostaje na CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6e2ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przenosz\u0119 model na: cuda\n",
      "\n",
      "--- GDZIE S\u0104 DANE? ---\n",
      "Weight (Param):       cuda:0\n",
      "Running Mean (Buffer): cuda:0\n",
      "Mistake (Zwyk\u0142y):      cpu\n",
      "\n",
      "\u274c B\u0141\u0104D! 'mistake_tensor' zosta\u0142 na CPU, mimo \u017ce model jest na GPU.\n",
      "W obliczeniach to spowoduje awari\u0119.\n"
     ]
    }
   ],
   "source": [
    "# Sprawd\u017amy, czy masz GPU (je\u015bli nie, test poka\u017cemy na idei)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Przenosz\u0119 model na: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"\\n--- GDZIE S\u0104 DANE? ---\")\n",
    "print(f\"Weight (Param):       {model.weight.device}\")\n",
    "print(f\"Running Mean (Buffer): {model.running_mean.device}\")\n",
    "print(f\"Mistake (Zwyk\u0142y):      {model.mistake_tensor.device}\")\n",
    "\n",
    "if str(device) != 'cpu' and str(model.mistake_tensor.device) == 'cpu':\n",
    "    print(\"\\n\u274c B\u0141\u0104D! 'mistake_tensor' zosta\u0142 na CPU, mimo \u017ce model jest na GPU.\")\n",
    "    print(\"W obliczeniach to spowoduje awari\u0119.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40979199",
   "metadata": {},
   "source": [
    "## Test 2: Zapisywanie modelu (`state_dict`)\n",
    "\n",
    "Czy nasze dane przetrwaj\u0105 zapis do pliku?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39e3cd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CO JEST W PLIKU ZAPISU? ---\n",
      "odict_keys(['weight', 'running_mean'])\n",
      "\u2705 Buffer: JEST ZAPISANY.\n",
      "\u274c Mistake: BRAK (Zgubili\u015bmy dane!).\n"
     ]
    }
   ],
   "source": [
    "state = model.state_dict()\n",
    "\n",
    "print(\"--- CO JEST W PLIKU ZAPISU? ---\")\n",
    "print(state.keys())\n",
    "\n",
    "# Weryfikacja\n",
    "if 'running_mean' in state:\n",
    "    print(\"\u2705 Buffer: JEST ZAPISANY.\")\n",
    "else:\n",
    "    print(\"\u274c Buffer: BRAK.\")\n",
    "\n",
    "if 'mistake_tensor' in state:\n",
    "    print(\"\u2705 Mistake: JEST ZAPISANY.\")\n",
    "else:\n",
    "    print(\"\u274c Mistake: BRAK (Zgubili\u015bmy dane!).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b7ee6",
   "metadata": {},
   "source": [
    "## Test 3: Optymalizator\n",
    "\n",
    "Czy optymalizator (np. Adam) spr\u00f3buje zmieni\u0107 nasze warto\u015bci?\n",
    "Bufor\u00f3w nie powinien dotyka\u0107."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b7fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CO WIDZI OPTYMALIZATOR? ---\n",
      "Trenowane parametry: ['weight']\n",
      "\u2705 Buffer jest bezpieczny (nie b\u0119dzie modyfikowany przez SGD).\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(\"--- CO WIDZI OPTYMALIZATOR? ---\")\n",
    "# Optymalizator iteruje po parametrach\n",
    "param_names = [n for n, p in model.named_parameters()]\n",
    "\n",
    "print(f\"Trenowane parametry: {param_names}\")\n",
    "\n",
    "if 'running_mean' not in param_names:\n",
    "    print(\"\u2705 Buffer jest bezpieczny (nie b\u0119dzie modyfikowany przez SGD).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e6430",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "**Kiedy u\u017cywa\u0107 `register_buffer`?**\n",
    "\n",
    "1.  **BatchNorm / LayerNorm:** Przechowywanie \u015bredniej i wariancji, kt\u00f3re s\u0105 aktualizowane matematycznie, a nie gradientem.\n",
    "2.  **Positional Encodings (Transformer):** Macierz sinus\u00f3w jest sta\u0142a. Musi by\u0107 na GPU, musi by\u0107 zapisana z modelem, ale nie mo\u017ce si\u0119 zmienia\u0107.\n",
    "3.  **Maski:** Maski przyczynowe (Causal Mask) w GPT.\n",
    "4.  **Liczniki:** Np. `self.register_buffer('steps', torch.tensor(0))`, je\u015bli chcesz, \u017ceby model wiedzia\u0142, ile krok\u00f3w ju\u017c trenowa\u0142 (i \u017ceby to si\u0119 zapisa\u0142o w checkpoincie).\n",
    "\n",
    "Je\u015bli napiszesz `self.cos_table = ...` w Transformerze \u2013 pope\u0142niasz b\u0142\u0105d. U\u017cyj `register_buffer`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}