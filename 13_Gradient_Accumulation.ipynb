{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/13_Gradient_Accumulation.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4341c56",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 13: Gradient Accumulation (Du\u017cy Batch na Ma\u0142ym GPU)\n",
    "\n",
    "W PyTorch `loss.backward()` nie nadpisuje gradient\u00f3w, ale je **akumuluje** (dodaje do istniej\u0105cych: `w.grad += new_grad`).\n",
    "Zazwyczaj walczymy z tym, wpisuj\u0105c `optimizer.zero_grad()` w ka\u017cdej p\u0119tli.\n",
    "\n",
    "Ale w **Gradient Accumulation** wykorzystujemy to jako zalet\u0119!\n",
    "\n",
    "**Algorytm:**\n",
    "1.  Podziel wirtualny \"Du\u017cy Batch\" (np. 128) na ma\u0142e \"Mikro Batche\" (np. 32).\n",
    "2.  Zr\u00f3b Forward i Backward dla Mikro Batcha.\n",
    "3.  **Wa\u017cne:** Podziel Loss przez liczb\u0119 krok\u00f3w akumulacji (\u017ceby \u015brednia si\u0119 zgadza\u0142a).\n",
    "4.  Powt\u00f3rz N razy.\n",
    "5.  Dopiero wtedy zr\u00f3b `step()` i `zero_grad()`.\n",
    "\n",
    "Dzi\u0119ki temu trenujesz model tak, jakby\u015b mia\u0142 superkomputer, u\u017cywaj\u0105c laptopa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a688dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Batch: 32\n",
      "Real Batch:   8\n",
      "Kroki akumulacji: 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Konfiguracja\n",
    "LARGE_BATCH_SIZE = 32   # Taki chcemy symulowa\u0107\n",
    "MICRO_BATCH_SIZE = 8    # Taki mie\u015bci si\u0119 w pami\u0119ci\n",
    "ACCUMULATION_STEPS = LARGE_BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "\n",
    "print(f\"Target Batch: {LARGE_BATCH_SIZE}\")\n",
    "print(f\"Real Batch:   {MICRO_BATCH_SIZE}\")\n",
    "print(f\"Kroki akumulacji: {ACCUMULATION_STEPS}\")\n",
    "\n",
    "# Dane i Model\n",
    "data = torch.randn(LARGE_BATCH_SIZE, 10)\n",
    "target = torch.randn(LARGE_BATCH_SIZE, 1)\n",
    "\n",
    "model = nn.Linear(10, 1)\n",
    "# Kopiujemy model, \u017ceby por\u00f3wna\u0107 dwie metody (czy daj\u0105 ten sam wynik)\n",
    "model_copy = nn.Linear(10, 1)\n",
    "model_copy.load_state_dict(model.state_dict())\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer_copy = optim.SGD(model_copy.parameters(), lr=0.01)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a749a",
   "metadata": {},
   "source": [
    "## Metoda 1: Standardowa (Du\u017cy Batch)\n",
    "\n",
    "To jest nasz punkt odniesienia (Baseline).\n",
    "Wrzucamy 32 pr\u00f3bki naraz. Zak\u0142adamy, \u017ce mamy niesko\u0144czono\u015b\u0107 RAM-u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4017295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi po standardowym kroku (pierwsze 5):\n",
      "tensor([-0.2719, -0.0496, -0.2903,  0.0671,  0.2706])\n"
     ]
    }
   ],
   "source": [
    "# 1. Standardowy krok (Wszystko naraz)\n",
    "optimizer_copy.zero_grad()\n",
    "\n",
    "pred_full = model_copy(data)\n",
    "loss_full = criterion(pred_full, target)\n",
    "\n",
    "loss_full.backward()\n",
    "optimizer_copy.step()\n",
    "\n",
    "print(\"Wagi po standardowym kroku (pierwsze 5):\")\n",
    "print(model_copy.weight.data[0, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd6f4b",
   "metadata": {},
   "source": [
    "## Metoda 2: Gradient Accumulation\n",
    "\n",
    "Teraz zrobimy to samo, ale \"na raty\", po 8 pr\u00f3bek.\n",
    "Kluczowe zmiany:\n",
    "1.  Dzielimy `loss` przez `ACCUMULATION_STEPS`. Dlaczego?\n",
    "    *   `MSELoss` liczy \u015bredni\u0105 z batcha.\n",
    "    *   \u015arednia z 32 element\u00f3w to `sum(errors) / 32`.\n",
    "    *   \u015arednia z 8 element\u00f3w to `sum(errors) / 8`.\n",
    "    *   Je\u015bli po prostu dodamy gradienty z 4 ma\u0142ych batchy, suma b\u0119dzie 4x za du\u017ca! Musimy to skorygowa\u0107 r\u0119cznie.\n",
    "2.  `optimizer.step()` wykonujemy tylko co N krok\u00f3w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da1e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krok 1/4: Gradient policzony (ale wagi stoj\u0105).\n",
      "Krok 2/4: Gradient policzony (ale wagi stoj\u0105).\n",
      "Krok 3/4: Gradient policzony (ale wagi stoj\u0105).\n",
      "Krok 4/4: Gradient policzony (ale wagi stoj\u0105).\n",
      "\n",
      "Wagi po akumulacji (pierwsze 5):\n",
      "tensor([-0.2719, -0.0496, -0.2903,  0.0671,  0.2706])\n"
     ]
    }
   ],
   "source": [
    "# 2. Akumulacja\n",
    "optimizer.zero_grad() # Zerujemy raz na pocz\u0105tku\n",
    "\n",
    "# P\u0119tla po mikro-batchach\n",
    "for i in range(ACCUMULATION_STEPS):\n",
    "    # Wycinamy kawa\u0142ek danych (Slicing)\n",
    "    start = i * MICRO_BATCH_SIZE\n",
    "    end = start + MICRO_BATCH_SIZE\n",
    "    \n",
    "    micro_data = data[start:end]\n",
    "    micro_target = target[start:end]\n",
    "    \n",
    "    # Forward\n",
    "    pred = model(micro_data)\n",
    "    loss = criterion(pred, micro_target)\n",
    "    \n",
    "    # --- MAGIA AKUMULACJI ---\n",
    "    # Normalizujemy strat\u0119!\n",
    "    loss = loss / ACCUMULATION_STEPS\n",
    "    \n",
    "    # Backward (Gradienty si\u0119 dodaj\u0105 do .grad)\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"Krok {i+1}/{ACCUMULATION_STEPS}: Gradient policzony (ale wagi stoj\u0105).\")\n",
    "\n",
    "# Dopiero teraz aktualizacja wag\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\nWagi po akumulacji (pierwsze 5):\")\n",
    "print(model.weight.data[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8091e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Maksymalna r\u00f3\u017cnica mi\u0119dzy metodami: 0.0000000000\n",
      "\u2705 SUKCES! Akumulacja dzia\u0142a matematycznie identycznie jak du\u017cy batch.\n"
     ]
    }
   ],
   "source": [
    "# WERYFIKACJA\n",
    "# Czy wyniki s\u0105 identyczne?\n",
    "diff = torch.abs(model.weight.data - model_copy.weight.data).max()\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Maksymalna r\u00f3\u017cnica mi\u0119dzy metodami: {diff:.10f}\")\n",
    "\n",
    "if diff < 1e-6:\n",
    "    print(\"\u2705 SUKCES! Akumulacja dzia\u0142a matematycznie identycznie jak du\u017cy batch.\")\n",
    "else:\n",
    "    print(\"\u274c CO\u015a NIE TAK. R\u00f3\u017cnica jest zbyt du\u017ca.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20aaefa",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "Gradient Accumulation to pot\u0119\u017cne narz\u0119dzie, ale ma **jeden haczyk**:\n",
    "\n",
    "**Batch Normalization.**\n",
    "Warstwy `BatchNorm` licz\u0105 \u015bredni\u0105 i wariancj\u0119 z **bie\u017c\u0105cego batcha**.\n",
    "*   W du\u017cym batchu (32): Statystyki s\u0105 liczone z 32 pr\u00f3bek.\n",
    "*   W akumulacji (8): Statystyki s\u0105 liczone z 8 pr\u00f3bek (s\u0105 bardziej zaszumione!).\n",
    "\n",
    "Akumulacja symuluje du\u017cy batch dla WAG, ale **NIE dla Batchorma**.\n",
    "Je\u015bli musisz u\u017cywa\u0107 akumulacji przy bardzo ma\u0142ych batchach (np. 1 lub 2), lepiej zamie\u0144 `BatchNorm` na `LayerNorm` lub `GroupNorm`, kt\u00f3re nie zale\u017c\u0105 od wielko\u015bci batcha."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}