{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/18_Num_Workers_and_Pin_Memory.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d9ece",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 18: Optymalizacja Transferu Danych (num_workers & pin_memory)\n",
    "\n",
    "Trening sieci to sztafeta.\n",
    "1.  **CPU (Dysk/RAM):** Czyta plik -> Dekoduje JPG -> Augmentacja -> Tensor.\n",
    "2.  **Transfer:** Kopiowanie z RAM do VRAM.\n",
    "3.  **GPU:** Obliczenia (Forward/Backward).\n",
    "\n",
    "Je\u015bli krok 1 i 2 s\u0105 wolniejsze ni\u017c 3, Twoje drogocenne GPU le\u017cy od\u0142ogiem.\n",
    "\n",
    "**Rozwi\u0105zania:**\n",
    "1.  **`num_workers > 0`**:\n",
    "    *   Domy\u015blnie (`0`) g\u0142\u00f3wny proces robi wszystko: Wczytaj -> Trenuj -> Wczytaj.\n",
    "    *   Z workerami (`4`): Workery \u0142aduj\u0105 kolejk\u0119 w tle. G\u0142\u00f3wny proces tylko bierze gotowe.\n",
    "2.  **`pin_memory=True`**:\n",
    "    *   Pami\u0119\u0107 RAM dzieli si\u0119 na **Pageable** (zwyk\u0142a) i **Pinned** (sztywna).\n",
    "    *   Transfer: Pageable RAM -> Pinned RAM -> GPU VRAM.\n",
    "    *   Ustawiaj\u0105c `pin_memory=True`, wrzucamy dane od razu do Pinned RAM. Oszcz\u0119dzamy jedno kopiowanie CPU-CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b9c46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Mamy GPU! pin_memory ma sens.\n",
      "Dataset gotowy: 10000 pr\u00f3bek.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "# Sprawd\u017amy sprz\u0119t\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"\u2705 Mamy GPU! pin_memory ma sens.\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"\u26a0\ufe0f Brak GPU. pin_memory nic nie da, ale num_workers nadal dzia\u0142a.\")\n",
    "\n",
    "# Generujemy spory dataset (100MB)\n",
    "data_size = 10000\n",
    "features = 1000\n",
    "dataset = TensorDataset(torch.randn(data_size, features), torch.randn(data_size, 1))\n",
    "\n",
    "print(f\"Dataset gotowy: {data_size} pr\u00f3bek.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dab5eb",
   "metadata": {},
   "source": [
    "## 1. Pin Memory (Autostrada)\n",
    "\n",
    "Standardowy tensor w PyTorch \u017cyje w pami\u0119ci stronicowanej (Pageable). System operacyjny mo\u017ce go przesuwa\u0107 lub zrzuci\u0107 na dysk (Swap).\n",
    "Karta graficzna (DMA - Direct Memory Access) nie mo\u017ce czyta\u0107 z takiej pami\u0119ci. Wymaga pami\u0119ci \"przypi\u0119tej\" (Pinned), kt\u00f3ra fizycznie nie zmienia adresu.\n",
    "\n",
    "Ustawienie `pin_memory=True` w DataLoaderze sprawia, \u017ce PyTorch alokuje specjalny bufor w RAM, co przyspiesza transfer `to(device)` nawet o **2-3 razy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40e8eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czy jest przypi\u0119ty? False\n",
      "Czy teraz jest przypi\u0119ty? True\n",
      "Czas Pageable -> GPU: 0.0065s\n",
      "Czas Pinned -> GPU:   0.0010s\n",
      "(Przy ma\u0142ych tensorach r\u00f3\u017cnica jest ma\u0142a, przy Gigabajtach - ogromna).\n"
     ]
    }
   ],
   "source": [
    "# Tworzymy tensor w zwyk\u0142ym RAM\n",
    "x = torch.randn(5, 5)\n",
    "print(f\"Czy jest przypi\u0119ty? {x.is_pinned()}\")\n",
    "\n",
    "# Przypinamy go r\u0119cznie (to robi DataLoader pod spodem)\n",
    "x_pinned = x.pin_memory()\n",
    "print(f\"Czy teraz jest przypi\u0119ty? {x_pinned.is_pinned()}\")\n",
    "\n",
    "# Benchmark transferu (tylko je\u015bli masz GPU)\n",
    "if device == \"cuda\":\n",
    "    # 1. Bez Pinningu\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = x.to(device)\n",
    "    print(f\"Czas Pageable -> GPU: {time.time() - start:.4f}s\")\n",
    "    \n",
    "    # 2. Z Pinningiem\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = x_pinned.to(device, non_blocking=True) # non_blocking pozwala na asynchroniczno\u015b\u0107!\n",
    "    print(f\"Czas Pinned -> GPU:   {time.time() - start:.4f}s\")\n",
    "    print(\"(Przy ma\u0142ych tensorach r\u00f3\u017cnica jest ma\u0142a, przy Gigabajtach - ogromna).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3339ae3",
   "metadata": {},
   "source": [
    "## 2. Num Workers (Wielow\u0105tkowo\u015b\u0107)\n",
    "\n",
    "To parametr, kt\u00f3ry decyduje, ile podproces\u00f3w \"przygotowuje posi\u0142ki\" dla GPU.\n",
    "\n",
    "**Zasada kciuka:**\n",
    "*   `0`: Debugowanie (najbezpieczniej, dzia\u0142a na Windows w Jupyterze).\n",
    "*   `2-4`: Zazwyczaj optymalne.\n",
    "*   `>8`: Cz\u0119sto spowalnia (zbyt du\u017cy narzut na zarz\u0105dzanie procesami).\n",
    "\n",
    "**\u26a0\ufe0f UWAGA DLA U\u017bYTKOWNIK\u00d3W WINDOWS:**\n",
    "Jeste\u015bmy w Jupyter Notebook na Windows. Ustawienie `num_workers > 0` tutaj cz\u0119sto powoduje b\u0142\u0119dy (BrokenPipeError, RuntimeError), o kt\u00f3rych rozmawiali\u015bmy wcze\u015bniej.\n",
    "Dlatego w poni\u017cszym te\u015bcie **zostawimy 0**, ale kod jest gotowy, by zmieni\u0107 t\u0119 liczb\u0119 na serwerze Linuxowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651c7708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BENCHMARK (Symulacja) ---\n",
      "Workers=0, Pin=False: 0.1539s\n",
      "Workers=0, Pin=True:  0.0782s\n",
      "Zysk z samego Pinningu: 1.97x\n",
      "Workers=4: (Pomini\u0119to ze wzgl\u0119du na stabilno\u015b\u0107 kernela na Windows)\n"
     ]
    }
   ],
   "source": [
    "def benchmark_loader(num_workers, pin_memory):\n",
    "    # Tworzymy loader z zadanymi parametrami\n",
    "    # UWAGA: Na Windows w Jupyterze num_workers > 0 mo\u017ce zawiesi\u0107 kernel!\n",
    "    # Je\u015bli to uruchamiasz u siebie, zachowaj ostro\u017cno\u015b\u0107.\n",
    "    loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=128, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    for batch_x, batch_y in loader:\n",
    "        # Symulacja transferu na GPU\n",
    "        if device == \"cuda\":\n",
    "            batch_x = batch_x.to(device, non_blocking=pin_memory)\n",
    "            batch_y = batch_y.to(device, non_blocking=pin_memory)\n",
    "            \n",
    "        # Symulacja oblicze\u0144 (kr\u00f3tka)\n",
    "        _ = batch_x * 2 \n",
    "        \n",
    "    return time.time() - start\n",
    "\n",
    "print(\"--- BENCHMARK (Symulacja) ---\")\n",
    "\n",
    "# Test 1: Baza (Jeden proces, zwyk\u0142a pami\u0119\u0107) - Bezpieczne na Windows\n",
    "t1 = benchmark_loader(num_workers=0, pin_memory=False)\n",
    "print(f\"Workers=0, Pin=False: {t1:.4f}s\")\n",
    "\n",
    "# Test 2: Tylko Pin Memory (Bezpieczne na Windows i GPU)\n",
    "if device == \"cuda\":\n",
    "    t2 = benchmark_loader(num_workers=0, pin_memory=True)\n",
    "    print(f\"Workers=0, Pin=True:  {t2:.4f}s\")\n",
    "    print(f\"Zysk z samego Pinningu: {t1/t2:.2f}x\")\n",
    "\n",
    "# Test 3: Workers > 0 (RYZYKOWNE W NOTATNIKU NA WINDOWS - POMIJAMY)\n",
    "# Na Linuxie/Produkcji ustawi\u0142by\u015b tu np. num_workers=4\n",
    "# t3 = benchmark_loader(num_workers=4, pin_memory=True) \n",
    "print(\"Workers=4: (Pomini\u0119to ze wzgl\u0119du na stabilno\u015b\u0107 kernela na Windows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1dc6d",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "Jak konfigurowa\u0107 `DataLoader` na produkcji?\n",
    "\n",
    "1.  **`pin_memory=True`**: ZAWSZE, je\u015bli u\u017cywasz GPU. To darmowe przyspieszenie.\n",
    "2.  **`num_workers`**:\n",
    "    *   Zacznij od `4`.\n",
    "    *   Je\u015bli masz szybki dysk (NVMe) i proste dane, CPU nie jest w\u0105skim gard\u0142em.\n",
    "    *   Je\u015bli robisz ci\u0119\u017ck\u0105 augmentacj\u0119 w locie (obracanie, skalowanie), zwi\u0119ksz liczb\u0119 worker\u00f3w, \u017ceby GPU nie czeka\u0142o.\n",
    "3.  **`non_blocking=True`**: Przy `x.to(device)` u\u017cywaj tej flagi razem z `pin_memory`. Pozwala to GPU wykonywa\u0107 obliczenia na poprzednim batchu w tym samym czasie, gdy CPU przesy\u0142a nast\u0119pny batch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}