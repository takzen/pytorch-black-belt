{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/notebooks/08_Computational_Graph_Viz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2ea86",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 8: Anatomia Grafu Obliczeniowego (DAG)\n",
    "\n",
    "Kiedy wykonujesz operacje na tensorach z `requires_grad=True`, PyTorch nie tylko liczy wynik.\n",
    "Buduje w pami\u0119ci drzewo (graf), kt\u00f3re zapami\u0119tuje **histori\u0119 operacji**.\n",
    "\n",
    "**Kluczowe poj\u0119cia:**\n",
    "1.  **Leaf Node (Li\u015b\u0107):** Tensor stworzony przez u\u017cytkownika (np. Wagi, Dane). Nie ma historii (`grad_fn` jest None).\n",
    "2.  **Root Node (Korze\u0144):** Wynik ko\u0144cowy (np. Loss).\n",
    "3.  **`grad_fn`:** Funkcja odwrotna. Je\u015bli zrobi\u0142e\u015b mno\u017cenie (`Mul`), w grafie powstaje w\u0119ze\u0142 `MulBackward0`, kt\u00f3ry wie, jak policzy\u0107 pochodn\u0105 mno\u017cenia.\n",
    "\n",
    "W tej lekcji prze\u015bledzimy ten graf r\u0119cznie, cofaj\u0105c si\u0119 od Wyniku do Wej\u015bcia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9efc9ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czy 'a' jest li\u015bciem? True\n",
      "Czy 'a' ma grad_fn? None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. TWORZYMY LI\u015aCIE (Wej\u015bcie)\n",
    "# requires_grad=True oznacza: \"Zacznij \u015bledzi\u0107 histori\u0119 od tego momentu\"\n",
    "a = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "print(f\"Czy 'a' jest li\u015bciem? {a.is_leaf}\")\n",
    "print(f\"Czy 'a' ma grad_fn? {a.grad_fn}\")  # None, bo to pocz\u0105tek historii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20128f",
   "metadata": {},
   "source": [
    "## Budowanie Grafu (Forward Pass)\n",
    "\n",
    "Wykonajmy proste dzia\u0142anie:\n",
    "$$ c = a \\times b $$\n",
    "$$ d = c + 5 $$\n",
    "$$ out = d \\times 2 $$\n",
    "\n",
    "Ka\u017cda z tych operacji dodaje nowy w\u0119ze\u0142 do grafu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0db52ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: tensor([6.], grad_fn=<MulBackward0>)\n",
      "Funkcja tworz\u0105ca c: <MulBackward0 object at 0x00000200F474DAE0>\n",
      "Funkcja tworz\u0105ca d: <AddBackward0 object at 0x00000200F474DAE0>\n",
      "Funkcja tworz\u0105ca out: <MulBackward0 object at 0x00000200F474DAE0>\n"
     ]
    }
   ],
   "source": [
    "# Krok 1: Mno\u017cenie\n",
    "c = a * b\n",
    "print(f\"c: {c}\")\n",
    "print(f\"Funkcja tworz\u0105ca c: {c.grad_fn}\") \n",
    "# MulBackward0 -> M\u00f3wi: \"Powsta\u0142em z mno\u017cenia\"\n",
    "\n",
    "# Krok 2: Dodawanie\n",
    "d = c + 5\n",
    "print(f\"Funkcja tworz\u0105ca d: {d.grad_fn}\")\n",
    "# AddBackward0\n",
    "\n",
    "# Krok 3: Wynik ko\u0144cowy\n",
    "out = d * 2\n",
    "print(f\"Funkcja tworz\u0105ca out: {out.grad_fn}\")\n",
    "# MulBackward0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770d2f1",
   "metadata": {},
   "source": [
    "## Spacer po Grafie (Traversing the Graph)\n",
    "\n",
    "Skoro `out` wie, \u017ce powsta\u0142 z mno\u017cenia `d * 2`, to musi mie\u0107 wska\u017anik do `d`.\n",
    "Mo\u017cemy u\u017cy\u0107 metody `.next_functions`, \u017ceby r\u0119cznie cofn\u0105\u0107 si\u0119 w historii a\u017c do `a` i `b`.\n",
    "\n",
    "To jest dok\u0142adnie to, co robi silnik Autograd podczas `backward()`, tylko my zrobimy to \"na piechot\u0119\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbe7659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \u015aLEDZTWO WSTECZNE ---\n",
      "KROK 0 (Out): <MulBackward0 object at 0x00000200F474E710>\n",
      "KROK 1 (Rodzice Out): ((<AddBackward0 object at 0x00000200F474D450>, 0), (None, 0))\n",
      "KROK 2 (Rodzice d): ((<MulBackward0 object at 0x00000200F474E710>, 0), (None, 0))\n",
      "KROK 3 (Rodzice c): ((<AccumulateGrad object at 0x00000200F474F370>, 0), (<AccumulateGrad object at 0x00000200F474E320>, 0))\n"
     ]
    }
   ],
   "source": [
    "print(\"--- \u015aLEDZTWO WSTECZNE ---\")\n",
    "\n",
    "# Krok 0: Jeste\u015bmy w 'out'\n",
    "print(f\"KROK 0 (Out): {out.grad_fn}\")\n",
    "\n",
    "# Krok 1: Z czego powsta\u0142 out?\n",
    "# next_functions zwraca list\u0119 krotek (funkcja, indeks)\n",
    "parents = out.grad_fn.next_functions\n",
    "print(f\"KROK 1 (Rodzice Out): {parents}\")\n",
    "# Widzimy AddBackward0 (to jest nasze 'd'). \n",
    "# Drugi element to None (bo mno\u017cyli\u015bmy przez sta\u0142\u0105 '2', kt\u00f3ra nie ma historii)\n",
    "\n",
    "# Wyci\u0105gamy funkcj\u0119, kt\u00f3ra stworzy\u0142a 'd'\n",
    "d_fn = parents[0][0]\n",
    "\n",
    "# Krok 2: Z czego powsta\u0142o d?\n",
    "grandparents = d_fn.next_functions\n",
    "print(f\"KROK 2 (Rodzice d): {grandparents}\")\n",
    "# Widzimy MulBackward0 (to jest nasze 'c')\n",
    "\n",
    "# Wyci\u0105gamy funkcj\u0119, kt\u00f3ra stworzy\u0142a 'c'\n",
    "c_fn = grandparents[0][0]\n",
    "\n",
    "# Krok 3: Z czego powsta\u0142o c?\n",
    "great_grandparents = c_fn.next_functions\n",
    "print(f\"KROK 3 (Rodzice c): {great_grandparents}\")\n",
    "# Widzimy dwa obiekty AccumulateGrad.\n",
    "# AccumulateGrad to \"opakowanie\" na nasze Li\u015bcie (a i b).\n",
    "# To tutaj gromadz\u0105 si\u0119 gradienty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2db79",
   "metadata": {},
   "source": [
    "## Wizualizacja W\u0142asna (ASCII Tree)\n",
    "\n",
    "Zamiast polega\u0107 na zewn\u0119trznych bibliotekach (jak `torchviz`, kt\u00f3ry cz\u0119sto sprawia problemy na Windowsie przez brak plik\u00f3w systemowych), zachowamy si\u0119 jak in\u017cynierowie i **napiszemy w\u0142asne narz\u0119dzie**.\n",
    "\n",
    "Stworzymy prost\u0105 funkcj\u0119 rekurencyjn\u0105, kt\u00f3ra przejdzie po grafie (u\u017cywaj\u0105c atrybutu `next_functions`, kt\u00f3ry odkryli\u015bmy wcze\u015bniej) i wypisze go w formie drzewa tekstowego.\n",
    "\n",
    "To rozwi\u0105zanie jest:\n",
    "1.  **Lekkie:** Czysty Python, zero zale\u017cno\u015bci.\n",
    "2.  **Niezawodne:** Zadzia\u0142a zawsze, nawet na serwerze bez graficznego interfejsu.\n",
    "3.  **Edukacyjne:** Poka\u017ce dok\u0142adnie struktur\u0119 `Mul` -> `Add` -> `Leaf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a6ce09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WIZUALIZACJA GRAFU (ASCII) ---\n",
      "ROOT (Wynik ko\u0144cowy)\n",
      "\u27a1\ufe0f MulBackward0\n",
      "    \u27a1\ufe0f AddBackward0\n",
      "        \u27a1\ufe0f MulBackward0\n",
      "            \u27a1\ufe0f AccumulateGrad\n",
      "                \ud83c\udf43 To jest LI\u015a\u0106 (Leaf Node) - np. Wagi\n",
      "            \u27a1\ufe0f AccumulateGrad\n",
      "                \ud83c\udf43 To jest LI\u015a\u0106 (Leaf Node) - np. Wagi\n",
      "        \ud83d\udd39 (Sta\u0142a / Brak historii)\n",
      "    \ud83d\udd39 (Sta\u0142a / Brak historii)\n"
     ]
    }
   ],
   "source": [
    "# Zamiast polega\u0107 na zewn\u0119trznym programie, napiszmy w\u0142asn\u0105 funkcj\u0119 rekurencyjn\u0105.\n",
    "# To przechodzi po grafie i rysuje go w konsoli.\n",
    "\n",
    "def print_graph(grad_fn, level=0):\n",
    "    # Wci\u0119cie dla wizualizacji poziomu\n",
    "    indent = \"    \" * level\n",
    "    \n",
    "    # Nazwa funkcji (np. MulBackward0)\n",
    "    name = grad_fn.__class__.__name__\n",
    "    \n",
    "    print(f\"{indent}\u27a1\ufe0f {name}\")\n",
    "    \n",
    "    # Je\u015bli funkcja ma \"rodzic\u00f3w\" (next_functions), idziemy g\u0142\u0119biej\n",
    "    if hasattr(grad_fn, 'next_functions'):\n",
    "        for parent, _ in grad_fn.next_functions:\n",
    "            if parent is not None:\n",
    "                print_graph(parent, level + 1)\n",
    "            else:\n",
    "                # To oznacza, \u017ce dotarli\u015bmy do sta\u0142ej lub tensora bez gradientu\n",
    "                print(f\"{indent}    \ud83d\udd39 (Sta\u0142a / Brak historii)\")\n",
    "    \n",
    "    # Je\u015bli to AccumulateGrad, to znaczy, \u017ce dotarli\u015bmy do Li\u015bcia (Zmiennej)\n",
    "    if \"AccumulateGrad\" in name:\n",
    "        # Mo\u017cemy spr\u00f3bowa\u0107 wyci\u0105gn\u0105\u0107 nazw\u0119 zmiennej, je\u015bli j\u0105 ma\n",
    "        print(f\"{indent}    \ud83c\udf43 To jest LI\u015a\u0106 (Leaf Node) - np. Wagi\")\n",
    "\n",
    "print(\"--- WIZUALIZACJA GRAFU (ASCII) ---\")\n",
    "print(\"ROOT (Wynik ko\u0144cowy)\")\n",
    "print_graph(out.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2cbe70",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  **Graf jest dynamiczny:** Graf powstaje w momencie wykonywania operacji (np. `+`, `*`). Je\u015bli w kodzie masz `if x > 0: y = x * 2`, to graf zmienia sw\u00f3j kszta\u0142t w ka\u017cdej iteracji (Define-by-Run).\n",
    "2.  **`grad_fn` to mapa:** Ka\u017cdy tensor (opr\u00f3cz li\u015bci) ma w plecaku map\u0119, jak wr\u00f3ci\u0107 do domu.\n",
    "3.  **Li\u015bcie (`is_leaf`):** To parametry (Wagi), kt\u00f3re chcemy aktualizowa\u0107. Tylko dla nich PyTorch gromadzi `.grad`.\n",
    "\n",
    "W nast\u0119pnej lekcji zobaczymy, jak manipulowa\u0107 t\u0105 histori\u0105 (odcinanie grafu i tryby inferencji)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}