{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/notebooks/34_Bottleneck_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f324f",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 34: Profilowanie (Gdzie ucieka czas?)\n",
    "\n",
    "Tw\u00f3j model trenuje si\u0119 wolno. Dlaczego?\n",
    "Zamiast zgadywa\u0107, u\u017cyj **`torch.profiler`**.\n",
    "\n",
    "Profiler \u015bledzi ka\u017cde wywo\u0142anie operacji (Operator) w PyTorch i mierzy dwa czasy:\n",
    "1.  **CPU Time:** Ile czasu procesor sp\u0119dzi\u0142 na wydawaniu rozkazu.\n",
    "2.  **CUDA Time:** Ile czasu karta graficzna faktycznie liczy\u0142a.\n",
    "\n",
    "**Kluczowe poj\u0119cia w tabeli wynik\u00f3w:**\n",
    "*   **Self Time:** Czas sp\u0119dzony w *tej konkretnej* funkcji (bez jej \"dzieci\").\n",
    "*   **Total Time:** Czas sp\u0119dzony w funkcji i wszystkich podfunkcjach, kt\u00f3re wywo\u0142a\u0142a.\n",
    "\n",
    "Je\u015bli `Self CPU` jest wysokie -> Masz wolny kod Pythona (p\u0119tle, listy).\n",
    "Je\u015bli `Self CUDA` jest wysokie -> Masz ci\u0119\u017ck\u0105 matematyk\u0119 (du\u017ce macierze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb94c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profilujemy na: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Konfiguracja\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Profilujemy na: {device}\")\n",
    "\n",
    "# Je\u015bli CPU, to nie b\u0119dziemy mieli kolumn CUDA, ale logika jest ta sama.\n",
    "activities = [ProfilerActivity.CPU]\n",
    "if device == \"cuda\":\n",
    "    activities.append(ProfilerActivity.CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf32cac",
   "metadata": {},
   "source": [
    "## Symulacja: Model z \"W\u0105skim Gard\u0142em\"\n",
    "\n",
    "Stworzymy sie\u0107, kt\u00f3ra ma 3 cz\u0119\u015bci:\n",
    "1.  **Szybka:** Ma\u0142e mno\u017cenie macierzy.\n",
    "2.  **Wolna (Matematycznie):** Gigantyczne mno\u017cenie macierzy (obci\u0105\u017ca GPU).\n",
    "3.  **G\u0142upia (Pythonowa):** P\u0119tla `for` wewn\u0105trz modelu (obci\u0105\u017ca CPU i blokuje GPU).\n",
    "\n",
    "Zobaczymy, czy Profiler to wykryje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e52320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gotowy. Czas na rentgen.\n"
     ]
    }
   ],
   "source": [
    "class SlowModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fast_layer = nn.Linear(100, 100)\n",
    "        self.heavy_layer = nn.Linear(4000, 4000) # 16 mln parametr\u00f3w!\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Cz\u0119\u015b\u0107 Szybka\n",
    "        # record_function nadaje nazw\u0119 temu blokowi w raporcie\n",
    "        with record_function(\"1_FAST_PART\"):\n",
    "            x = self.fast_layer(x)\n",
    "            x = torch.relu(x)\n",
    "        \n",
    "        # 2. Cz\u0119\u015b\u0107 G\u0142upia (P\u0119tla w Pythonie)\n",
    "        # To zabija wydajno\u015b\u0107, bo GPU czeka na Pythona\n",
    "        with record_function(\"2_STUPID_LOOP\"):\n",
    "            # Symulujemy bezsensown\u0105 operacj\u0119\n",
    "            for _ in range(100): \n",
    "                x = x + 0.001\n",
    "        \n",
    "        # 3. Cz\u0119\u015b\u0107 Ci\u0119\u017cka (Du\u017ce macierze)\n",
    "        # Tu GPU powinno si\u0119 napoci\u0107\n",
    "        with record_function(\"3_HEAVY_MATH\"):\n",
    "            # Rozdmuchujemy x, \u017ceby pasowa\u0142 do du\u017cej warstwy\n",
    "            x_big = x.repeat(1, 40) \n",
    "            x_out = self.heavy_layer(x_big)\n",
    "            \n",
    "        return x_out\n",
    "\n",
    "model = SlowModel().to(device)\n",
    "dummy_input = torch.randn(128, 100).to(device)\n",
    "\n",
    "print(\"Model gotowy. Czas na rentgen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d3723",
   "metadata": {},
   "source": [
    "## Uruchomienie Profilera\n",
    "\n",
    "U\u017cywamy `with profile(...)`.\n",
    "*   `record_shapes=True`: Zapisuje wymiary tensor\u00f3w (pomaga znale\u017a\u0107, gdzie zjadamy pami\u0119\u0107).\n",
    "*   `with_stack=True`: Zapisuje, w kt\u00f3rej linijce kodu to si\u0119 sta\u0142o.\n",
    "\n",
    "**Wa\u017cne:** Pierwsze przej\u015bcie przez sie\u0107 jest zawsze wolne (rozgrzewka/alokacja pami\u0119ci). Profiler to poka\u017ce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a4eb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profilowanie zako\u0144czone. Generowanie raportu...\n"
     ]
    }
   ],
   "source": [
    "# Uruchamiamy profilowanie\n",
    "with profile(activities=activities, record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(dummy_input)\n",
    "\n",
    "print(\"Profilowanie zako\u0144czone. Generowanie raportu...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3287d",
   "metadata": {},
   "source": [
    "## Analiza Wynik\u00f3w\n",
    "\n",
    "Wy\u015bwietlimy tabel\u0119 posortowan\u0105 wed\u0142ug czasu **CUDA Total** (je\u015bli masz GPU) lub **CPU Total**.\n",
    "\n",
    "Czego szukamy?\n",
    "1.  **`2_STUPID_LOOP`**: Powinno mie\u0107 wysoki czas CPU, a niski lub zerowy czas CUDA (bo to Python mieli).\n",
    "2.  **`3_HEAVY_MATH`**: Powinno mie\u0107 wysoki czas CUDA (bo to ci\u0119\u017cka macierz).\n",
    "3.  **`1_FAST_PART`**: Powinno by\u0107 na dole listy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06999263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "     model_inference         0.22%     224.300us       100.00%     101.516ms     101.516ms             1  \n",
      "         1_FAST_PART         0.41%     411.600us        80.46%      81.679ms      81.679ms             1  \n",
      "        aten::linear         0.19%     196.800us        58.45%      59.339ms      29.670ms             2  \n",
      "             aten::t         0.21%     211.800us         0.54%     549.400us     274.700us             2  \n",
      "     aten::transpose         0.31%     317.600us         0.33%     337.600us     168.800us             2  \n",
      "    aten::as_strided         0.03%      30.000us         0.03%      30.000us       5.000us             6  \n",
      "         aten::addmm        57.72%      58.593ms        57.72%      58.593ms      29.296ms             2  \n",
      "          aten::relu         0.23%     235.300us        21.98%      22.314ms      22.314ms             1  \n",
      "     aten::clamp_min        21.75%      22.079ms        21.75%      22.079ms      22.079ms             1  \n",
      "       2_STUPID_LOOP         0.53%     538.000us         9.83%       9.983ms       9.983ms             1  \n",
      "           aten::add         9.30%       9.445ms         9.30%       9.445ms      94.452us           100  \n",
      "        3_HEAVY_MATH         0.20%     202.600us         9.49%       9.630ms       9.630ms             1  \n",
      "        aten::repeat         0.84%     849.200us         8.91%       9.041ms       9.041ms             1  \n",
      "        aten::expand         0.02%      15.900us         0.02%      22.800us      11.400us             2  \n",
      "         aten::empty         0.03%      29.000us         0.03%      29.000us      29.000us             1  \n",
      "--------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 101.516ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sortujemy po czasie GPU (lub CPU je\u015bli brak GPU)\n",
    "sort_key = \"cuda_time_total\" if device == \"cuda\" else \"cpu_time_total\"\n",
    "\n",
    "print(prof.key_averages().table(sort_by=sort_key, row_limit=15))\n",
    "\n",
    "# Je\u015bli tabela jest nieczytelna, sp\u00f3jrz na nazwy, kt\u00f3re sami nadali\u015bmy (1_, 2_, 3_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29f5c7",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "Jak czyta\u0107 ten raport?\n",
    "\n",
    "1.  **Addmm (Matrix Multiply):** To zazwyczaj `nn.Linear`. Je\u015bli zajmuje 90% czasu CUDA -> Zmniejsz model lub Batch Size.\n",
    "2.  **Aten::add / Aten::mul (Drobne operacje):** Je\u015bli widzisz ich tysi\u0105ce i zajmuj\u0105 du\u017co czasu CPU -> Masz p\u0119tl\u0119 `for` w kodzie. U\u017cyj `torch.compile` (Lekcja 33) lub przepisz to wektorowo.\n",
    "3.  **Memcpy (Kopiowanie):** Je\u015bli `to(device)` jest na szczycie -> U\u017cyj `num_workers` i `pin_memory` (Lekcja 18).\n",
    "\n",
    "**Zasada optymalizacji:**\n",
    "Zawsze najpierw naprawiaj to, co jest na szczycie listy (\"Low Hanging Fruit\"). Przyspieszanie ma\u0142ej warstwy (FAST_PART) nie ma sensu, je\u015bli 90% czasu zjada HEAVY_MATH."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}