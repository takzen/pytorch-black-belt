{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/notebooks/44_FSDP_Concepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b3bda",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 44: FSDP (Jak trenowa\u0107 giganty?)\n",
    "\n",
    "W DDP pami\u0119\u0107 jest ograniczona przez najs\u0142absz\u0105 kart\u0119.\n",
    "W FSDP pami\u0119\u0107 to **suma VRAM wszystkich kart**.\n",
    "\n",
    "**Koncepcja ZeRO (Zero Redundancy Optimizer):**\n",
    "Standardowy trening (Adam) zu\u017cywa pami\u0119\u0107 na:\n",
    "1.  **Parametry (Wagi):** fp32 (4 bajty).\n",
    "2.  **Gradienty:** fp32 (4 bajty).\n",
    "3.  **Stan Optymalizatora (Momentum + Variance):** fp32 (8 bajt\u00f3w).\n",
    "\n",
    "Razem: **16 bajt\u00f3w na jeden parametr**.\n",
    "Model 1B parametr\u00f3w wymaga **16 GB VRAM** (tylko na \"statyk\u0119\", bez aktywacji!).\n",
    "\n",
    "**Rozwi\u0105zanie FSDP:**\n",
    "Podzielmy te 16GB na 8 kart graficznych. Ka\u017cda trzyma tylko 2GB.\n",
    "Kiedy potrzebujemy wag do oblicze\u0144, robimy **All-Gather** (pobieramy reszt\u0119), a po obliczeniach natychmiast je kasujemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8184b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba parametr\u00f3w: 104,960,000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Symulacja wielkiego modelu (Transformer)\n",
    "# 100 milion\u00f3w parametr\u00f3w to ma\u0142o dla LLM, ale du\u017co dla laptopa\n",
    "class BigTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 12 warstw, model dimension 1024\n",
    "        self.layers = nn.Sequential(*[\n",
    "            nn.Linear(1024, 4096) for _ in range(25) # Du\u017co du\u017cych warstw\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = BigTransformer()\n",
    "\n",
    "# Liczymy parametry\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Liczba parametr\u00f3w: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8ada4",
   "metadata": {},
   "source": [
    "## Kalkulator Pami\u0119ci VRAM\n",
    "\n",
    "Zanim kupisz karty graficzne, musisz umie\u0107 policzy\u0107, czy model si\u0119 zmie\u015bci.\n",
    "Napiszmy funkcj\u0119 in\u017cyniersk\u0105, kt\u00f3ra to szacuje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c7e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SZACUNEK PAMI\u0118CI (Dla modelu 100M) ---\n",
      "1 GPU (DDP):    1.56 GB VRAM\n",
      "4 GPU (DDP):    1.56 GB VRAM (Brak zysku pami\u0119ci!)\n",
      "4 GPU (FSDP):   0.39 GB VRAM (Zysk!)\n",
      "\n",
      "--- GPT-3 (175B) ---\n",
      "Wymagane VRAM (1 GPU): 2607.70 GB\n",
      "\u017badna karta tyle nie ma (A100 ma 80GB).\n",
      "Wymagane na kart\u0119 przy 64 GPU (FSDP): 40.75 GB (To si\u0119 zmie\u015bci!)\n"
     ]
    }
   ],
   "source": [
    "def estimate_memory(params_count, num_gpus=1, use_fsdp=False):\n",
    "    # 1. Wagi (FP32) - 4 bajty\n",
    "    weights_mem = params_count * 4\n",
    "    \n",
    "    # 2. Gradienty (FP32) - 4 bajty\n",
    "    grads_mem = params_count * 4\n",
    "    \n",
    "    # 3. Optimizer (Adam trzyma 2 stany: momentum i variance) - 8 bajt\u00f3w\n",
    "    opt_mem = params_count * 8\n",
    "    \n",
    "    total_mem = weights_mem + grads_mem + opt_mem\n",
    "    \n",
    "    if use_fsdp:\n",
    "        # FSDP dzieli to wszystko przez liczb\u0119 GPU!\n",
    "        # (Teoretycznie idealne skalowanie)\n",
    "        total_mem /= num_gpus\n",
    "        \n",
    "    # Konwersja na GB\n",
    "    return total_mem / (1024**3)\n",
    "\n",
    "print(\"--- SZACUNEK PAMI\u0118CI (Dla modelu 100M) ---\")\n",
    "print(f\"1 GPU (DDP):    {estimate_memory(total_params, 1):.2f} GB VRAM\")\n",
    "print(f\"4 GPU (DDP):    {estimate_memory(total_params, 4, use_fsdp=False):.2f} GB VRAM (Brak zysku pami\u0119ci!)\")\n",
    "print(f\"4 GPU (FSDP):   {estimate_memory(total_params, 4, use_fsdp=True):.2f} GB VRAM (Zysk!)\")\n",
    "\n",
    "# A co z modelem GPT-3 (175 miliard\u00f3w parametr\u00f3w)?\n",
    "gpt3_params = 175_000_000_000\n",
    "print(f\"\\n--- GPT-3 (175B) ---\")\n",
    "print(f\"Wymagane VRAM (1 GPU): {estimate_memory(gpt3_params, 1):.2f} GB\")\n",
    "print(\"\u017badna karta tyle nie ma (A100 ma 80GB).\")\n",
    "print(f\"Wymagane na kart\u0119 przy 64 GPU (FSDP): {estimate_memory(gpt3_params, 64, True):.2f} GB (To si\u0119 zmie\u015bci!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a8223",
   "metadata": {},
   "source": [
    "## Sk\u0142adnia FSDP (Wrapper)\n",
    "\n",
    "W PyTorch FSDP dzia\u0142a podobnie do DDP \u2013 owijamy model klas\u0105.\n",
    "Ale jest haczyk: **`auto_wrap_policy`**.\n",
    "\n",
    "Nie chcemy shardingowa\u0107 byle jak (np. przeci\u0105\u0107 pojedynczy neuron na p\u00f3\u0142).\n",
    "Chcemy shardingowa\u0107 ca\u0142e bloki Transformera.\n",
    "Policy m\u00f3wi: *\"Je\u015bli warstwa ma wi\u0119cej ni\u017c 10mln parametr\u00f3w, potnij j\u0105 i rozdziel na GPU\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a9dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kod FSDP gotowy (do u\u017cycia w skrypcie torchrun).\n"
     ]
    }
   ],
   "source": [
    "# To jest kod pogl\u0105dowy (wymaga \u015brodowiska rozproszonego do uruchomienia)\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n",
    "\n",
    "def fsdp_wrapper_example(model):\n",
    "    # Polityka: Owijaj (tnij) warstwy wi\u0119ksze ni\u017c 10 milion\u00f3w parametr\u00f3w\n",
    "    my_policy = lambda module, recurse, **kwargs: size_based_auto_wrap_policy(\n",
    "        module, recurse, min_num_params=10_000_000, **kwargs\n",
    "    )\n",
    "    \n",
    "    # Owijanie (na CPU przed wys\u0142aniem na GPU, \u017ceby oszcz\u0119dzi\u0107 pami\u0119\u0107 przy starcie!)\n",
    "    sharded_model = FSDP(\n",
    "        model,\n",
    "        auto_wrap_policy=my_policy,\n",
    "        cpu_offload=None # Mo\u017cna ustawi\u0107 na True, \u017ceby zrzuci\u0107 wagi do RAMu zwyk\u0142ego!\n",
    "    )\n",
    "    \n",
    "    return sharded_model\n",
    "\n",
    "print(\"Kod FSDP gotowy (do u\u017cycia w skrypcie torchrun).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915cf6e",
   "metadata": {},
   "source": [
    "## CPU Offloading (Ostatnia deska ratunku)\n",
    "\n",
    "Co je\u015bli FSDP na 8 kartach to wci\u0105\u017c za ma\u0142o?\n",
    "FSDP ma asa w r\u0119kawie: **CPU Offload**.\n",
    "\n",
    "Wagi le\u017c\u0105 w tanim RAM-ie komputera (CPU).\n",
    "S\u0105 przesy\u0142ane na GPU tylko w momencie, gdy s\u0105 potrzebne do oblicze\u0144 (Forward/Backward), a potem natychmiast wracaj\u0105 do RAM.\n",
    "*   **Zaleta:** Mo\u017cesz trenowa\u0107 gigantyczne modele na s\u0142abych kartach.\n",
    "*   **Wada:** Jest to wolne (w\u0105skim gard\u0142em jest szyna PCIe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a100f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Offload skonfigurowany: Wagi b\u0119d\u0105 \u017cy\u0142y w RAMie, odwiedzaj\u0105c GPU tylko na chwil\u0119.\n"
     ]
    }
   ],
   "source": [
    "from torch.distributed.fsdp import CPUOffload\n",
    "\n",
    "# W\u0142\u0105czenie tej flagi pozwala trenowa\u0107 modele wi\u0119ksze ni\u017c VRAM\n",
    "offload = CPUOffload(offload_params=True)\n",
    "\n",
    "print(\"CPU Offload skonfigurowany: Wagi b\u0119d\u0105 \u017cy\u0142y w RAMie, odwiedzaj\u0105c GPU tylko na chwil\u0119.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fdfdec",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  **DDP vs FSDP:**\n",
    "    *   **DDP:** Szybkie, ale ka\u017cdy GPU musi pomie\u015bci\u0107 ca\u0142y model. (Dobre do ResNet50).\n",
    "    *   **FSDP:** Wolniejsze (du\u017co komunikacji sieciowej), ale pozwala trenowa\u0107 modele wi\u0119ksze ni\u017c pami\u0119\u0107 GPU. (Konieczne do LLM).\n",
    "2.  **ZeRO Stages (Odpowiedniki w DeepSpeed):**\n",
    "    *   Stage 1: Sharding Stanu Optymalizatora (Najwi\u0119kszy zysk, ma\u0142y narzut).\n",
    "    *   Stage 2: Sharding Gradient\u00f3w.\n",
    "    *   Stage 3: Sharding Parametr\u00f3w (Pe\u0142ne FSDP).\n",
    "3.  **Koszty:** FSDP wymaga szybkiej sieci mi\u0119dzy kartami (NVLink), inaczej karty b\u0119d\u0105 czeka\u0107 na przesy\u0142anie kawa\u0142k\u00f3w modelu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}