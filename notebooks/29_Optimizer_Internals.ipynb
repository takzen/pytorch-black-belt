{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/notebooks/29_Optimizer_Internals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dfeced",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 29: Wn\u0119trze Optymalizatora (Pisanie w\u0142asnego SGD)\n",
    "\n",
    "Optymalizator w PyTorch to klasa, kt\u00f3ra:\n",
    "1.  Przechowuje list\u0119 parametr\u00f3w (`params`) do aktualizacji.\n",
    "2.  Przechowuje stan wewn\u0119trzny (`state`) - np. p\u0119d (Momentum) dla ka\u017cdego parametru.\n",
    "3.  W metodzie `step()` wykonuje matematyk\u0119: $W_{new} = W_{old} - lr \\cdot \\nabla W$.\n",
    "\n",
    "**Kluczowa zasada:**\n",
    "Optymalizator modyfikuje wagi **In-Place** (bez tworzenia nowych tensor\u00f3w) i **bez gradient\u00f3w** (u\u017cywaj\u0105c `torch.no_grad()`).\n",
    "\n",
    "Zbudujemy w\u0142asn\u0105 klas\u0119 `MySGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ce0fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startowa waga: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Prosty problem do rozwi\u0105zania: y = 2x\n",
    "# Chcemy znale\u017a\u0107 wag\u0119 2.0\n",
    "X = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "Y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "model = nn.Linear(1, 1, bias=False)\n",
    "model.weight.data.fill_(0.0) # Startujemy od zera\n",
    "\n",
    "print(f\"Startowa waga: {model.weight.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152cc15",
   "metadata": {},
   "source": [
    "## Implementacja `MySGD`\n",
    "\n",
    "Musimy dziedziczy\u0107 po `torch.optim.Optimizer`.\n",
    "Wymaga to zdefiniowania metody `step()`.\n",
    "\n",
    "Zwr\u00f3\u0107 uwag\u0119 na p\u0119tl\u0119 `for group in self.param_groups`.\n",
    "To dlatego mo\u017cemy napisa\u0107:\n",
    "`optim.SGD([{'params': model.fc1.parameters(), 'lr': 0.1}, {'params': rest, 'lr': 0.01}])`\n",
    "Optymalizator traktuje ka\u017cd\u0105 grup\u0119 osobno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545f4188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\u0142asny SGD gotowy.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "class MySGD(Optimizer):\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        # P\u0119tla po grupach parametr\u00f3w (zazwyczaj jest jedna, ale mo\u017ce by\u0107 wi\u0119cej)\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            \n",
    "            # P\u0119tla po parametrach w grupie (Wagi, Biasy)\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # --- TUTAJ DZIEJE SI\u0118 MAGIA ---\n",
    "                # Modyfikujemy wag\u0119 w oparciu o gradient.\n",
    "                # Musimy u\u017cy\u0107 no_grad(), \u017ceby ta operacja nie trafi\u0142a do grafu!\n",
    "                with torch.no_grad():\n",
    "                    # W = W - lr * grad\n",
    "                    p.sub_(lr * p.grad)\n",
    "                    \n",
    "        return None\n",
    "\n",
    "# Inicjalizacja naszego optymalizatora\n",
    "optimizer = MySGD(model.parameters(), lr=0.1)\n",
    "print(\"W\u0142asny SGD gotowy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac6aa2",
   "metadata": {},
   "source": [
    "## Test: Czy to dzia\u0142a?\n",
    "\n",
    "Uruchomimy standardow\u0105 p\u0119tl\u0119 treningow\u0105.\n",
    "Je\u015bli waga `model.weight` zacznie zbli\u017ca\u0107 si\u0119 do 2.0, to znaczy, \u017ce nasz kod dzia\u0142a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4cbcd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waga przed: 0.0000\n",
      "Epoka 1: Loss=30.0000, Waga=3.0000\n",
      "Epoka 2: Loss=7.5000, Waga=1.5000\n",
      "Epoka 3: Loss=1.8750, Waga=2.2500\n",
      "Epoka 4: Loss=0.4688, Waga=1.8750\n",
      "Epoka 5: Loss=0.1172, Waga=2.0625\n",
      "Epoka 6: Loss=0.0293, Waga=1.9688\n",
      "Epoka 7: Loss=0.0073, Waga=2.0156\n",
      "Epoka 8: Loss=0.0018, Waga=1.9922\n",
      "Epoka 9: Loss=0.0005, Waga=2.0039\n",
      "Epoka 10: Loss=0.0001, Waga=1.9980\n",
      "\u2705 Sukces! Waga d\u0105\u017cy do 2.0.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"Waga przed: {model.weight.item():.4f}\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    # 1. Forward\n",
    "    # Musimy zmieni\u0107 kszta\u0142t X na [N, 1] bo Linear tego oczekuje\n",
    "    pred = model(X.unsqueeze(1))\n",
    "    \n",
    "    # 2. Loss\n",
    "    loss = criterion(pred, Y.unsqueeze(1))\n",
    "    \n",
    "    # 3. Zero Grad (To te\u017c metoda Optimizera, ale dziedziczymy j\u0105 z klasy bazowej)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backward (Tu PyTorch liczy .grad dla ka\u017cdego parametru)\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Step (Tu nasz MySGD odejmuje gradient od wagi)\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoka {epoch+1}: Loss={loss.item():.4f}, Waga={model.weight.item():.4f}\")\n",
    "\n",
    "print(\"\u2705 Sukces! Waga d\u0105\u017cy do 2.0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5e571",
   "metadata": {},
   "source": [
    "## Level Up: Dodajemy Momentum (P\u0119d)\n",
    "\n",
    "Zwyk\u0142y SGD utyka w lokalnych do\u0142kach.\n",
    "Momentum dzia\u0142a jak ci\u0119\u017cka kulka. Je\u015bli gradient m\u00f3wi \"Stop\", kulka toczy si\u0119 dalej si\u0142\u0105 rozp\u0119du.\n",
    "\n",
    "Wz\u00f3r:\n",
    "$$ v_{t} = \\mu \\cdot v_{t-1} + g_{t} $$\n",
    "$$ w_{t} = w_{t-1} - lr \\cdot v_{t} $$\n",
    "\n",
    "Musimy u\u017cy\u0107 s\u0142ownika `self.state`, \u017ceby zapami\u0119ta\u0107 pr\u0119dko\u015b\u0107 ($v$) dla ka\u017cdego parametru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93bff21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST MOMENTUM ---\n",
      "Krok 1: Waga=3.0000\n",
      "Krok 2: Waga=4.2000\n"
     ]
    }
   ],
   "source": [
    "class MyMomentumSGD(Optimizer):\n",
    "    def __init__(self, params, lr=0.01, momentum=0.9):\n",
    "        defaults = dict(lr=lr, momentum=momentum)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            mu = group['momentum']\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # Pobieramy stan dla tego konkretnego parametru\n",
    "                state = self.state[p]\n",
    "                \n",
    "                # Je\u015bli to pierwszy krok, inicjalizujemy bufor pr\u0119dko\u015bci zerami\n",
    "                if 'velocity' not in state:\n",
    "                    state['velocity'] = torch.zeros_like(p.data)\n",
    "                \n",
    "                buf = state['velocity']\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # 1. Aktualizujemy pr\u0119dko\u015b\u0107: v = mu * v + grad\n",
    "                    buf.mul_(mu).add_(p.grad)\n",
    "                    \n",
    "                    # 2. Aktualizujemy wag\u0119: w = w - lr * v\n",
    "                    p.sub_(lr * buf)\n",
    "\n",
    "# Resetujemy model i testujemy Momentum\n",
    "model.weight.data.fill_(0.0)\n",
    "opt_mom = MyMomentumSGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "print(\"\\n--- TEST MOMENTUM ---\")\n",
    "# Zrobimy tylko 2 kroki, \u017ceby zobaczy\u0107 \"rozp\u0119dzanie\"\n",
    "for i in range(2):\n",
    "    opt_mom.zero_grad()\n",
    "    loss = criterion(model(X.unsqueeze(1)), Y.unsqueeze(1))\n",
    "    loss.backward()\n",
    "    opt_mom.step()\n",
    "    print(f\"Krok {i+1}: Waga={model.weight.item():.4f}\")\n",
    "\n",
    "# Zauwa\u017c: W kroku 2 zmiana powinna by\u0107 WI\u0118KSZA ni\u017c w kroku 1 (bo nabrali\u015bmy p\u0119du!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce5db0",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  **Optymalizator jest \"g\u0142upi\":** On nie wie, co to jest sie\u0107 neuronowa. On po prostu dostaje list\u0119 tensor\u00f3w i odejmuje od nich ich `.grad`.\n",
    "2.  **Stan (`state`):** To tutaj `Adam` trzyma swoje \u015brednie ruchome (m i v), a `SGD` trzyma p\u0119d. To dlatego optymalizatory zajmuj\u0105 pami\u0119\u0107 VRAM! (Adam zu\u017cywa 2x wi\u0119cej pami\u0119ci na parametry ni\u017c SGD).\n",
    "3.  **In-place:** Wszystkie operacje wewn\u0105trz `step()` musz\u0105 by\u0107 in-place (`sub_`, `add_`) i w bloku `no_grad()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}