{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/notebooks/49_Custom_Loss_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6a3d3",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 49: Custom Loss Functions (Triplet Loss & Vectorization)\n",
    "\n",
    "Pisanie w\u0142asnej funkcji kosztu w PyTorch jest proste: wystarczy napisa\u0107 funkcj\u0119, kt\u00f3ra przyjmuje Tensory i zwraca skalar, u\u017cywaj\u0105c operacji r\u00f3\u017cniczkowalnych PyTorcha.\n",
    "\n",
    "Trudno\u015b\u0107 le\u017cy w **wydajno\u015bci** i **stabilno\u015bci numerycznej**.\n",
    "\n",
    "**Studium przypadku: Triplet Loss**\n",
    "Chcemy nauczy\u0107 sie\u0107, \u017ce:\n",
    "*   Twarz A (Anchor) jest podobna do Twarzy P (Positive).\n",
    "*   Twarz A jest r\u00f3\u017cna od Twarzy N (Negative).\n",
    "\n",
    "Wz\u00f3r:\n",
    "$$ L = \\max(0, \\text{dist}(A, P) - \\text{dist}(A, N) + \\text{margin}) $$\n",
    "\n",
    "Wyzwaniem jest obliczenie odleg\u0142o\u015bci euklidesowej dla ca\u0142ego batcha naraz, bez p\u0119tli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ef062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urz\u0105dzenie: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Urz\u0105dzenie: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52fe990",
   "metadata": {},
   "source": [
    "## Wersja 1: Naiwna (Powolna)\n",
    "\n",
    "Zaimplementujmy to \"po ludzku\", u\u017cywaj\u0105c wbudowanej funkcji `pairwise_distance`.\n",
    "To dzia\u0142a, ale w bardziej skomplikowanych wariantach (np. szukanie najtrudniejszych negatyw\u00f3w w batchu - Hard Mining) wymaga\u0142oby p\u0119tli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b86bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Loss: 1.4534\n"
     ]
    }
   ],
   "source": [
    "class NaiveTripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # anchor, positive, negative: [Batch, Embed_Dim]\n",
    "        \n",
    "        # 1. Liczymy dystanse\n",
    "        dist_pos = F.pairwise_distance(anchor, positive, p=2)\n",
    "        dist_neg = F.pairwise_distance(anchor, negative, p=2)\n",
    "        \n",
    "        # 2. Wz\u00f3r Hinge Loss\n",
    "        loss = torch.relu(dist_pos - dist_neg + self.margin)\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "# Test\n",
    "criterion_naive = NaiveTripletLoss()\n",
    "a = torch.randn(32, 128, requires_grad=True).to(DEVICE)\n",
    "p = torch.randn(32, 128, requires_grad=True).to(DEVICE)\n",
    "n = torch.randn(32, 128, requires_grad=True).to(DEVICE)\n",
    "\n",
    "loss = criterion_naive(a, p, n)\n",
    "print(f\"Naive Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb83dd2",
   "metadata": {},
   "source": [
    "## Wersja 2: Professional (Macierzowa)\n",
    "\n",
    "W zaawansowanych systemach (np. SimCLR, Metric Learning) cz\u0119sto musimy policzy\u0107 macierz odleg\u0142o\u015bci **ka\u017cdy z ka\u017cdym** wewn\u0105trz batcha.\n",
    "U\u017cycie p\u0119tli jest tu zab\u00f3jcze.\n",
    "\n",
    "U\u017cyjemy wzoru skr\u00f3conego mno\u017cenia dla odleg\u0142o\u015bci euklidesowej:\n",
    "$$ ||A - B||^2 = ||A||^2 + ||B||^2 - 2 \\cdot A \\cdot B^T $$\n",
    "\n",
    "Dzi\u0119ki temu mo\u017cemy u\u017cy\u0107 ultraszybkiego mno\u017cenia macierzy (`@` lub `matmul`).\n",
    "\n",
    "**Pu\u0142apka NaN:**\n",
    "Pochodna z $\\sqrt{x}$ to $\\frac{1}{2\\sqrt{x}}$.\n",
    "Je\u015bli $x=0$ (dystans wynosi zero, bo obrazy s\u0105 identyczne), mianownik wynosi 0 -> Gradient wybucha do `inf` -> Wagi staj\u0105 si\u0119 `NaN`.\n",
    "Musimy doda\u0107 ma\u0142y $\\epsilon$ przed pierwiastkowaniem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea98a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zaawansowana funkcja kosztu gotowa.\n"
     ]
    }
   ],
   "source": [
    "def pairwise_distance_matrix(x, y):\n",
    "    \"\"\"\n",
    "    Oblicza dystans Euklidesowy mi\u0119dzy ka\u017cdym elementem x a ka\u017cdym elementem y.\n",
    "    x: [N, D]\n",
    "    y: [M, D]\n",
    "    Wynik: [N, M]\n",
    "    \"\"\"\n",
    "    # 1. Kwadraty norm\n",
    "    x_sq = torch.sum(x**2, dim=1, keepdim=True) # [N, 1]\n",
    "    y_sq = torch.sum(y**2, dim=1, keepdim=True) # [M, 1] -> transponujemy wirtualnie do [1, M]\n",
    "    \n",
    "    # 2. Iloczyn skalarny (2ab)\n",
    "    # [N, D] @ [D, M] -> [N, M]\n",
    "    prod = torch.matmul(x, y.t())\n",
    "    \n",
    "    # 3. Wz\u00f3r (a^2 + b^2 - 2ab)\n",
    "    # Broadcasting zadba o wymiary: [N, 1] + [1, M] - [N, M] -> [N, M]\n",
    "    dist_sq = x_sq + y_sq.t() - 2 * prod\n",
    "    \n",
    "    # 4. Zabezpieczenie przed ujemnymi zerami (b\u0142\u0119dy float)\n",
    "    dist_sq = torch.clamp(dist_sq, min=1e-12)\n",
    "    \n",
    "    return torch.sqrt(dist_sq)\n",
    "\n",
    "class AdvancedTripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Tutaj liczymy tylko pary (i, i), ale dzi\u0119ki funkcji macierzowej\n",
    "        # mogliby\u015bmy \u0142atwo zaimplementowa\u0107 \"Batch Hard Mining\" (najtrudniejszy negatyw w ca\u0142ym batchu).\n",
    "        \n",
    "        # Obliczamy dystanse\n",
    "        # Uwaga: funkcja zwraca macierz NxN, my chcemy tylko przek\u0105tn\u0105 (odleg\u0142o\u015b\u0107 pary i-i)\n",
    "        # Ale dla edukacji u\u017cyjemy tej funkcji.\n",
    "        \n",
    "        # Dystans A-P\n",
    "        dists_ap = pairwise_distance_matrix(anchor, positive)\n",
    "        # Bierzemy przek\u0105tn\u0105 (dystans mi\u0119dzy anchor[i] a positive[i])\n",
    "        d_ap = torch.diag(dists_ap)\n",
    "        \n",
    "        # Dystans A-N\n",
    "        dists_an = pairwise_distance_matrix(anchor, negative)\n",
    "        d_an = torch.diag(dists_an)\n",
    "        \n",
    "        loss = torch.relu(d_ap - d_an + self.margin)\n",
    "        return loss.mean()\n",
    "\n",
    "print(\"Zaawansowana funkcja kosztu gotowa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e686d0",
   "metadata": {},
   "source": [
    "## Weryfikacja: Gradienty i NaN\n",
    "\n",
    "Sprawd\u017amy, czy nasza funkcja jest stabilna.\n",
    "Stworzymy przypadek, gdzie `anchor == positive` (dystans = 0).\n",
    "W naiwnej implementacji (bez epsilora) `backward()` m\u00f3g\u0142by zwr\u00f3ci\u0107 `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "227b74e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss przy idealnym dopasowaniu: 0.0\n",
      "Gradient Anchora (norma): 0.0\n",
      "\u2705 SUKCES: Gradient jest stabilny (dzi\u0119ki clamp/epsilon).\n"
     ]
    }
   ],
   "source": [
    "criterion_adv = AdvancedTripletLoss()\n",
    "\n",
    "# --- POPRAWKA ---\n",
    "# Tworzymy tensor BEZPO\u015aREDNIO na urz\u0105dzeniu (device=DEVICE).\n",
    "# Dzi\u0119ki temu 'a_zero' jest Li\u015bciem (Leaf Tensor) i jego .grad zostanie zachowany.\n",
    "a_zero = torch.randn(5, 10, device=DEVICE, requires_grad=True)\n",
    "\n",
    "# p_zero to klon a_zero.\n",
    "# Uwaga: p_zero nie jest li\u015bciem (jest wynikiem klonowania), \n",
    "# ale nas interesuje gradient na 'a_zero', wi\u0119c jest OK.\n",
    "p_zero = a_zero.clone() \n",
    "\n",
    "n_zero = torch.randn(5, 10, device=DEVICE, requires_grad=True)\n",
    "\n",
    "# Liczymy strat\u0119\n",
    "loss = criterion_adv(a_zero, p_zero, n_zero)\n",
    "\n",
    "print(f\"Loss przy idealnym dopasowaniu: {loss.item()}\")\n",
    "\n",
    "# Pr\u00f3ba Backward\n",
    "try:\n",
    "    loss.backward()\n",
    "    \n",
    "    # Teraz a_zero.grad b\u0119dzie istnia\u0142 i nie b\u0119dzie ostrze\u017cenia\n",
    "    grad_norm = a_zero.grad.norm().item()\n",
    "    print(f\"Gradient Anchora (norma): {grad_norm}\")\n",
    "    \n",
    "    if torch.isnan(a_zero.grad).any():\n",
    "        print(\"\u274c B\u0141\u0104D: Gradient to NaN! (Dzielenie przez zero w pierwiastku)\")\n",
    "    else:\n",
    "        print(\"\u2705 SUKCES: Gradient jest stabilny (dzi\u0119ki clamp/epsilon).\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"B\u0142\u0105d: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde688f3",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  **Unikaj p\u0119tli `for`** w funkcjach kosztu. Je\u015bli masz batcha, u\u017cywaj operacji macierzowych (`matmul`, broadcasting).\n",
    "2.  **`clamp(min=1e-8)`**: Zawsze u\u017cywaj tego przed pierwiastkowaniem (`sqrt`) lub logarytmowaniem (`log`). W Deep Learningu zero jest Twoim wrogiem przy liczeniu pochodnych.\n",
    "3.  **Wz\u00f3r skr\u00f3conego mno\u017cenia:** $||a-b||^2 = a^2 + b^2 - 2ab$ to najszybszy spos\u00f3b na policzenie macierzy odleg\u0142o\u015bci na GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}