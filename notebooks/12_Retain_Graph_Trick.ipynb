{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/notebooks/12_Retain_Graph_Trick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1edb4cd",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 12: Retain Graph (\u017bycie po \u015bmierci grafu)\n",
    "\n",
    "Domy\u015blny cykl \u017cycia w PyTorch:\n",
    "1.  **Forward:** Budujemy graf, zapisujemy tensory po\u015brednie.\n",
    "2.  **Backward:** U\u017cywamy grafu do policzenia gradient\u00f3w.\n",
    "3.  **Destrukcja:** Graf jest usuwany z pami\u0119ci (Free Memory).\n",
    "\n",
    "Je\u015bli spr\u00f3bujesz zrobi\u0107 `.backward()` drugi raz na tym samym wyj\u015bciu, dostaniesz b\u0142\u0105d, bo \"mapa drogowa\" ju\u017c nie istnieje.\n",
    "\n",
    "**Kiedy potrzebujemy `retain_graph=True`?**\n",
    "1.  **Multi-Task Learning:** Masz jedn\u0105 sie\u0107, ale dwie r\u00f3\u017cne funkcje straty (Loss A i Loss B), kt\u00f3re chcesz aplikowa\u0107 sekwencyjnie.\n",
    "2.  **Wizualizacja:** Chcesz podejrze\u0107 gradienty przed wykonaniem \"prawdziwego\" kroku.\n",
    "3.  **GANy:** Czasami przy skomplikowanych p\u0119tlach treningowych Dyskryminatora i Generatora.\n",
    "\n",
    "Zasymulujemy ten b\u0142\u0105d i go naprawimy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220dece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graf zbudowany.\n",
      "Loss fn: <SumBackward0 object at 0x000001F0FBFB0520>\n",
      "Pierwszy backward: SUKCES\n",
      "\n",
      "\ud83d\udeab B\u0141\u0104D (Zgodnie z planem):\n",
      "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Prosta sie\u0107\n",
    "x = torch.randn(1, 10)\n",
    "w = torch.randn(10, 1, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = x @ w\n",
    "loss = y.sum()\n",
    "\n",
    "print(\"Graf zbudowany.\")\n",
    "print(f\"Loss fn: {loss.grad_fn}\")\n",
    "\n",
    "# Pierwszy Backward - Standardowy\n",
    "loss.backward()\n",
    "print(\"Pierwszy backward: SUKCES\")\n",
    "\n",
    "# Drugi Backward - Na tym samym grafie\n",
    "try:\n",
    "    loss.backward()\n",
    "except RuntimeError as e:\n",
    "    print(\"\\n\ud83d\udeab B\u0141\u0104D (Zgodnie z planem):\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3041f",
   "metadata": {},
   "source": [
    "## Scenariusz: Dwie niezale\u017cne straty\n",
    "\n",
    "Wyobra\u017a sobie, \u017ce trenujesz model, kt\u00f3ry ma:\n",
    "1.  Dobrze klasyfikowa\u0107 obrazki (`Loss_Main`).\n",
    "2.  Mie\u0107 ma\u0142e wagi (Regularyzacja L2 - `Loss_Reg`).\n",
    "\n",
    "Chcesz policzy\u0107 wp\u0142yw obu tych strat oddzielnie (np. \u017ceby zalogowa\u0107 gradienty dla ka\u017cdej z nich osobno przed zsumowaniem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c960e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamy dwie straty wisz\u0105ce na jednym grafie.\n",
      "Gradient po Loss Main: tensor([-2.3429, -5.5241, -4.7889])... (cz\u0119\u015b\u0107)\n",
      "Gradient po obu stratach: tensor([-1.1643, -7.6936, -4.6166])... (suma)\n"
     ]
    }
   ],
   "source": [
    "# Resetujemy wagi i gradienty\n",
    "w = torch.randn(10, 1, requires_grad=True)\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "# 1. Forward\n",
    "y = x @ w\n",
    "\n",
    "# 2. Definiujemy dwie r\u00f3\u017cne straty na podstawie tego samego 'y'\n",
    "loss_main = (y - 1).pow(2).sum()  # Chcemy, \u017ceby wynik by\u0142 1\n",
    "loss_reg  = w.pow(2).sum()        # Chcemy, \u017ceby wagi by\u0142y ma\u0142e\n",
    "\n",
    "print(\"Mamy dwie straty wisz\u0105ce na jednym grafie.\")\n",
    "\n",
    "# 3. Backward dla pierwszej straty\n",
    "# WA\u017bNE: retain_graph=True\n",
    "# M\u00f3wimy: \"Policz gradienty dla loss_main, ale NIE NISZCZ grafu (x@w), bo loss_reg te\u017c go potrzebuje!\"\n",
    "loss_main.backward(retain_graph=True)\n",
    "\n",
    "print(f\"Gradient po Loss Main: {w.grad.view(-1)[:3]}... (cz\u0119\u015b\u0107)\")\n",
    "\n",
    "# 4. Backward dla drugiej straty\n",
    "# Teraz mo\u017cemy zniszczy\u0107 graf (domy\u015blnie retain_graph=False)\n",
    "loss_reg.backward()\n",
    "\n",
    "# Gradienty si\u0119 ZSUMOWA\u0141Y (Accumulation)\n",
    "print(f\"Gradient po obu stratach: {w.grad.view(-1)[:3]}... (suma)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000f3db",
   "metadata": {},
   "source": [
    "## `retain_graph` vs `create_graph`\n",
    "\n",
    "To cz\u0119ste nieporozumienie na rekrutacjach.\n",
    "\n",
    "1.  **`retain_graph=True`**:\n",
    "    *   \"Nie kasuj bufor\u00f3w po\u015brednich po backwardzie\".\n",
    "    *   Potrzebne, gdy robisz **wiele backward\u00f3w na tym samym forwardzie**.\n",
    "\n",
    "2.  **`create_graph=True`**:\n",
    "    *   \"Traktuj proces liczenia gradientu jako operacj\u0119, kt\u00f3r\u0105 te\u017c mo\u017cna r\u00f3\u017cniczkowa\u0107\".\n",
    "    *   Buduje graf pochodnej.\n",
    "    *   Potrzebne do **pochodnych wy\u017cszego rz\u0119du** (Hessian, MAML - notatnik 11 i 75).\n",
    "\n",
    "**Przyk\u0142ad:** Czy `retain_graph` zu\u017cywa du\u017co pami\u0119ci?\n",
    "Tak! Trzyma w VRAM ca\u0142\u0105 histori\u0119 aktywacji. Dlatego u\u017cywaj tego tylko wtedy, gdy musisz. W 99% przypadk\u00f3w lepiej zsumowa\u0107 straty (`total_loss = loss1 + loss2`) i zrobi\u0107 jeden `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c96ef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pami\u0119\u0107 zwolniona.\n"
     ]
    }
   ],
   "source": [
    "# TEST PAMI\u0118CI (Zrozumienie ryzyka)\n",
    "\n",
    "# Du\u017cy tensor\n",
    "huge = torch.randn(1000, 1000, requires_grad=True)\n",
    "y = huge * 2\n",
    "loss = y.sum()\n",
    "\n",
    "# Backward z zatrzymaniem grafu\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# Tutaj graf (i tensor 'huge' w pami\u0119ci grafu) NADAL WISI w RAM.\n",
    "# Dopiero gdy zrobimy kolejny backward bez retain, albo usuniemy zmienn\u0105, pami\u0119\u0107 zostanie zwolniona.\n",
    "\n",
    "loss.backward(retain_graph=False) \n",
    "# Teraz graf posprz\u0105tany.\n",
    "print(\"Pami\u0119\u0107 zwolniona.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f257a81",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  Domy\u015blnie PyTorch jest **agresywny w sprz\u0105taniu**. Po `.backward()` bufory znikaj\u0105.\n",
    "2.  U\u017cywaj `retain_graph=True` **TYLKO** wtedy, gdy musisz wywo\u0142a\u0107 `.backward()` wielokrotnie na tym samym pod-grafie.\n",
    "3.  **Alternatywa:** Zamiast robi\u0107 dwa backwardy:\n",
    "    ```python\n",
    "    loss1.backward(retain_graph=True)\n",
    "    loss2.backward()\n",
    "    ```\n",
    "    Zazwyczaj lepiej (szybciej i l\u017cej dla pami\u0119ci) jest zrobi\u0107:\n",
    "    ```python\n",
    "    total_loss = loss1 + loss2\n",
    "    total_loss.backward()\n",
    "    ```\n",
    "    Wtedy PyTorch sam ogarnie graf raz, a porz\u0105dnie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}