{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5dfeced",
   "metadata": {},
   "source": [
    "# ðŸ¥‹ Lekcja 29: WnÄ™trze Optymalizatora (Pisanie wÅ‚asnego SGD)\n",
    "\n",
    "Optymalizator w PyTorch to klasa, ktÃ³ra:\n",
    "1.  Przechowuje listÄ™ parametrÃ³w (`params`) do aktualizacji.\n",
    "2.  Przechowuje stan wewnÄ™trzny (`state`) - np. pÄ™d (Momentum) dla kaÅ¼dego parametru.\n",
    "3.  W metodzie `step()` wykonuje matematykÄ™: $W_{new} = W_{old} - lr \\cdot \\nabla W$.\n",
    "\n",
    "**Kluczowa zasada:**\n",
    "Optymalizator modyfikuje wagi **In-Place** (bez tworzenia nowych tensorÃ³w) i **bez gradientÃ³w** (uÅ¼ywajÄ…c `torch.no_grad()`).\n",
    "\n",
    "Zbudujemy wÅ‚asnÄ… klasÄ™ `MySGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ce0fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startowa waga: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Prosty problem do rozwiÄ…zania: y = 2x\n",
    "# Chcemy znaleÅºÄ‡ wagÄ™ 2.0\n",
    "X = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "Y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "model = nn.Linear(1, 1, bias=False)\n",
    "model.weight.data.fill_(0.0) # Startujemy od zera\n",
    "\n",
    "print(f\"Startowa waga: {model.weight.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152cc15",
   "metadata": {},
   "source": [
    "## Implementacja `MySGD`\n",
    "\n",
    "Musimy dziedziczyÄ‡ po `torch.optim.Optimizer`.\n",
    "Wymaga to zdefiniowania metody `step()`.\n",
    "\n",
    "ZwrÃ³Ä‡ uwagÄ™ na pÄ™tlÄ™ `for group in self.param_groups`.\n",
    "To dlatego moÅ¼emy napisaÄ‡:\n",
    "`optim.SGD([{'params': model.fc1.parameters(), 'lr': 0.1}, {'params': rest, 'lr': 0.01}])`\n",
    "Optymalizator traktuje kaÅ¼dÄ… grupÄ™ osobno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545f4188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WÅ‚asny SGD gotowy.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "class MySGD(Optimizer):\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        # PÄ™tla po grupach parametrÃ³w (zazwyczaj jest jedna, ale moÅ¼e byÄ‡ wiÄ™cej)\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            \n",
    "            # PÄ™tla po parametrach w grupie (Wagi, Biasy)\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # --- TUTAJ DZIEJE SIÄ˜ MAGIA ---\n",
    "                # Modyfikujemy wagÄ™ w oparciu o gradient.\n",
    "                # Musimy uÅ¼yÄ‡ no_grad(), Å¼eby ta operacja nie trafiÅ‚a do grafu!\n",
    "                with torch.no_grad():\n",
    "                    # W = W - lr * grad\n",
    "                    p.sub_(lr * p.grad)\n",
    "                    \n",
    "        return None\n",
    "\n",
    "# Inicjalizacja naszego optymalizatora\n",
    "optimizer = MySGD(model.parameters(), lr=0.1)\n",
    "print(\"WÅ‚asny SGD gotowy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac6aa2",
   "metadata": {},
   "source": [
    "## Test: Czy to dziaÅ‚a?\n",
    "\n",
    "Uruchomimy standardowÄ… pÄ™tlÄ™ treningowÄ….\n",
    "JeÅ›li waga `model.weight` zacznie zbliÅ¼aÄ‡ siÄ™ do 2.0, to znaczy, Å¼e nasz kod dziaÅ‚a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4cbcd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waga przed: 0.0000\n",
      "Epoka 1: Loss=30.0000, Waga=3.0000\n",
      "Epoka 2: Loss=7.5000, Waga=1.5000\n",
      "Epoka 3: Loss=1.8750, Waga=2.2500\n",
      "Epoka 4: Loss=0.4688, Waga=1.8750\n",
      "Epoka 5: Loss=0.1172, Waga=2.0625\n",
      "Epoka 6: Loss=0.0293, Waga=1.9688\n",
      "Epoka 7: Loss=0.0073, Waga=2.0156\n",
      "Epoka 8: Loss=0.0018, Waga=1.9922\n",
      "Epoka 9: Loss=0.0005, Waga=2.0039\n",
      "Epoka 10: Loss=0.0001, Waga=1.9980\n",
      "âœ… Sukces! Waga dÄ…Å¼y do 2.0.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"Waga przed: {model.weight.item():.4f}\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    # 1. Forward\n",
    "    # Musimy zmieniÄ‡ ksztaÅ‚t X na [N, 1] bo Linear tego oczekuje\n",
    "    pred = model(X.unsqueeze(1))\n",
    "    \n",
    "    # 2. Loss\n",
    "    loss = criterion(pred, Y.unsqueeze(1))\n",
    "    \n",
    "    # 3. Zero Grad (To teÅ¼ metoda Optimizera, ale dziedziczymy jÄ… z klasy bazowej)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backward (Tu PyTorch liczy .grad dla kaÅ¼dego parametru)\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Step (Tu nasz MySGD odejmuje gradient od wagi)\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoka {epoch+1}: Loss={loss.item():.4f}, Waga={model.weight.item():.4f}\")\n",
    "\n",
    "print(\"âœ… Sukces! Waga dÄ…Å¼y do 2.0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5e571",
   "metadata": {},
   "source": [
    "## Level Up: Dodajemy Momentum (PÄ™d)\n",
    "\n",
    "ZwykÅ‚y SGD utyka w lokalnych doÅ‚kach.\n",
    "Momentum dziaÅ‚a jak ciÄ™Å¼ka kulka. JeÅ›li gradient mÃ³wi \"Stop\", kulka toczy siÄ™ dalej siÅ‚Ä… rozpÄ™du.\n",
    "\n",
    "WzÃ³r:\n",
    "$$ v_{t} = \\mu \\cdot v_{t-1} + g_{t} $$\n",
    "$$ w_{t} = w_{t-1} - lr \\cdot v_{t} $$\n",
    "\n",
    "Musimy uÅ¼yÄ‡ sÅ‚ownika `self.state`, Å¼eby zapamiÄ™taÄ‡ prÄ™dkoÅ›Ä‡ ($v$) dla kaÅ¼dego parametru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93bff21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST MOMENTUM ---\n",
      "Krok 1: Waga=3.0000\n",
      "Krok 2: Waga=4.2000\n"
     ]
    }
   ],
   "source": [
    "class MyMomentumSGD(Optimizer):\n",
    "    def __init__(self, params, lr=0.01, momentum=0.9):\n",
    "        defaults = dict(lr=lr, momentum=momentum)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            mu = group['momentum']\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # Pobieramy stan dla tego konkretnego parametru\n",
    "                state = self.state[p]\n",
    "                \n",
    "                # JeÅ›li to pierwszy krok, inicjalizujemy bufor prÄ™dkoÅ›ci zerami\n",
    "                if 'velocity' not in state:\n",
    "                    state['velocity'] = torch.zeros_like(p.data)\n",
    "                \n",
    "                buf = state['velocity']\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # 1. Aktualizujemy prÄ™dkoÅ›Ä‡: v = mu * v + grad\n",
    "                    buf.mul_(mu).add_(p.grad)\n",
    "                    \n",
    "                    # 2. Aktualizujemy wagÄ™: w = w - lr * v\n",
    "                    p.sub_(lr * buf)\n",
    "\n",
    "# Resetujemy model i testujemy Momentum\n",
    "model.weight.data.fill_(0.0)\n",
    "opt_mom = MyMomentumSGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "print(\"\\n--- TEST MOMENTUM ---\")\n",
    "# Zrobimy tylko 2 kroki, Å¼eby zobaczyÄ‡ \"rozpÄ™dzanie\"\n",
    "for i in range(2):\n",
    "    opt_mom.zero_grad()\n",
    "    loss = criterion(model(X.unsqueeze(1)), Y.unsqueeze(1))\n",
    "    loss.backward()\n",
    "    opt_mom.step()\n",
    "    print(f\"Krok {i+1}: Waga={model.weight.item():.4f}\")\n",
    "\n",
    "# ZauwaÅ¼: W kroku 2 zmiana powinna byÄ‡ WIÄ˜KSZA niÅ¼ w kroku 1 (bo nabraliÅ›my pÄ™du!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce5db0",
   "metadata": {},
   "source": [
    "## ðŸ¥‹ Black Belt Summary\n",
    "\n",
    "1.  **Optymalizator jest \"gÅ‚upi\":** On nie wie, co to jest sieÄ‡ neuronowa. On po prostu dostaje listÄ™ tensorÃ³w i odejmuje od nich ich `.grad`.\n",
    "2.  **Stan (`state`):** To tutaj `Adam` trzyma swoje Å›rednie ruchome (m i v), a `SGD` trzyma pÄ™d. To dlatego optymalizatory zajmujÄ… pamiÄ™Ä‡ VRAM! (Adam zuÅ¼ywa 2x wiÄ™cej pamiÄ™ci na parametry niÅ¼ SGD).\n",
    "3.  **In-place:** Wszystkie operacje wewnÄ…trz `step()` muszÄ… byÄ‡ in-place (`sub_`, `add_`) i w bloku `no_grad()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
