{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/28_Model_Surgery.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f0a15",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 28: Model Surgery (Przeszczepianie Warstw)\n",
    "\n",
    "Bierzemy gotowy model (`pretrained=True`), ale musimy go dostosowa\u0107 do naszych danych.\n",
    "\n",
    "1.  **Head Replacement (Proste):** Podmieniamy ostatni\u0105 warstw\u0119 liniow\u0105 (`fc` lub `classifier`), \u017ceby zmieni\u0107 liczb\u0119 klas.\n",
    "2.  **Stem Replacement (Trudne):** Podmieniamy pierwsz\u0105 warstw\u0119 konwolucyjn\u0105 (`conv1`), \u017ceby zmieni\u0107 liczb\u0119 kana\u0142\u00f3w wej\u015bciowych.\n",
    "\n",
    "**Black Belt Trick:**\n",
    "Je\u015bli zmieniamy wej\u015bcie z 3 kana\u0142\u00f3w (RGB) na 1 kana\u0142 (Grayscale), nie inicjalizujemy nowej warstwy losowo!\n",
    "Bierzemy wagi z oryginalnej warstwy RGB, **u\u015bredniamy je** i wk\u0142adamy do nowej warstwy.\n",
    "Dzi\u0119ki temu model od razu \"umie\" wykrywa\u0107 kraw\u0119dzie i kszta\u0142ty, zamiast uczy\u0107 si\u0119 widzenia od nowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d48ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ORYGINA\u0141 ---\n",
      "Wej\u015bcie (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "Wyj\u015bcie (fc):    Linear(in_features=512, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# 1. Pacjent: ResNet18 (Wytrenowany na ImageNet)\n",
    "# W nowych wersjach torchvision u\u017cywamy weights=...\n",
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "print(\"--- ORYGINA\u0141 ---\")\n",
    "print(f\"Wej\u015bcie (conv1): {model.conv1}\")\n",
    "print(f\"Wyj\u015bcie (fc):    {model.fc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd618a4",
   "metadata": {},
   "source": [
    "## Operacja 1: Wymiana G\u0142owy (Output Layer)\n",
    "\n",
    "To standardowy Fine-Tuning.\n",
    "ResNet18 ma na ko\u0144cu warstw\u0119 `Linear(512, 1000)`.\n",
    "My chcemy klasyfikowa\u0107 np. **2 klasy** (Kot vs Pies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e131457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PO WYMIANIE G\u0141OWY ---\n",
      "Linear(in_features=512, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Sprawdzamy ile cech wchodzi do ostatniej warstwy\n",
    "in_features = model.fc.in_features\n",
    "\n",
    "# Podmieniamy warstw\u0119 (Stara idzie do \u015bmieci, nowa jest losowa)\n",
    "model.fc = nn.Linear(in_features, 2)\n",
    "\n",
    "print(\"--- PO WYMIANIE G\u0141OWY ---\")\n",
    "print(model.fc)\n",
    "# Teraz model zwraca 2 logity zamiast 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e1512",
   "metadata": {},
   "source": [
    "## Operacja 2: Wymiana Oczu (Input Layer) + Przeszczep Wag\n",
    "\n",
    "To jest trudniejsze.\n",
    "Mamy zdj\u0119cia Rentgenowskie (1 kana\u0142). ResNet chce 3 kana\u0142y.\n",
    "Je\u015bli zrobimy `nn.Conv2d(1, 64, ...)`, nowa warstwa b\u0119dzie losowa. Zniszczymy ca\u0142\u0105 wiedz\u0119 o wykrywaniu kraw\u0119dzi, kt\u00f3r\u0105 ResNet zdoby\u0142 na ImageNet.\n",
    "\n",
    "**Trik:**\n",
    "Wagi w `conv1` maj\u0105 kszta\u0142t `[64, 3, 7, 7]` (64 filtry, 3 kana\u0142y, kernel 7x7).\n",
    "Mo\u017cemy zsumowa\u0107 (lub u\u015bredni\u0107) te 3 kana\u0142y, \u017ceby dosta\u0107 `[64, 1, 7, 7]`.\n",
    "To zadzia\u0142a, bo kraw\u0119d\u017a na zdj\u0119ciu czarno-bia\u0142ym wygl\u0105da tak samo jak na kolorowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c83e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stare wagi: torch.Size([64, 3, 7, 7])\n",
      "Nowe wagi (losowe): torch.Size([64, 1, 7, 7])\n",
      "\n",
      "\u2705 Przeszczep udany. Wagi z ImageNet zosta\u0142y zachowane (jako Grayscale).\n"
     ]
    }
   ],
   "source": [
    "# 1. Zapisujemy star\u0105 warstw\u0119\n",
    "old_conv = model.conv1\n",
    "\n",
    "# 2. Tworzymy now\u0105 warstw\u0119 (1 kana\u0142 wej\u015bciowy zamiast 3)\n",
    "# Musimy zachowa\u0107 te same parametry (kernel, stride, padding, bias)\n",
    "new_conv = nn.Conv2d(\n",
    "    in_channels=1, \n",
    "    out_channels=old_conv.out_channels, \n",
    "    kernel_size=old_conv.kernel_size, \n",
    "    stride=old_conv.stride, \n",
    "    padding=old_conv.padding,\n",
    "    bias=old_conv.bias is not None\n",
    ")\n",
    "\n",
    "print(f\"Stare wagi: {old_conv.weight.shape}\")\n",
    "print(f\"Nowe wagi (losowe): {new_conv.weight.shape}\")\n",
    "\n",
    "# 3. PRZESZCZEP WAG (Surgical Transplant)\n",
    "with torch.no_grad():\n",
    "    # Sumujemy wagi po wymiarze kana\u0142\u00f3w (dim=1) i dzielimy przez 3 (\u015brednia)\n",
    "    # [64, 3, 7, 7] -> [64, 1, 7, 7]\n",
    "    weight_avg = old_conv.weight.mean(dim=1, keepdim=True)\n",
    "    \n",
    "    # Wstrzykujemy do nowej warstwy\n",
    "    new_conv.weight.copy_(weight_avg)\n",
    "\n",
    "# 4. Podmieniamy warstw\u0119 w modelu\n",
    "model.conv1 = new_conv\n",
    "\n",
    "print(\"\\n\u2705 Przeszczep udany. Wagi z ImageNet zosta\u0142y zachowane (jako Grayscale).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cef1ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST PRZEP\u0141YWU ---\n",
      "Wej\u015bcie: torch.Size([1, 1, 224, 224])\n",
      "Wyj\u015bcie: torch.Size([1, 2]) (Oczekiwane: [1, 2])\n",
      "Pacjent prze\u017cy\u0142 operacj\u0119.\n"
     ]
    }
   ],
   "source": [
    "# TEST \u017bYWY\n",
    "# Generujemy losowy obrazek w skali szaro\u015bci (1 kana\u0142)\n",
    "dummy_xray = torch.randn(1, 1, 224, 224)\n",
    "\n",
    "try:\n",
    "    output = model(dummy_xray)\n",
    "    print(\"\\n--- TEST PRZEP\u0141YWU ---\")\n",
    "    print(f\"Wej\u015bcie: {dummy_xray.shape}\")\n",
    "    print(f\"Wyj\u015bcie: {output.shape} (Oczekiwane: [1, 2])\")\n",
    "    print(\"Pacjent prze\u017cy\u0142 operacj\u0119.\")\n",
    "except Exception as e:\n",
    "    print(f\"\ud83d\udc80 B\u0142\u0105d: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830536f",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  **Nie trenuj od zera**, je\u015bli nie musisz. Nawet je\u015bli masz inny rozmiar wej\u015bcia, mo\u017cesz zaadaptowa\u0107 wagi.\n",
    "2.  **Suma wag:** Je\u015bli zmieniasz wej\u015bcie z 3 kana\u0142\u00f3w na 4 (np. RGB + Podczerwie\u0144), mo\u017cesz wzi\u0105\u0107 wagi z RGB, a dla 4. kana\u0142u zainicjowa\u0107 zerami (lub \u015bredni\u0105). Wtedy model na starcie dzia\u0142a jak zwyk\u0142y ResNet, a z czasem uczy si\u0119 u\u017cywa\u0107 podczerwieni.\n",
    "3.  **Model Surgery** to codzienno\u015b\u0107 w pracy z obrazami medycznymi i satelitarnymi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}