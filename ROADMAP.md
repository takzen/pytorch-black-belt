# ğŸ”¥ PyTorch Advanced Course - Deep Dive Roadmap

Kompleksowy kurs zaawansowanego PyTorch dla inÅ¼ynierÃ³w ML, ktÃ³rzy chcÄ… zrozumieÄ‡, jak naprawdÄ™ dziaÅ‚a deep learning pod maskÄ….

---

## ğŸ—ï¸ ModuÅ‚ 1: GÅ‚Ä™bokie zanurzenie w Tensory (Fundamenty)

**Jak PyTorch zarzÄ…dza pamiÄ™ciÄ… i matematykÄ….**

- **01_Storage_vs_View.ipynb** â€“ Storage vs Tensor. Zrozumienie, czym jest Stride (krok) i dlaczego `view()` rzuca bÅ‚Ä™dy na nieciÄ…gÅ‚ych tensorach (contiguous).
- **02_Broadcasting_Magic.ipynb** â€“ Matematyka rozgÅ‚aszania. Jak dodawaÄ‡ wektor do macierzy bez pÄ™tli for i kopiowania pamiÄ™ci.
- **03_Einsum_Is_All_You_Need.ipynb** â€“ `torch.einsum`. Jedna funkcja, by rzÄ…dziÄ‡ wszystkimi (mnoÅ¼enie macierzy, iloczyny skalarne, transpozycje w jednym stringu).
- **04_Advanced_Indexing.ipynb** â€“ `gather`, `scatter` i `index_select`. Jak manipulowaÄ‡ danymi w Reinforcement Learning i Grafach (gdzie indeksy sÄ… skomplikowane).
- **05_In_Place_Operations.ipynb** â€“ `x += 1` vs `x = x + 1`. Kiedy oszczÄ™dzasz pamiÄ™Ä‡, a kiedy niszczysz historiÄ™ gradientÃ³w?
- **06_Einops_Tutorial.ipynb** â€“ Biblioteka einops. Nowoczesne, czytelne manipulacje tensorami (`rearrange`, `reduce`), ktÃ³re zastÄ™pujÄ… skomplikowane `view`/`permute`.
- **07_Named_Tensors.ipynb** â€“ Eksperymentalna funkcja: Tensory z nazwami wymiarÃ³w (np. `img.rename(C, H, W)`). BezpieczeÅ„stwo typÃ³w w Deep Learningu.

---

## ğŸ§® ModuÅ‚ 2: WnÄ™trze Autogradu (Silnik)

**Jak dziaÅ‚a rÃ³Å¼niczkowanie automatyczne i jak je hackowaÄ‡.**

- **08_Computational_Graph_Viz.ipynb** â€“ Wizualizacja DAG (Directed Acyclic Graph). Czym sÄ… liÅ›cie (`is_leaf`) i funkcje `grad_fn`.
- **09_Requires_Grad_Mechanics.ipynb** â€“ Kiedy uÅ¼ywaÄ‡ `.detach()`, `with torch.no_grad()` a kiedy `inference_mode()`. Subtelne rÃ³Å¼nice w wydajnoÅ›ci.
- **10_Custom_Autograd_Function.ipynb** â€“ Pisanie wÅ‚asnej warstwy z rÄ™cznÄ… metodÄ… `backward()`. (Np. dla funkcji, ktÃ³rej PyTorch nie obsÅ‚uguje lub dla optymalizacji).
- **11_Jacobian_and_Hessian.ipynb** â€“ Obliczanie pochodnych wyÅ¼szego rzÄ™du (np. do meta-learningu MAML) za pomocÄ… `torch.autograd.functional`.
- **12_Retain_Graph_Trick.ipynb** â€“ BÅ‚Ä…d "Trying to backward through the graph a second time". Kiedy i dlaczego musimy uÅ¼ywaÄ‡ `retain_graph=True`?
- **13_Gradient_Accumulation.ipynb** â€“ Jak trenowaÄ‡ na Batch Size = 128, majÄ…c pamiÄ™Ä‡ tylko na 8? Symulacja duÅ¼ych batchy.
- **14_Forward_Mode_AD.ipynb** â€“ NowoÅ›Ä‡ w AI. RÃ³Å¼niczkowanie w przÃ³d (Forward Mode) vs klasyczne wstecz (Reverse Mode). Kiedy to siÄ™ przydaje?

---

## ğŸ’¿ ModuÅ‚ 3: InÅ¼ynieria Danych (Paliwo)

**Optymalizacja pipeline'u danych, Å¼eby GPU nie czekaÅ‚o.**

- **15_Dataset_vs_IterableDataset.ipynb** â€“ Kiedy dane nie mieszczÄ… siÄ™ w RAM. Streaming danych z dysku/sieci.
- **16_Custom_Collate_Fn.ipynb** â€“ ObsÅ‚uga danych o rÃ³Å¼nej dÅ‚ugoÅ›ci (np. tekst, audio). Jak pisaÄ‡ wÅ‚asne funkcje sklejajÄ…ce batch.
- **17_Samplers_and_Imbalance.ipynb** â€“ `WeightedRandomSampler`. Jak trenowaÄ‡ na niezbalansowanych danych bez duplikowania plikÃ³w.
- **18_Num_Workers_and_Pin_Memory.ipynb** â€“ Analiza wielowÄ…tkowoÅ›ci w DataLoaderze. Czym jest Page-Locked Memory (`pin_memory`) i kiedy przyspiesza transfer na GPU.
- **19_Data_Augmentation_GPU.ipynb** â€“ Kornia vs Torchvision. Dlaczego augmentacja na CPU (w DataLoaderze) to wÄ…skie gardÅ‚o i jak przenieÅ›Ä‡ jÄ… na GPU.
- **20_WebDataset_Concept.ipynb** â€“ (Teoria/Demo) Format TAR do ultra-szybkiego czytania milionÃ³w maÅ‚ych plikÃ³w (standard w treningu LLM/Stable Diffusion).

---

## ğŸ§  ModuÅ‚ 4: Zaawansowana Architektura (Konstrukcja)

**Triki architektoniczne i zarzÄ…dzanie stanem modelu.**

- **21_Module_Life_Cycle.ipynb** â€“ `__init__`, `forward`, `__call__`. Jak dziaÅ‚a magia `nn.Module` pod spodem.
- **22_Buffers_vs_Parameters.ipynb** â€“ Czym siÄ™ rÃ³Å¼ni `self.param` od `register_buffer`? (PrzykÅ‚ad na BatchNorm i Positional Encoding).
- **23_Hooks_Anatomy.ipynb** â€“ `register_forward_hook` i `register_backward_hook`. Jak wyciÄ…gaÄ‡ aktywacje z Å›rodka sieci bez zmieniania jej kodu (Feature Extraction).
- **24_Weight_Initialization.ipynb** â€“ Dlaczego `kaiming_normal` i `xavier_uniform` sÄ… kluczowe? Wizualizacja eksplozji/zaniku gradientu przy zÅ‚ej inicjalizacji.
- **25_Weight_Sharing.ipynb** â€“ Jak uÅ¼yÄ‡ tej samej warstwy w dwÃ³ch miejscach sieci (np. w Autoenkoderach Tied-Weights).
- **26_Dynamic_Control_Flow.ipynb** â€“ UÅ¼ywanie pÄ™tli `for` i `if` wewnÄ…trz `forward`. Jak PyTorch radzi sobie z dynamicznymi grafami (w przeciwieÅ„stwie do TensorFlow).
- **27_Gradient_Checkpointing.ipynb** â€“ Handel: Czas za PamiÄ™Ä‡. Jak zmieÅ›ciÄ‡ 10x wiÄ™kszy model w VRAM, obliczajÄ…c czÄ™Å›Ä‡ grafu dwukrotnie.
- **28_Model_Surgery.ipynb** â€“ Wczytywanie pretrenowanego modelu i podmienianie jego warstw (np. zmiana rozmiaru wejÅ›cia w ResNet).

---

## âš¡ ModuÅ‚ 5: Trening i Optymalizacja (SzybkoÅ›Ä‡)

**Stabilizacja i przyspieszanie uczenia.**

- **29_Optimizer_Internals.ipynb** â€“ Jak dziaÅ‚a `torch.optim`? Pisanie wÅ‚asnego optymalizatora od zera (SGD z Momentum).
- **30_Learning_Rate_Schedulers.ipynb** â€“ `CosineAnnealing`, `OneCycleLR`, `ReduceLROnPlateau`. Wizualizacja wpÅ‚ywu na zbieÅ¼noÅ›Ä‡.
- **31_Mixed_Precision_AMP.ipynb** â€“ `torch.cuda.amp`. Jak uÅ¼ywaÄ‡ Autocast i GradScaler, Å¼eby trenowaÄ‡ 2x szybciej w FP16.
- **32_Gradient_Clipping.ipynb** â€“ Jak zapobiegaÄ‡ NaN w treningu (szczegÃ³lnie w RNN/Transformerach) poprzez przycinanie normy gradientu.
- **33_Torch_Compile_Intro.ipynb** â€“ PyTorch 2.0. Wprowadzenie do `torch.compile()` i trybÃ³w optymalizacji (`reduce-overhead`, `max-autotune`).
- **34_Bottleneck_Analysis.ipynb** â€“ Jak uÅ¼ywaÄ‡ `torch.autograd.profiler`, Å¼eby sprawdziÄ‡, ktÃ³ra warstwa zjada najwiÄ™cej czasu.
- **35_Weight_Decay_vs_L2.ipynb** â€“ Subtelna rÃ³Å¼nica miÄ™dzy Weight Decay w AdamW a regularyzacjÄ… L2 (i dlaczego AdamW jest lepszy).
- **36_Reproducibility_Seeding.ipynb** â€“ Jak poprawnie ustawiÄ‡ ziarna losowoÅ›ci (`manual_seed`, `deterministic`), Å¼eby wynik byÅ‚ zawsze ten sam (rÃ³wnieÅ¼ na GPU).

---

## ğŸ“¦ ModuÅ‚ 6: Ekosystem i Produkcja (Skala)

**NarzÄ™dzia dojrzaÅ‚ego inÅ¼yniera.**

- **37_PyTorch_Lightning_Refactor.ipynb** â€“ Przepisanie pÄ™tli treningowej na `LightningModule`. Czysty kod bez boilerplate'u.
- **38_TensorBoard_Logging.ipynb** â€“ Jak logowaÄ‡ nie tylko stratÄ™, ale teÅ¼ histogramy wag, obrazy i graf modelu do TensorBoarda.
- **39_TorchScript_Tracing.ipynb** â€“ `torch.jit.trace`. Zamiana dynamicznego modelu w statyczny graf dla C++. Ograniczenia i puÅ‚apki.
- **40_TorchScript_Scripting.ipynb** â€“ `torch.jit.script`. Jak kompilowaÄ‡ modele z logikÄ… sterowania (`if`/`else`), ktÃ³rej Tracing nie widzi.
- **41_ONNX_Advanced_Export.ipynb** â€“ Dynamiczne osie w ONNX (zmienna dÅ‚ugoÅ›Ä‡ batcha). Debugowanie bÅ‚Ä™dÃ³w eksportu.
- **42_Inference_Optimization.ipynb** â€“ ÅÄ…czenie warstw (Conv+BN fusion) przed wdroÅ¼eniem dla szybszego dziaÅ‚ania.
- **43_DDP_Concepts.ipynb** â€“ Teoria treningu rozproszonego (Distributed Data Parallel). Jak dziaÅ‚a synchronizacja gradientÃ³w miÄ™dzy wieloma GPU.
- **44_FSDP_Concepts.ipynb** â€“ Fully Sharded Data Parallel. Jak trenowaÄ‡ modele, ktÃ³re nie mieszczÄ… siÄ™ na jednej karcie (dzielenie modelu na kawaÅ‚ki).

---

## ğŸ® ModuÅ‚ 7: Eksperymenty i Ciekawostki (Bonus)

**Rzeczy dziwne i przydatne.**

- **45_Meta_Learning_Higher.ipynb** â€“ UÅ¼ycie biblioteki `higher` do rÃ³Å¼niczkowania przez pÄ™tlÄ™ optymalizatora (Unrolled optimization).
- **46_PyTorch_Hooks_Visualization.ipynb** â€“ Wykorzystanie hookÃ³w do wizualizacji map aktywacji (CAM - Class Activation Mapping).
- **47_Adversarial_Example_Generation.ipynb** â€“ UÅ¼ycie dostÄ™pu do gradientÃ³w wejÅ›cia, aby stworzyÄ‡ obraz mylÄ…cy sieÄ‡ (FGSM).
- **48_Neural_Style_Transfer_Raw.ipynb** â€“ Manipulacja aktywacjami wewnÄ…trz VGG do przenoszenia stylu artystycznego (bez gotowych bibliotek).
- **49_Custom_Loss_Functions.ipynb** â€“ Pisanie zÅ‚oÅ¼onych funkcji kosztu (np. Triplet Loss, Contrastive Loss) z wykorzystaniem operacji macierzowych.
- **50_The_Grand_Exam.ipynb** â€“ "Egzamin KoÅ„cowy". Zestaw trudnych pytaÅ„ rekrutacyjnych i snippetÃ³w kodu do debugowania ("ZnajdÅº bÅ‚Ä…d w tej pÄ™tli treningowej").

---

**Powodzenia w zgÅ‚Ä™bianiu PyTorch! ğŸ”¥**
