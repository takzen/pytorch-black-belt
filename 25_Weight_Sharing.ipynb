{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/pytorch-black-belt/blob/main/25_Weight_Sharing.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# \u2601\ufe0f COLAB SETUP (Automatyczna instalacja \u015brodowiska)\n",
    "# --------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sprawdzamy, czy jeste\u015bmy w Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('\u2601\ufe0f Wykryto \u015brodowisko Google Colab. Konfiguruj\u0119...')\n",
    "\n",
    "    # 1. Pobieramy plik requirements.txt bezpo\u015brednio z repozytorium\n",
    "    !wget -q https://raw.githubusercontent.com/takzen/ai-engineering-handbook/main/requirements.txt -O requirements.txt\n",
    "\n",
    "    # 2. Instalujemy biblioteki\n",
    "    print('\u23f3 Instaluj\u0119 zale\u017cno\u015bci (to mo\u017ce chwil\u0119 potrwa\u0107)...')\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print('\u2705 Gotowe! \u015arodowisko jest zgodne z repozytorium.')\n",
    "else:\n",
    "    print('\ud83d\udcbb Wykryto \u015brodowisko lokalne. Zak\u0142adam, \u017ce masz ju\u017c uv/venv.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d04620",
   "metadata": {},
   "source": [
    "# \ud83e\udd4b Lekcja 25: Weight Sharing (Wi\u0105zanie Wag)\n",
    "\n",
    "Wi\u0105zanie wag to technika, gdzie ten sam tensor parametr\u00f3w jest u\u017cywany w kilku miejscach sieci.\n",
    "\n",
    "**Zastosowania:**\n",
    "1.  **Autoenkodery:** Wagi Dekodera s\u0105 transpozycj\u0105 Encodera.\n",
    "2.  **NLP (Language Models):** Warstwa Embeddingu (wej\u015bcie) i warstwa projekcji (wyj\u015bcie) cz\u0119sto wsp\u00f3\u0142dziel\u0105 t\u0119 sam\u0105 macierz (wielk\u0105, np. 50k s\u0142\u00f3w).\n",
    "3.  **Sieci Syjamskie:** Dwa obrazki przechodz\u0105 przez *t\u0119 sam\u0105* sie\u0107 (to te\u017c forma weight sharingu).\n",
    "\n",
    "**Jak to zrobi\u0107 w PyTorch?**\n",
    "Po prostu przypisz ten sam obiekt `nn.Parameter` do dw\u00f3ch atrybut\u00f3w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b64ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STANDARD ---\n",
      "Adres wagi Encoder: 3704677794240\n",
      "Adres wagi Decoder: 3704677794560\n",
      "Adresy s\u0105 R\u00d3\u017bNE. To dwie osobne macierze.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Podej\u015bcie Standardowe (Niezale\u017cne wagi)\n",
    "class StandardAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim) # Nowa, niezale\u017cna macierz\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.encoder(x))\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model_std = StandardAutoencoder(10, 5)\n",
    "\n",
    "print(\"--- STANDARD ---\")\n",
    "print(f\"Adres wagi Encoder: {model_std.encoder.weight.data_ptr()}\")\n",
    "print(f\"Adres wagi Decoder: {model_std.decoder.weight.data_ptr()}\")\n",
    "print(\"Adresy s\u0105 R\u00d3\u017bNE. To dwie osobne macierze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b5603",
   "metadata": {},
   "source": [
    "## Implementacja Tied Weights\n",
    "\n",
    "Teraz zrobimy to sprytnie.\n",
    "Nie b\u0119dziemy tworzy\u0107 `nn.Linear` dla decodera.\n",
    "U\u017cyjemy `nn.Linear` dla Encodera, a w Decoderze u\u017cyjemy **funkcyjnego** wywo\u0142ania `F.linear`, podaj\u0105c mu wag\u0119 Encodera (transponowan\u0105)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6aab1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TIED WEIGHTS ---\n",
      "Waga Encodera: torch.Size([5, 10])\n",
      "Wagi s\u0105 fizycznie tym samym obiektem.\n"
     ]
    }
   ],
   "source": [
    "class TiedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Encoder: Standardowa warstwa\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Decoder: NIE tworzymy nowej warstwy Linear.\n",
    "        # Mo\u017cemy ewentualnie stworzy\u0107 osobny bias, bo bias nie musi by\u0107 wi\u0105zany\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Encode\n",
    "        x = F.relu(self.encoder(x))\n",
    "        \n",
    "        # 2. Decode (U\u017cywamy wagi Encodera!)\n",
    "        # Wz\u00f3r: x @ W_enc.T + bias\n",
    "        # F.linear robi x @ W.T, wi\u0119c my musimy poda\u0107 W_enc.T.T = W_enc? \n",
    "        # Czekaj! F.linear(input, weight) wykonuje: input @ weight.T\n",
    "        # My chcemy u\u017cy\u0107 tej samej macierzy, ale transponowanej wzgl\u0119dem Encodera.\n",
    "        # Waga Encodera ma kszta\u0142t [Hidden, Input].\n",
    "        # Do Decodera potrzebujemy [Input, Hidden] (\u017ceby pomno\u017cy\u0107 [Batch, Hidden]).\n",
    "        # F.linear oczekuje wagi o kszta\u0142cie [Out, In].\n",
    "        \n",
    "        # Tutaj waga encoder.weight ma kszta\u0142t [Hidden, Input].\n",
    "        # My chcemy wyj\u015bcie o rozmiarze Input.\n",
    "        # F.linear(x, self.encoder.weight.t()) -> x @ (W.t()).t() -> x @ W\n",
    "        \n",
    "        # Dla Tied Weights w AE zazwyczaj u\u017cywamy: W_dec = W_enc.T\n",
    "        x = F.linear(x, self.encoder.weight.t(), self.decoder_bias)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model_tied = TiedAutoencoder(10, 5)\n",
    "\n",
    "print(\"\\n--- TIED WEIGHTS ---\")\n",
    "print(f\"Waga Encodera: {model_tied.encoder.weight.shape}\")\n",
    "# Nie ma 'model_tied.decoder', bo u\u017cyli\u015bmy F.linear\n",
    "print(\"Wagi s\u0105 fizycznie tym samym obiektem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd8e11",
   "metadata": {},
   "source": [
    "## Dow\u00f3d: Gradienty\n",
    "\n",
    "Skoro wagi s\u0105 wsp\u00f3\u0142dzielone, to podczas `backward()` gradienty z \"obu stron\" (z wej\u015bcia i wyj\u015bcia) powinny si\u0119 **zsumowa\u0107** w jednym parametrze.\n",
    "\n",
    "Zr\u00f3bmy test:\n",
    "1.  Pu\u015bcimy dane.\n",
    "2.  Policzymy strat\u0119.\n",
    "3.  Zrobimy `backward`.\n",
    "4.  Sprawdzimy, czy waga Encodera otrzyma\u0142a gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bdf73b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GRADIENTY ---\n",
      "Czy waga Encodera ma gradient? True\n",
      "Warto\u015b\u0107 gradientu (norma): 7.5713\n",
      "\n",
      "Po zmianie wagi Encodera, wynik Decodera: 1769.00\n",
      "(Wynik jest ogromny, co dowodzi, \u017ce zmiana wagi wp\u0142yn\u0119\u0142a na ca\u0142\u0105 sie\u0107).\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 10)\n",
    "y = x.clone() # Autoenkoder ma odtworzy\u0107 wej\u015bcie\n",
    "\n",
    "# Forward\n",
    "out = model_tied(x)\n",
    "loss = ((out - y)**2).sum()\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n",
    "\n",
    "print(\"--- GRADIENTY ---\")\n",
    "print(f\"Czy waga Encodera ma gradient? {model_tied.encoder.weight.grad is not None}\")\n",
    "print(f\"Warto\u015b\u0107 gradientu (norma): {model_tied.encoder.weight.grad.norm().item():.4f}\")\n",
    "\n",
    "# Zmie\u0144my wag\u0119 r\u0119cznie i sprawd\u017amy czy 'decoder' (logika) to odczuje\n",
    "with torch.no_grad():\n",
    "    model_tied.encoder.weight.fill_(10.0)\n",
    "\n",
    "out_new = model_tied(x)\n",
    "print(f\"\\nPo zmianie wagi Encodera, wynik Decodera: {out_new.mean().item():.2f}\")\n",
    "print(\"(Wynik jest ogromny, co dowodzi, \u017ce zmiana wagi wp\u0142yn\u0119\u0142a na ca\u0142\u0105 sie\u0107).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b720aa",
   "metadata": {},
   "source": [
    "## Alternatywa: Przypisanie atrybutu\n",
    "\n",
    "Mo\u017cna te\u017c zrobi\u0107 to bardziej \"obiektowo\", przypisuj\u0105c ten sam parametr do dw\u00f3ch warstw `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ef1a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC1 ptr: 3704678056576\n",
      "FC2 ptr: 3704678056576\n",
      "\u2705 Adresy identyczne. PyTorch b\u0119dzie aktualizowa\u0142 oba naraz.\n"
     ]
    }
   ],
   "source": [
    "class SharedLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        \n",
    "        # NADPISUJEMY wag\u0119 fc2 wag\u0105 fc1\n",
    "        self.fc2.weight = self.fc1.weight\n",
    "        # Teraz to ten sam obiekt w pami\u0119ci!\n",
    "\n",
    "model_shared = SharedLinear()\n",
    "\n",
    "print(f\"FC1 ptr: {model_shared.fc1.weight.data_ptr()}\")\n",
    "print(f\"FC2 ptr: {model_shared.fc2.weight.data_ptr()}\")\n",
    "\n",
    "if model_shared.fc1.weight.data_ptr() == model_shared.fc2.weight.data_ptr():\n",
    "    print(\"\u2705 Adresy identyczne. PyTorch b\u0119dzie aktualizowa\u0142 oba naraz.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f177b",
   "metadata": {},
   "source": [
    "## \ud83e\udd4b Black Belt Summary\n",
    "\n",
    "1.  **Weight Sharing** to nie magia. To po prostu u\u017cycie tego samego tensora w wielu miejscach grafu obliczeniowego.\n",
    "2.  **Autograd** radzi sobie z tym doskonale. Gradienty z r\u00f3\u017cnych miejsc grafu, kt\u00f3re u\u017cywaj\u0105 tej samej wagi, s\u0105 po prostu **sumowane** (Accumulate Grad).\n",
    "3.  **Zastosowanie:**\n",
    "    *   Zmniejszenie liczby parametr\u00f3w (mniejszy model).\n",
    "    *   Regularyzacja (trudniej o overfitting, gdy wagi s\u0105 zwi\u0105zane).\n",
    "    *   W NLP (Transformers) wi\u0105zanie Embeddingu wej\u015bciowego z wyj\u015bciowym to standard (Input/Output Embedding Tying)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}