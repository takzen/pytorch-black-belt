{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d04620",
   "metadata": {},
   "source": [
    "# ðŸ¥‹ Lekcja 25: Weight Sharing (WiÄ…zanie Wag)\n",
    "\n",
    "WiÄ…zanie wag to technika, gdzie ten sam tensor parametrÃ³w jest uÅ¼ywany w kilku miejscach sieci.\n",
    "\n",
    "**Zastosowania:**\n",
    "1.  **Autoenkodery:** Wagi Dekodera sÄ… transpozycjÄ… Encodera.\n",
    "2.  **NLP (Language Models):** Warstwa Embeddingu (wejÅ›cie) i warstwa projekcji (wyjÅ›cie) czÄ™sto wspÃ³Å‚dzielÄ… tÄ™ samÄ… macierz (wielkÄ…, np. 50k sÅ‚Ã³w).\n",
    "3.  **Sieci Syjamskie:** Dwa obrazki przechodzÄ… przez *tÄ™ samÄ…* sieÄ‡ (to teÅ¼ forma weight sharingu).\n",
    "\n",
    "**Jak to zrobiÄ‡ w PyTorch?**\n",
    "Po prostu przypisz ten sam obiekt `nn.Parameter` do dwÃ³ch atrybutÃ³w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b64ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STANDARD ---\n",
      "Adres wagi Encoder: 3704677794240\n",
      "Adres wagi Decoder: 3704677794560\n",
      "Adresy sÄ… RÃ“Å»NE. To dwie osobne macierze.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. PodejÅ›cie Standardowe (NiezaleÅ¼ne wagi)\n",
    "class StandardAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim) # Nowa, niezaleÅ¼na macierz\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.encoder(x))\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model_std = StandardAutoencoder(10, 5)\n",
    "\n",
    "print(\"--- STANDARD ---\")\n",
    "print(f\"Adres wagi Encoder: {model_std.encoder.weight.data_ptr()}\")\n",
    "print(f\"Adres wagi Decoder: {model_std.decoder.weight.data_ptr()}\")\n",
    "print(\"Adresy sÄ… RÃ“Å»NE. To dwie osobne macierze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b5603",
   "metadata": {},
   "source": [
    "## Implementacja Tied Weights\n",
    "\n",
    "Teraz zrobimy to sprytnie.\n",
    "Nie bÄ™dziemy tworzyÄ‡ `nn.Linear` dla decodera.\n",
    "UÅ¼yjemy `nn.Linear` dla Encodera, a w Decoderze uÅ¼yjemy **funkcyjnego** wywoÅ‚ania `F.linear`, podajÄ…c mu wagÄ™ Encodera (transponowanÄ…)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6aab1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TIED WEIGHTS ---\n",
      "Waga Encodera: torch.Size([5, 10])\n",
      "Wagi sÄ… fizycznie tym samym obiektem.\n"
     ]
    }
   ],
   "source": [
    "class TiedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Encoder: Standardowa warstwa\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Decoder: NIE tworzymy nowej warstwy Linear.\n",
    "        # MoÅ¼emy ewentualnie stworzyÄ‡ osobny bias, bo bias nie musi byÄ‡ wiÄ…zany\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Encode\n",
    "        x = F.relu(self.encoder(x))\n",
    "        \n",
    "        # 2. Decode (UÅ¼ywamy wagi Encodera!)\n",
    "        # WzÃ³r: x @ W_enc.T + bias\n",
    "        # F.linear robi x @ W.T, wiÄ™c my musimy podaÄ‡ W_enc.T.T = W_enc? \n",
    "        # Czekaj! F.linear(input, weight) wykonuje: input @ weight.T\n",
    "        # My chcemy uÅ¼yÄ‡ tej samej macierzy, ale transponowanej wzglÄ™dem Encodera.\n",
    "        # Waga Encodera ma ksztaÅ‚t [Hidden, Input].\n",
    "        # Do Decodera potrzebujemy [Input, Hidden] (Å¼eby pomnoÅ¼yÄ‡ [Batch, Hidden]).\n",
    "        # F.linear oczekuje wagi o ksztaÅ‚cie [Out, In].\n",
    "        \n",
    "        # Tutaj waga encoder.weight ma ksztaÅ‚t [Hidden, Input].\n",
    "        # My chcemy wyjÅ›cie o rozmiarze Input.\n",
    "        # F.linear(x, self.encoder.weight.t()) -> x @ (W.t()).t() -> x @ W\n",
    "        \n",
    "        # Dla Tied Weights w AE zazwyczaj uÅ¼ywamy: W_dec = W_enc.T\n",
    "        x = F.linear(x, self.encoder.weight.t(), self.decoder_bias)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model_tied = TiedAutoencoder(10, 5)\n",
    "\n",
    "print(\"\\n--- TIED WEIGHTS ---\")\n",
    "print(f\"Waga Encodera: {model_tied.encoder.weight.shape}\")\n",
    "# Nie ma 'model_tied.decoder', bo uÅ¼yliÅ›my F.linear\n",
    "print(\"Wagi sÄ… fizycznie tym samym obiektem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd8e11",
   "metadata": {},
   "source": [
    "## DowÃ³d: Gradienty\n",
    "\n",
    "Skoro wagi sÄ… wspÃ³Å‚dzielone, to podczas `backward()` gradienty z \"obu stron\" (z wejÅ›cia i wyjÅ›cia) powinny siÄ™ **zsumowaÄ‡** w jednym parametrze.\n",
    "\n",
    "ZrÃ³bmy test:\n",
    "1.  PuÅ›cimy dane.\n",
    "2.  Policzymy stratÄ™.\n",
    "3.  Zrobimy `backward`.\n",
    "4.  Sprawdzimy, czy waga Encodera otrzymaÅ‚a gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bdf73b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GRADIENTY ---\n",
      "Czy waga Encodera ma gradient? True\n",
      "WartoÅ›Ä‡ gradientu (norma): 7.5713\n",
      "\n",
      "Po zmianie wagi Encodera, wynik Decodera: 1769.00\n",
      "(Wynik jest ogromny, co dowodzi, Å¼e zmiana wagi wpÅ‚ynÄ™Å‚a na caÅ‚Ä… sieÄ‡).\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 10)\n",
    "y = x.clone() # Autoenkoder ma odtworzyÄ‡ wejÅ›cie\n",
    "\n",
    "# Forward\n",
    "out = model_tied(x)\n",
    "loss = ((out - y)**2).sum()\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n",
    "\n",
    "print(\"--- GRADIENTY ---\")\n",
    "print(f\"Czy waga Encodera ma gradient? {model_tied.encoder.weight.grad is not None}\")\n",
    "print(f\"WartoÅ›Ä‡ gradientu (norma): {model_tied.encoder.weight.grad.norm().item():.4f}\")\n",
    "\n",
    "# ZmieÅ„my wagÄ™ rÄ™cznie i sprawdÅºmy czy 'decoder' (logika) to odczuje\n",
    "with torch.no_grad():\n",
    "    model_tied.encoder.weight.fill_(10.0)\n",
    "\n",
    "out_new = model_tied(x)\n",
    "print(f\"\\nPo zmianie wagi Encodera, wynik Decodera: {out_new.mean().item():.2f}\")\n",
    "print(\"(Wynik jest ogromny, co dowodzi, Å¼e zmiana wagi wpÅ‚ynÄ™Å‚a na caÅ‚Ä… sieÄ‡).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b720aa",
   "metadata": {},
   "source": [
    "## Alternatywa: Przypisanie atrybutu\n",
    "\n",
    "MoÅ¼na teÅ¼ zrobiÄ‡ to bardziej \"obiektowo\", przypisujÄ…c ten sam parametr do dwÃ³ch warstw `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ef1a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC1 ptr: 3704678056576\n",
      "FC2 ptr: 3704678056576\n",
      "âœ… Adresy identyczne. PyTorch bÄ™dzie aktualizowaÅ‚ oba naraz.\n"
     ]
    }
   ],
   "source": [
    "class SharedLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        \n",
    "        # NADPISUJEMY wagÄ™ fc2 wagÄ… fc1\n",
    "        self.fc2.weight = self.fc1.weight\n",
    "        # Teraz to ten sam obiekt w pamiÄ™ci!\n",
    "\n",
    "model_shared = SharedLinear()\n",
    "\n",
    "print(f\"FC1 ptr: {model_shared.fc1.weight.data_ptr()}\")\n",
    "print(f\"FC2 ptr: {model_shared.fc2.weight.data_ptr()}\")\n",
    "\n",
    "if model_shared.fc1.weight.data_ptr() == model_shared.fc2.weight.data_ptr():\n",
    "    print(\"âœ… Adresy identyczne. PyTorch bÄ™dzie aktualizowaÅ‚ oba naraz.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f177b",
   "metadata": {},
   "source": [
    "## ðŸ¥‹ Black Belt Summary\n",
    "\n",
    "1.  **Weight Sharing** to nie magia. To po prostu uÅ¼ycie tego samego tensora w wielu miejscach grafu obliczeniowego.\n",
    "2.  **Autograd** radzi sobie z tym doskonale. Gradienty z rÃ³Å¼nych miejsc grafu, ktÃ³re uÅ¼ywajÄ… tej samej wagi, sÄ… po prostu **sumowane** (Accumulate Grad).\n",
    "3.  **Zastosowanie:**\n",
    "    *   Zmniejszenie liczby parametrÃ³w (mniejszy model).\n",
    "    *   Regularyzacja (trudniej o overfitting, gdy wagi sÄ… zwiÄ…zane).\n",
    "    *   W NLP (Transformers) wiÄ…zanie Embeddingu wejÅ›ciowego z wyjÅ›ciowym to standard (Input/Output Embedding Tying)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-black-belt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
